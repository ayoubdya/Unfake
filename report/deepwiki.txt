"# Overview

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [README.md](README.md)
- [api/main.py](api/main.py)
- [frontend/app.js](frontend/app.js)
- [frontend/index.html](frontend/index.html)
- [pyproject.toml](pyproject.toml)

 /details 



## Purpose and Scope

UnFake is an AI-powered fake news detection system that classifies news content as either \"Fake\" or \"Real\" using machine learning models. The system provides a web-based interface for users to submit text for verification and receive confidence-scored predictions. This document provides a high-level overview of the UnFake architecture, its key components, and how they interact.

For detailed information about specific subsystems, see:
- API endpoint specifications: [API Endpoints](#3.1)
- Machine learning model details: [Machine Learning Models](#5)
- Training procedures: [Training Pipeline](#6)
- Deployment instructions: [Deployment and Configuration](#8)

**Sources:** [README.md:1-1](), [api/main.py:87-92]()

---

## System Architecture

UnFake implements a three-tier architecture separating presentation, application logic, and machine learning inference into distinct layers. The following diagram illustrates the overall system structure and component relationships:

### System Component Architecture

```mermaid
graph TB
    subgraph Frontend[\"Frontend Layer (frontend/)\"]
        IndexHTML[\"index.html\"]
        AppJS[\"app.js\"]
        StylesCSS[\"styles.css\"]
    end
    
    subgraph Backend[\"Backend Layer (api/)\"]
        MainPy[\"main.py br/ FastAPI Application\"]
        TextProcPy[\"text_processing.py br/ clean_text(), clean_article()\"]
        TypesPy[\"types.py br/ Pydantic Models\"]
    end
    
    subgraph Models[\"ML Model Layer (data/)\"]
        RoBERTaModel[\"RoBERTa_Classifier/ br/ Fine-tuned Transformer\"]
        GBCModel[\"gradient_boosting_classifier.pkl br/ Gradient Boosting Classifier\"]
        TfidfVec[\"tfidf_vectorizer.pkl br/ TF-IDF Vectorizer\"]
    end
    
    subgraph Training[\"Training Layer (notebooks/)\"]
        TrainHeadline[\"train_headline.ipynb\"]
        TrainArticle[\"train_article.ipynb\"]
        DataFiles[\"True.csv, Fake.csv br/ politifact_statements.csv\"]
    end
    
    AppJS -- |\"POST /predict\"| MainPy
    AppJS -- |\"POST /predict/article\"| MainPy
    
    MainPy --  TextProcPy
    MainPy --  TypesPy
    MainPy --  RoBERTaModel
    MainPy --  GBCModel
    MainPy --  TfidfVec
    
    TrainHeadline -- |produces| RoBERTaModel
    TrainArticle -- |produces| GBCModel
    TrainArticle -- |produces| TfidfVec
    DataFiles -- |feeds| TrainHeadline
    DataFiles -- |feeds| TrainArticle
```

**Sources:** [api/main.py:1-290](), [frontend/app.js:1-228](), [frontend/index.html:1-174]()

---

## Key Components

### Frontend Application

The frontend provides a single-page web application built with vanilla HTML, CSS, and JavaScript. It implements mode switching between statement and article analysis, real-time API communication, and animated result visualization.

| Component | File | Purpose |
|-----------|------|---------|
| UI Structure | `frontend/index.html` | Defines semantic HTML with input section, results section, and error section |
| Application Logic | `frontend/app.js` | Manages state, API calls, and DOM manipulation through functions like `handleAnalyze()`, `showResults()`, `switchMode()` |
| Visual Design | `frontend/styles.css` | Implements CSS variables, animations, and responsive layouts |

**Key Functions:**
- `switchMode(mode)` [frontend/app.js:46-65](): Toggles between statement and article input modes
- `handleAnalyze()` [frontend/app.js:67-115](): Sends content to appropriate API endpoint
- `showResults(data, isArticle)` [frontend/app.js:117-155](): Displays prediction with animated confidence indicators
- `checkApiHealth()` [frontend/app.js:211-222](): Verifies backend availability on page load

**Sources:** [frontend/index.html:1-174](), [frontend/app.js:1-228]()

### Backend API

The backend is a FastAPI application that orchestrates model loading, request validation, text preprocessing, and model inference. It exposes RESTful endpoints for both single and batch predictions.

```mermaid
graph LR
    Client[\"Client Request\"] --  Validation[\"Request Validation br/ StatementRequest br/ ArticleRequest\"]
    Validation --  Preprocessing[\"Text Preprocessing br/ clean_text() br/ clean_article()\"]
    Preprocessing --  Inference[\"Model Inference br/ RoBERTa or GBC\"]
    Inference --  Response[\"JSON Response br/ PredictionResponse br/ ArticlePredictionResponse\"]
```

**Key Components:**

| Component | Location | Responsibility |
|-----------|----------|----------------|
| FastAPI app | `api/main.py:87-92` | Application initialization with CORS and lifespan management |
| Model loaders | `api/main.py:41-78` | `load_headline_model()`, `load_article_model()` functions |
| Endpoints | `api/main.py:103-283` | Health check, statement prediction, article prediction (single and batch) |
| Text processing | `api/text_processing.py` | Input cleaning and normalization |
| Data models | `api/types.py` | Pydantic models for request/response validation |

**Sources:** [api/main.py:1-290](), [api/text_processing.py:1-50]()

### Machine Learning Models

UnFake employs two distinct machine learning models optimized for different text lengths and characteristics:

| Model | Purpose | Technology | Input Processing | Location |
|-------|---------|------------|------------------|----------|
| Headline Model | Short statements (  256 tokens) | Fine-tuned RoBERTa transformer | AutoTokenizer with padding/truncation | `data/RoBERTa_Classifier/` |
| Article Model | Long-form content | Gradient Boosting Classifier | TF-IDF vectorization | `data/gradient_boosting_classifier.pkl` |

**Model Loading and Device Selection:**

```mermaid
graph TB
    Startup[\"Application Startup br/ lifespan()\"]
    Startup --  LoadHeadline[\"load_headline_model()\"]
    Startup --  LoadArticle[\"load_article_model()\"]
    
    LoadHeadline --  CheckCUDA{\"torch.cuda.is_available()\"}
    CheckCUDA -- |True| GPU[\"Move RoBERTa to GPU br/ DEVICE = cuda\"]
    CheckCUDA -- |False| CPU[\"Keep RoBERTa on CPU br/ DEVICE = cpu\"]
    
    LoadArticle --  LoadPickle[\"Load GBC + Vectorizer br/ pickle.load()\"]
    LoadPickle --  CPUInference[\"CPU Inference br/ scikit-learn\"]
    
    GPU --  ReadyHeadline[\"headline_model.eval()\"]
    CPU --  ReadyHeadline
    ReadyHeadline --  Ready[\"Models Ready\"]
    CPUInference --  Ready
```

**Sources:** [api/main.py:19-78](), [api/main.py:80-84]()

---

## Technology Stack

### Runtime Dependencies

The system requires Python 3.12+ with the following core libraries:

| Library | Version | Purpose |
|---------|---------|---------|
| `fastapi` |  =0.127.0 | Web framework for REST API |
| `uvicorn` |  =0.34.0 | ASGI server for FastAPI |
| `torch` |  =2.9.1 | Deep learning framework for RoBERTa |
| `transformers` |  =4.57.3 | HuggingFace library for transformer models |
| `scikit-learn` |  =1.8.0 | Traditional ML library for Gradient Boosting |
| `nltk` |  =3.9.2 | Natural language processing utilities |
| `pandas` |  =2.3.3 | Data manipulation and analysis |
| `numpy` |  =2.3.5 | Numerical computing |

**Sources:** [pyproject.toml:1-33]()

### Server Configuration

The application runs on Uvicorn ASGI server with the following configuration:

- **Host:** `0.0.0.0` [api/main.py:289]()
- **Port:** `8000` [api/main.py:289]()
- **CORS:** Enabled for all origins [api/main.py:94-100]()
- **Lifespan Management:** Async context manager loads models at startup [api/main.py:80-84]()

**Sources:** [api/main.py:286-290]()

---

## Dual-Model Strategy

UnFake uses two separate models to optimize accuracy across different content types. This architectural decision addresses the distinct characteristics of headlines versus full articles:

### Model Selection Logic

```mermaid
graph TD
    UserInput[\"User Input\"]
    ModeCheck{\"Current Mode?\"}
    
    UserInput --  ModeCheck
    ModeCheck -- |statement| StatementPath[\"Statement Path\"]
    ModeCheck -- |article| ArticlePath[\"Article Path\"]
    
    StatementPath --  CleanText[\"clean_text() br/ Light preprocessing\"]
    ArticlePath --  CleanArticle[\"clean_article() br/ Aggressive preprocessing\"]
    
    CleanText --  Tokenize[\"AutoTokenizer br/ max_length=256\"]
    CleanArticle --  Vectorize[\"TfidfVectorizer br/ transform()\"]
    
    Tokenize --  RoBERTaInfer[\"RoBERTa.forward() br/ torch.no_grad()\"]
    Vectorize --  GBCInfer[\"GBC.predict_proba()\"]
    
    RoBERTaInfer --  Softmax[\"torch.softmax()\"]
    GBCInfer --  Probs[\"Class Probabilities\"]
    
    Softmax --  Response[\"PredictionResponse\"]
    Probs --  Response
```

### Rationale

| Aspect | Headline Model (RoBERTa) | Article Model (Gradient Boosting) |
|--------|--------------------------|-----------------------------------|
| **Input Type** | Statements, headlines, claims | Full articles, paragraphs |
| **Max Length** | 256 tokens [api/main.py:23]() | Unlimited (vectorized to fixed features) |
| **Processing** | Context-aware transformer attention | Bag-of-words TF-IDF features |
| **Inference Speed** | GPU-accelerated (if available) | Fast CPU inference |
| **Training Data** | PolitiFact statements, HuggingFace datasets | True.csv, Fake.csv news articles |

**Sources:** [api/main.py:19-38](), [api/main.py:112-151](), [api/main.py:208-239]()

---

## API Endpoint Overview

The backend exposes five RESTful endpoints for health monitoring and prediction:

### Endpoint Summary

| Endpoint | Method | Purpose | Model Used | Request Body |
|----------|--------|---------|------------|--------------|
| `/` | GET | Health check and model status | N/A | None |
| `/predict` | POST | Single statement classification | RoBERTa | `{\"statement\": \"...\"}` |
| `/predict/batch` | POST | Batch statement classification | RoBERTa | `[\"statement1\", \"statement2\", ...]` |
| `/predict/article` | POST | Single article classification | Gradient Boosting | `{\"article\": \"...\"}` |
| `/predict/article/batch` | POST | Batch article classification | Gradient Boosting | `[\"article1\", \"article2\", ...]` |

### Request/Response Flow

```mermaid
sequenceDiagram
    participant C as \"app.js\"
    participant API as \"FastAPI (main.py)\"
    participant Val as \"Pydantic Validation\"
    participant Proc as \"text_processing\"
    participant Model as \"ML Model\"
    
    C-  API: \"POST /predict or /predict/article\"
    API-  Val: \"Validate with StatementRequest/ArticleRequest\"
    Val--  API: \"Validated data or 400 error\"
    API-  Proc: \"clean_text() or clean_article()\"
    Proc--  API: \"Cleaned text\"
    API-  Model: \"Inference (tokenize + forward or vectorize + predict)\"
    Model--  API: \"Logits/probabilities\"
    API-  API: \"Calculate prediction + confidence\"
    API--  C: \"PredictionResponse/ArticlePredictionResponse JSON\"
```

**Sources:** [api/main.py:103-283](), [frontend/app.js:67-115]()

---

## Data Flow: End-to-End

The following diagram illustrates the complete data transformation pipeline from user input to displayed results:

### Complete Processing Pipeline

```mermaid
graph TD
    Input[\"User Input br/ Textarea\"]
    
    Input --  Validate[\"Frontend Validation br/ !content.trim()\"]
    Validate --  SetLoading[\"setLoading(true) br/ Disable button\"]
    
    SetLoading --  SelectEndpoint{\"Mode?\"}
    SelectEndpoint -- |statement| StmtEndpoint[\"/predict endpoint\"]
    SelectEndpoint -- |article| ArtEndpoint[\"/predict/article endpoint\"]
    
    StmtEndpoint --  StmtProcess[\"Text Processing br/ clean_text()\"]
    ArtEndpoint --  ArtProcess[\"Text Processing br/ clean_article()\"]
    
    StmtProcess --  Tokenization[\"Tokenization br/ AutoTokenizer br/ max_length=256\"]
    ArtProcess --  Vectorization[\"Vectorization br/ TfidfVectorizer.transform()\"]
    
    Tokenization --  TensorBuild[\"Build Tensors br/ input_ids, attention_mask\"]
    TensorBuild --  MoveDevice[\"Move to Device br/ DEVICE (cuda/cpu)\"]
    
    MoveDevice --  RoBERTaForward[\"RoBERTa Forward Pass br/ torch.no_grad()\"]
    RoBERTaForward --  Softmax[\"torch.softmax(logits) br/ torch.argmax()\"]
    
    Vectorization --  GBCPredict[\"GBC.predict_proba() br/ GBC.predict()\"]
    
    Softmax --  BuildResponse1[\"Build PredictionResponse\"]
    GBCPredict --  BuildResponse2[\"Build ArticlePredictionResponse\"]
    
    BuildResponse1 --  JSONResponse[\"JSON Response br/ prediction, confidence, probabilities\"]
    BuildResponse2 --  JSONResponse
    
    JSONResponse --  Frontend[\"app.js receives data\"]
    Frontend --  ShowResults[\"showResults() br/ Update DOM\"]
    
    ShowResults --  AnimateVerdict[\"Animate Verdict Icon br/ fadeIn + bounce\"]
    ShowResults --  AnimateRing[\"Animate Progress Ring br/ strokeDashoffset transition\"]
    ShowResults --  AnimateBars[\"Animate Probability Bars br/ width transitions\"]
    
    AnimateVerdict --  Display[\"Display Results to User\"]
    AnimateRing --  Display
    AnimateBars --  Display
```

**Sources:** [frontend/app.js:67-155](), [api/main.py:112-151](), [api/main.py:208-239]()

---

## Training to Deployment Pipeline

The system follows an offline training, online inference pattern where models are trained in Jupyter notebooks and then loaded into the production API:

### Training Pipeline

```mermaid
graph LR
    subgraph Training[\"Offline Training\"]
        Data[\"Training Data br/ CSV files\"]
        NotebookH[\"train_headline.ipynb\"]
        NotebookA[\"train_article.ipynb\"]
        
        Data --  NotebookH
        Data --  NotebookA
        
        NotebookH --  ExportRoBERTa[\"Export RoBERTa br/ RoBERTa_Classifier/\"]
        NotebookA --  ExportGBC[\"Export GBC + Vectorizer br/ *.pkl files\"]
    end
    
    subgraph Deployment[\"Production Deployment\"]
        Startup[\"uvicorn.run(app)\"]
        Lifespan[\"lifespan() context manager\"]
        
        Startup --  Lifespan
        
        Lifespan --  LoadH[\"load_headline_model()\"]
        Lifespan --  LoadA[\"load_article_model()\"]
        
        LoadH --  HeadlineReady[\"headline_model ready\"]
        LoadA --  ArticleReady[\"article_model ready\"]
        
        HeadlineReady --  Serve[\"Serve Requests\"]
        ArticleReady --  Serve
    end
    
    ExportRoBERTa -.- |\"Read at startup\"| LoadH
    ExportGBC -.- |\"Read at startup\"| LoadA
```

**Model Artifact Locations:**
- **RoBERTa Model:** `HEADLINE_MODEL_DIR = data/RoBERTa_Classifier/` [api/main.py:20-22]()
- **GBC Model:** `ARTICLE_MODEL_DIR = data/gradient_boosting_classifier.pkl` [api/main.py:30-32]()
- **TF-IDF Vectorizer:** `VECTORIZER_DIR = data/tfidf_vectorizer.pkl` [api/main.py:33-35]()

**Sources:** [api/main.py:19-84]()

---

## Error Handling and Validation

The system implements comprehensive error handling across all layers:

### Frontend Error Handling

- **Empty Input Validation:** Checks for whitespace-only content [frontend/app.js:71-76]()
- **Network Errors:** Catches fetch failures and displays user-friendly messages [frontend/app.js:103-111]()
- **API Errors:** Parses HTTP error responses and extracts detail messages [frontend/app.js:94-96]()
- **Visual Feedback:** Displays errors in shake-animated error section [frontend/index.html:151-163]()

### Backend Error Handling

- **Model Loading Errors:** Raises `RuntimeError` if model files not found [api/main.py:44-48](), [api/main.py:63-67]()
- **Request Validation:** Pydantic models enforce schema compliance [api/main.py:113-118](), [api/main.py:209-214]()
- **Batch Size Limits:** Enforces maximum batch sizes (100 statements, 50 articles) [api/main.py:162-163](), [api/main.py:250-251]()
- **HTTP Status Codes:** Returns 400 for bad requests, 503 for service unavailable [api/main.py:115](), [api/main.py:118]()

**Sources:** [api/main.py:41-78](), [api/main.py:112-283](), [frontend/app.js:67-115]()

---

## Deployment Configuration

### Environment Variables and Paths

The application uses relative paths for model loading, allowing flexible deployment:

| Variable | Default | Purpose |
|----------|---------|---------|
| `HEADLINE_MODEL_DIR` | `data/RoBERTa_Classifier/` | RoBERTa model directory |
| `ARTICLE_MODEL_DIR` | `data/gradient_boosting_classifier.pkl` | GBC pickle file |
| `VECTORIZER_DIR` | `data/tfidf_vectorizer.pkl` | TF-IDF vectorizer file |
| `MAX_LENGTH` | `256` | Maximum token length for statements |
| `DEVICE` | Auto-detected | `cuda` if GPU available, else `cpu` |

### Running the Application

The application can be started using:

```bash
python -m api.main
```

Or via uvicorn directly:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

The server will:
1. Load both ML models into memory [api/main.py:80-84]()
2. Move RoBERTa to GPU if available [api/main.py:73]()
3. Start accepting requests on port 8000 [api/main.py:289]()

**Sources:** [api/main.py:19-38](), [api/main.py:286-290]()

---

## Summary

UnFake implements a production-ready fake news detection system with clear architectural separation between frontend, backend, and ML layers. The dual-model approach optimizes for both short statements (using transformer-based RoBERTa) and long articles (using Gradient Boosting with TF-IDF), providing accurate predictions with confidence scores. The system is designed for easy deployment, with automatic GPU detection, comprehensive error handling, and a polished user interface.

For detailed documentation on specific subsystems:
- Backend implementation details: [Backend API](#3)
- Frontend architecture: [Frontend Application](#4)
- Model training procedures: [Training Pipeline](#6)
- Dataset information: [Data and Datasets](#7)

**Sources:** [api/main.py:1-290](), [frontend/app.js:1-228](), [frontend/index.html:1-174]()"])</script><script>self.__next_f.push([1,"18:T2be0,"])</script><script>self.__next_f.push([1,"# Getting Started

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [.env.example](.env.example)
- [.python-version](.python-version)
- [api/main.py](api/main.py)
- [pyproject.toml](pyproject.toml)
- [uv.lock](uv.lock)

 /details 



This page provides step-by-step instructions for installing dependencies, configuring the UnFake system, and running the application for the first time. It covers both the backend API and frontend web interface setup. For detailed information about the API architecture and endpoints, see [Backend API](#3). For training models from scratch, see [Training Pipeline](#6).

---

## Prerequisites

UnFake requires the following system components:

| Component | Requirement | Purpose |
|-----------|-------------|---------|
| Python | 3.12 or higher | Runtime environment |
| pip/uv | Latest version | Package management |
| CUDA (optional) | 11.0+ with compatible GPU | GPU acceleration for RoBERTa model |
| Storage | ~2GB free space | Model artifacts and dependencies |
| RAM | 4GB minimum, 8GB recommended | Model inference |

**Sources:** [pyproject.toml:6](), [.python-version:1]()

---

## Installation Flow

```mermaid
graph TD
    Start[\"User starts installation\"]
    CloneRepo[\"Clone repository\"]
    CheckPython[\"Verify Python 3.12+\"]
    InstallDeps[\"Install dependencies br/ via uv or pip\"]
    CheckModels{\"Pre-trained br/ models br/ available?\"}
    DownloadModels[\"Download model artifacts\"]
    TrainModels[\"Train models br/ (see Training Pipeline)\"]
    ConfigEnv[\"Configure environment br/ (optional)\"]
    VerifySetup[\"Verify installation\"]
    Ready[\"System ready\"]
    
    Start --  CloneRepo
    CloneRepo --  CheckPython
    CheckPython --  InstallDeps
    InstallDeps --  CheckModels
    CheckModels -- |\"Yes\"| DownloadModels
    CheckModels -- |\"No\"| TrainModels
    DownloadModels --  ConfigEnv
    TrainModels --  ConfigEnv
    ConfigEnv --  VerifySetup
    VerifySetup --  Ready
```

**Sources:** [pyproject.toml:1-33]()

---

## Step 1: Clone Repository and Install Dependencies

### 1.1 Clone the Repository

```bash
git clone https://github.com/ayoubdya/Unfake.git
cd Unfake
```

### 1.2 Install Dependencies Using uv (Recommended)

```bash
# Install uv if not already installed
pip install uv

# Install project dependencies
uv sync
```

### 1.3 Alternative: Install Using pip

```bash
pip install -e .
```

The installation includes the following key packages:

| Package | Version | Purpose |
|---------|---------|---------|
| `fastapi` | ≥0.127.0 | Web API framework |
| `uvicorn` | ≥0.34.0 | ASGI server |
| `torch` | ≥2.9.1 | Deep learning framework |
| `transformers` | ≥4.57.3 | RoBERTa model support |
| `scikit-learn` | ≥1.8.0 | Gradient Boosting Classifier |
| `nltk` | ≥3.9.2 | Text preprocessing |

**Sources:** [pyproject.toml:7-26]()

---

## Step 2: Model Artifacts Setup

The application requires two trained models and a TF-IDF vectorizer. You have two options:

### Option A: Use Pre-trained Models (Recommended for Quick Start)

Download the pre-trained model artifacts and place them in the expected directory structure:

```
Unfake/
├── data/
│   ├── RoBERTa_Classifier/      # Headline model directory
│   │   ├── config.json
│   │   ├── model.safetensors
│   │   ├── tokenizer.json
│   │   └── ...
│   ├── gradient_boosting_classifier.pkl  # Article model
│   └── tfidf_vectorizer.pkl              # TF-IDF vectorizer
```

### Option B: Train Models from Scratch

If pre-trained models are not available, train them using the provided notebooks:

1. **Train the headline model:** Open and execute [notebooks/train_headline.ipynb]()
2. **Train the article model:** Open and execute [notebooks/train_article.ipynb]()

For detailed training instructions, see [Training Pipeline](#6).

**Sources:** [api/main.py:20-35]()

---

## Step 3: Model Loading Architecture

The following diagram shows how the FastAPI application discovers and loads model artifacts at startup:

```mermaid
graph TB
    subgraph \"Startup Process\"
        AppStart[\"FastAPI app initialization\"]
        Lifespan[\"lifespan context manager br/ (api/main.py:80-84)\"]
    end
    
    subgraph \"Model Loading Functions\"
        LoadHeadline[\"load_headline_model() br/ (api/main.py:60-77)\"]
        LoadArticle[\"load_article_model() br/ (api/main.py:41-57)\"]
    end
    
    subgraph \"File System Paths\"
        HeadlineDir[\"HEADLINE_MODEL_DIR br/ data/RoBERTa_Classifier\"]
        ArticleFile[\"ARTICLE_MODEL_DIR br/ data/gradient_boosting_classifier.pkl\"]
        VectorizerFile[\"VECTORIZER_DIR br/ data/tfidf_vectorizer.pkl\"]
    end
    
    subgraph \"Loaded Models (Global Variables)\"
        HeadlineModel[\"headline_model br/ AutoModelForSequenceClassification\"]
        Tokenizer[\"tokenizer br/ AutoTokenizer\"]
        ArticleModel[\"article_model br/ GradientBoostingClassifier\"]
        Vectorizer[\"vectorizer br/ TfidfVectorizer\"]
        Device[\"DEVICE br/ torch.device (cuda/cpu)\"]
    end
    
    AppStart --  Lifespan
    Lifespan --  LoadHeadline
    Lifespan --  LoadArticle
    
    LoadHeadline --  HeadlineDir
    HeadlineDir --  HeadlineModel
    HeadlineDir --  Tokenizer
    HeadlineModel --  Device
    
    LoadArticle --  ArticleFile
    LoadArticle --  VectorizerFile
    ArticleFile --  ArticleModel
    VectorizerFile --  Vectorizer
```

**Sources:** [api/main.py:19-84]()

---

## Step 4: Configuration (Optional)

### 4.1 Environment Variables

Create a `.env` file for optional configuration:

```bash
cp .env.example .env
```

Edit `.env` to add any required API keys:

```
GEMINI_API_KEY=your_api_key_here
```

### 4.2 Model Directory Configuration

The default model paths are defined in [api/main.py:20-35](). To use custom paths, modify these constants:

- `HEADLINE_MODEL_DIR`: RoBERTa model directory
- `ARTICLE_MODEL_DIR`: Gradient Boosting Classifier pickle file
- `VECTORIZER_DIR`: TF-IDF vectorizer pickle file

### 4.3 Server Configuration

The server runs on `0.0.0.0:8000` by default. To change this, modify the `uvicorn.run()` call in [api/main.py:289]().

**Sources:** [.env.example:1](), [api/main.py:20-35](), [api/main.py:286-289]()

---

## Step 5: Running the Application

### 5.1 Start the Backend API

Using uvicorn directly:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

Or run the main module:

```bash
python -m api.main
```

Expected startup output:

```
Model loaded from data/RoBERTa_Classifier
Using device: cuda  # or 'cpu' if CUDA unavailable
Article model loaded from data/gradient_boosting_classifier.pkl
Vectorizer loaded from data/tfidf_vectorizer.pkl
INFO:     Started server process [PID]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### 5.2 Access the Frontend

Open a web browser and navigate to:

```
file:///path/to/Unfake/frontend/index.html
```

Or serve the frontend using a simple HTTP server:

```bash
cd frontend
python -m http.server 8080
```

Then access: `http://localhost:8080`

**Sources:** [api/main.py:286-289](), [api/main.py:60-77]()

---

## Step 6: Verification and Testing

### 6.1 API Health Check

Test the root endpoint to verify the backend is running:

```bash
curl http://localhost:8000/
```

Expected response:

```json
{
  \"status\": \"healthy\",
  \"message\": \"Fake News Classifier API is running\",
  \"model_loaded\": true
}
```

### 6.2 Test Statement Prediction

Test the headline/statement endpoint:

```bash
curl -X POST http://localhost:8000/predict \\
  -H \"Content-Type: application/json\" \\
  -d '{\"statement\": \"Scientists discover new planet in solar system\"}'
```

Expected response format:

```json
{
  \"statement\": \"Scientists discover new planet in solar system\",
  \"prediction\": \"Real\",
  \"confidence\": 0.87,
  \"probabilities\": {
    \"fake\": 0.13,
    \"real\": 0.87
  }
}
```

### 6.3 Test Article Prediction

Test the article endpoint:

```bash
curl -X POST http://localhost:8000/predict/article \\
  -H \"Content-Type: application/json\" \\
  -d '{\"article\": \"A new study published in Nature reveals...\"}'
```

### 6.4 API Endpoint Summary

```mermaid
graph LR
    subgraph \"Available Endpoints\"
        Root[\"GET / br/ Health check\"]
        Predict[\"POST /predict br/ Single statement\"]
        PredictBatch[\"POST /predict/batch br/ Batch statements\"]
        Article[\"POST /predict/article br/ Single article\"]
        ArticleBatch[\"POST /predict/article/batch br/ Batch articles\"]
    end
    
    subgraph \"Request Models (api/types.py)\"
        StmtReq[\"StatementRequest br/ {statement: str}\"]
        ArtReq[\"ArticleRequest br/ {article: str}\"]
    end
    
    subgraph \"Response Models (api/types.py)\"
        PredResp[\"PredictionResponse br/ {prediction, confidence, probabilities}\"]
        ArtPredResp[\"ArticlePredictionResponse br/ {prediction, confidence, probabilities}\"]
    end
    
    Predict --  StmtReq
    Predict --  PredResp
    Article --  ArtReq
    Article --  ArtPredResp
```

**Sources:** [api/main.py:103-283](), [api/types.py]()

---

## Troubleshooting

### Model Loading Errors

**Error:** `RuntimeError: Model directory not found`

**Solution:** Ensure model artifacts exist at the expected paths. Check [api/main.py:20-35]() for default paths. Either:
- Download pre-trained models to `data/` directory
- Train models using notebooks in `notebooks/` directory
- Update path constants in `api/main.py`

**Sources:** [api/main.py:63-67](), [api/main.py:44-48]()

---

### CUDA/GPU Issues

**Error:** `CUDA out of memory`

**Solution:** The RoBERTa model automatically falls back to CPU if CUDA is unavailable. To force CPU usage:

```python
# Modify DEVICE in api/main.py:24
DEVICE = torch.device(\"cpu\")
```

**Sources:** [api/main.py:24](), [api/main.py:73]()

---

### Port Already in Use

**Error:** `Address already in use`

**Solution:** Change the port in the uvicorn command:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8001
```

**Sources:** [api/main.py:289]()

---

### CORS Issues with Frontend

**Error:** Frontend cannot connect to API

**Solution:** The API has CORS enabled for all origins by default. Verify:
1. Backend is running on the expected port
2. Frontend is using the correct API URL in [frontend/app.js]()
3. Check browser console for specific CORS errors

**Sources:** [api/main.py:94-100]()

---

### Missing Dependencies

**Error:** `ModuleNotFoundError: No module named 'X'`

**Solution:** Reinstall dependencies:

```bash
uv sync --force
# or
pip install -e . --force-reinstall
```

**Sources:** [pyproject.toml:7-26]()

---

## Next Steps

After successfully installing and running UnFake:

- **Explore the API:** See [API Endpoints](#3.1) for detailed endpoint documentation
- **Understand the models:** Read [Machine Learning Models](#5) to learn about the dual-model architecture
- **Train custom models:** Follow [Training Pipeline](#6) to train models on custom datasets
- **Deploy to production:** See [Deployment and Configuration](#8) for production deployment guidance

**Sources:** Overall system architecture from diagrams"])</script><script>self.__next_f.push([1,"19:T45b2,"])</script><script>self.__next_f.push([1,"# Backend API

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)

 /details 



## Purpose and Scope

This document provides a comprehensive overview of the UnFake backend API, built with FastAPI. The backend serves as the orchestration layer between the web frontend and machine learning models, exposing HTTP endpoints for fake news classification. This page covers the API architecture, core components, application lifecycle, and configuration. For detailed information about specific endpoints and their request/response formats, see [API Endpoints](#3.1). For text preprocessing implementation, see [Text Processing](#3.2). For model loading mechanics, see [Model Loading and Lifecycle](#3.3).

**Sources:** [api/main.py:1-290]()

---

## Architecture Overview

The backend API is implemented in [api/main.py]() as a FastAPI application that serves as the central request handler. The architecture consists of three primary modules:

| Module | File | Purpose |
|--------|------|---------|
| Main Application | `api/main.py` | FastAPI app, endpoints, model loading, lifecycle management |
| Type Definitions | `api/types.py` | Pydantic models for request/response validation |
| Text Processing | `api/text_processing.py` | Text cleaning and preprocessing functions |

```mermaid
graph TB
    subgraph \"api/main.py\"
        app[\"FastAPI app\"]
        lifespan[\"lifespan() br/ Context Manager\"]
        load_headline[\"load_headline_model()\"]
        load_article[\"load_article_model()\"]
        
        root[\"GET / br/ root()\"]
        predict[\"POST /predict br/ predict()\"]
        predict_batch[\"POST /predict/batch br/ predict_batch()\"]
        article[\"POST /predict/article br/ predict_article()\"]
        article_batch[\"POST /predict/article/batch br/ predict_article_batch()\"]
    end
    
    subgraph \"api/types.py\"
        StatementRequest[\"StatementRequest\"]
        ArticleRequest[\"ArticleRequest\"]
        PredictionResponse[\"PredictionResponse\"]
        ArticlePredictionResponse[\"ArticlePredictionResponse\"]
    end
    
    subgraph \"api/text_processing.py\"
        clean_text[\"clean_text()\"]
        clean_article[\"clean_article()\"]
    end
    
    subgraph \"Global State\"
        headline_model[\"headline_model br/ AutoModelForSequenceClassification\"]
        tokenizer[\"tokenizer br/ AutoTokenizer\"]
        article_model[\"article_model br/ GradientBoostingClassifier\"]
        vectorizer[\"vectorizer br/ TfidfVectorizer\"]
    end
    
    app --  lifespan
    lifespan --  load_headline
    lifespan --  load_article
    
    load_headline --  headline_model
    load_headline --  tokenizer
    load_article --  article_model
    load_article --  vectorizer
    
    predict --  StatementRequest
    predict --  clean_text
    predict --  headline_model
    predict --  tokenizer
    predict --  PredictionResponse
    
    article --  ArticleRequest
    article --  clean_article
    article --  article_model
    article --  vectorizer
    article --  ArticlePredictionResponse
    
    predict_batch --  predict
    article_batch --  article
```

**Sources:** [api/main.py:1-290](), [api/types.py:1-58](), [api/text_processing.py:1-22]()

---

## Core Components

### FastAPI Application

The FastAPI application is instantiated at [api/main.py:87-92]() with configuration metadata:

```python
app = FastAPI(
  title=\"Fake News Classifier API\",
  description=\"API for classifying statements as fake or real news using a fine-tuned RoBERTa model.\",
  version=\"1.0.0\",
  lifespan=lifespan,
)
```

The `lifespan` parameter connects to the asynchronous context manager that manages model loading and cleanup.

**Sources:** [api/main.py:87-92]()

### CORS Middleware

CORS middleware is configured at [api/main.py:94-100]() to allow cross-origin requests from any domain:

| Configuration | Value | Purpose |
|---------------|-------|---------|
| `allow_origins` | `[\"*\"]` | Accept requests from any origin |
| `allow_credentials` | `True` | Allow cookies and authentication headers |
| `allow_methods` | `[\"*\"]` | Accept all HTTP methods |
| `allow_headers` | `[\"*\"]` | Accept all request headers |

This permissive configuration is suitable for development and internal deployments. For production internet-facing deployments, `allow_origins` should be restricted to specific frontend domains.

**Sources:** [api/main.py:94-100]()

### Global Model State

The API maintains four global variables for model state, initialized to `None` and populated during application startup:

| Variable | Type | Source | Line |
|----------|------|--------|------|
| `headline_model` | `AutoModelForSequenceClassification` | RoBERTa transformer | [api/main.py:26]() |
| `tokenizer` | `AutoTokenizer` | RoBERTa tokenizer | [api/main.py:27]() |
| `article_model` | `GradientBoostingClassifier` | Scikit-learn model | [api/main.py:37]() |
| `vectorizer` | `TfidfVectorizer` | TF-IDF vectorizer | [api/main.py:38]() |

These globals enable efficient access to models across request handlers without reloading. The type hints use union syntax (`| None`) to indicate nullable state before startup completes.

**Sources:** [api/main.py:26-38]()

---

## Application Lifecycle

### Lifespan Context Manager

```mermaid
sequenceDiagram
    participant Server as \"Uvicorn Server\"
    participant Lifespan as \"lifespan(app)\"
    participant LoadH as \"load_headline_model()\"
    participant LoadA as \"load_article_model()\"
    participant Models as \"Global Model State\"
    
    Server-  Lifespan: Application startup
    Lifespan-  LoadH: Call function
    LoadH-  LoadH: Check HEADLINE_MODEL_DIR exists
    LoadH-  LoadH: Load AutoTokenizer
    LoadH-  LoadH: Load AutoModelForSequenceClassification
    LoadH-  LoadH: Move to DEVICE (GPU/CPU)
    LoadH-  LoadH: Set to eval mode
    LoadH-  Models: Update headline_model, tokenizer
    
    Lifespan-  LoadA: Call function
    LoadA-  LoadA: Check files exist
    LoadA-  LoadA: Unpickle gradient_boosting_classifier.pkl
    LoadA-  LoadA: Unpickle tfidf_vectorizer.pkl
    LoadA-  Models: Update article_model, vectorizer
    
    Lifespan-  Server: Yield (models loaded)
    
    Note over Server,Models: Application handles requests
    
    Server-  Lifespan: Application shutdown
    Lifespan-  Server: Cleanup complete
```

The `lifespan` function at [api/main.py:80-84]() is an asynchronous context manager decorated with `@asynccontextmanager` that orchestrates model initialization:

1. **Startup Phase**: Sequentially calls `load_headline_model()` and `load_article_model()`
2. **Running Phase**: Yields control to the FastAPI application for request handling
3. **Shutdown Phase**: Implicit cleanup (Python garbage collection handles model deallocation)

The sequential loading ensures both models are available before the first request is processed. Any exceptions during loading propagate up and prevent application startup.

**Sources:** [api/main.py:80-84]()

### Model Loading Functions

#### `load_headline_model()`

Located at [api/main.py:60-77](), this function:

1. Validates `HEADLINE_MODEL_DIR` exists ([api/main.py:63-67]())
2. Loads the tokenizer using `AutoTokenizer.from_pretrained()` ([api/main.py:69]())
3. Loads the model using `AutoModelForSequenceClassification.from_pretrained()` ([api/main.py:70-72]())
4. Moves the model to the appropriate device via `.to(DEVICE)` ([api/main.py:73]())
5. Sets the model to evaluation mode via `.eval()` ([api/main.py:74]())

The `DEVICE` constant at [api/main.py:24]() is determined by `torch.cuda.is_available()`, automatically selecting GPU acceleration when available.

#### `load_article_model()`

Located at [api/main.py:41-57](), this function:

1. Validates both `ARTICLE_MODEL_DIR` and `VECTORIZER_DIR` exist ([api/main.py:44-48]())
2. Unpickles the Gradient Boosting Classifier from disk ([api/main.py:50-51]())
3. Unpickles the TF-IDF vectorizer from disk ([api/main.py:53-54]())

The article model runs on CPU regardless of GPU availability since scikit-learn models don't utilize GPU acceleration.

**Sources:** [api/main.py:41-77]()

---

## Configuration and Constants

### Directory Paths

| Constant | Value | Purpose |
|----------|-------|---------|
| `HEADLINE_MODEL_DIR` | `../data/RoBERTa_Classifier` | Directory containing RoBERTa model and tokenizer files |
| `ARTICLE_MODEL_DIR` | `../data/gradient_boosting_classifier.pkl` | Path to serialized Gradient Boosting model |
| `VECTORIZER_DIR` | `../data/tfidf_vectorizer.pkl` | Path to serialized TF-IDF vectorizer |

All paths are constructed relative to `api/main.py` using `os.path.join()` with `os.path.dirname(__file__)` ([api/main.py:20-35]()).

### Processing Parameters

| Constant | Value | Purpose |
|----------|-------|---------|
| `MAX_LENGTH` | `256` | Maximum token sequence length for RoBERTa tokenization |
| `DEVICE` | `torch.device(...)` | Computation device (CUDA if available, else CPU) |

The `MAX_LENGTH` of 256 tokens balances processing speed with context preservation for headline-length inputs.

**Sources:** [api/main.py:19-35]()

---

## Request/Response Flow

### Statement Prediction Flow

```mermaid
flowchart TD
    Request[\"POST /predict br/ {statement: str}\"]
    Validate[\"Validate StatementRequest br/ (Pydantic)\"]
    CheckModel{\"headline_model br/ is not None?\"}
    CheckEmpty{\"statement.strip() br/ not empty?\"}
    CleanText[\"clean_text(statement) br/ api/text_processing.py\"]
    Tokenize[\"tokenizer(..., br/ max_length=256, br/ padding='max_length')\"]
    ToDevice[\"Move tensors br/ to DEVICE\"]
    Inference[\"with torch.no_grad(): br/   outputs = headline_model(...)\"]
    Softmax[\"probs = torch.softmax(outputs.logits, dim=1)\"]
    ArgMax[\"pred_class = torch.argmax(probs, dim=1).item()\"]
    BuildResp[\"PredictionResponse( br/ statement, prediction, br/ confidence, probabilities)\"]
    Return[\"Return JSON response\"]
    Error503[\"HTTPException(503, br/ 'Model not loaded')\"]
    Error400[\"HTTPException(400, br/ 'Statement cannot be empty')\"]
    
    Request --  Validate
    Validate --  CheckModel
    CheckModel -- |No| Error503
    CheckModel -- |Yes| CheckEmpty
    CheckEmpty -- |No| Error400
    CheckEmpty -- |Yes| CleanText
    CleanText --  Tokenize
    Tokenize --  ToDevice
    ToDevice --  Inference
    Inference --  Softmax
    Softmax --  ArgMax
    ArgMax --  BuildResp
    BuildResp --  Return
```

The prediction endpoint at [api/main.py:112-151]() follows this sequence:

1. **Validation**: Pydantic validates the request body against `StatementRequest`
2. **Model Check**: Verifies `headline_model` and `tokenizer` are loaded ([api/main.py:114-115]())
3. **Empty Check**: Ensures the statement is not empty after stripping ([api/main.py:117-118]())
4. **Preprocessing**: Calls `clean_text()` to normalize the input ([api/main.py:120]())
5. **Tokenization**: Converts text to token IDs with padding/truncation ([api/main.py:122-130]())
6. **Device Transfer**: Moves input tensors to GPU/CPU ([api/main.py:132-133]())
7. **Inference**: Runs forward pass in no-grad context ([api/main.py:135-142]())
8. **Post-processing**: Applies softmax, determines class, extracts probabilities
9. **Response**: Returns `PredictionResponse` with results ([api/main.py:146-151]())

**Sources:** [api/main.py:112-151]()

### Article Prediction Flow

```mermaid
flowchart TD
    Request[\"POST /predict/article br/ {article: str}\"]
    Validate[\"Validate ArticleRequest br/ (Pydantic)\"]
    CheckModel{\"article_model and br/ vectorizer not None?\"}
    CheckEmpty{\"article.strip() br/ not empty?\"}
    CleanArticle[\"clean_article(article) br/ api/text_processing.py\"]
    Vectorize[\"vectorizer.transform([cleaned_text])\"]
    PredictProba[\"probabilities = br/ article_model.predict_proba(...)\"]
    Predict[\"pred_class = br/ article_model.predict(...)\"]
    ExtractProbs[\"Extract prob_fake, prob_real br/ confidence = probabilities[pred_class]\"]
    BuildResp[\"ArticlePredictionResponse( br/ article[:500], prediction, br/ confidence, probabilities)\"]
    Return[\"Return JSON response\"]
    Error503[\"HTTPException(503, br/ 'Article model not loaded')\"]
    Error400[\"HTTPException(400, br/ 'Article cannot be empty')\"]
    
    Request --  Validate
    Validate --  CheckModel
    CheckModel -- |No| Error503
    CheckModel -- |Yes| CheckEmpty
    CheckEmpty -- |No| Error400
    CheckEmpty -- |Yes| CleanArticle
    CleanArticle --  Vectorize
    Vectorize --  PredictProba
    PredictProba --  Predict
    Predict --  ExtractProbs
    ExtractProbs --  BuildResp
    BuildResp --  Return
```

The article endpoint at [api/main.py:208-239]() implements a similar but distinct flow:

1. **Validation**: Pydantic validates against `ArticleRequest`
2. **Model Check**: Verifies `article_model` and `vectorizer` are loaded ([api/main.py:210-211]())
3. **Empty Check**: Ensures the article is not empty ([api/main.py:213-214]())
4. **Preprocessing**: Calls `clean_article()` for aggressive text cleaning ([api/main.py:216]())
5. **Vectorization**: Transforms text to TF-IDF features ([api/main.py:218]())
6. **Inference**: Calls both `predict_proba()` and `predict()` ([api/main.py:220-221]())
7. **Post-processing**: Extracts class probabilities and confidence ([api/main.py:223-225]())
8. **Response**: Returns `ArticlePredictionResponse` with truncated article text ([api/main.py:229-239]())

Note that the article response truncates long articles to 500 characters in the returned JSON ([api/main.py:230-232]()).

**Sources:** [api/main.py:208-239]()

### Batch Processing

Both statement and article endpoints have batch variants ([api/main.py:154-205]() and [api/main.py:242-283]()):

| Endpoint | Batch Limit | Iteration Strategy | Error Handling |
|----------|-------------|-------------------|----------------|
| `/predict/batch` | 100 statements | Sequential loop, individual tokenization | Continues on empty statements, appends error to results |
| `/predict/article/batch` | 50 articles | Sequential loop, individual vectorization | Continues on empty articles, appends error to results |

Batch endpoints validate list length before processing and iterate through each item, maintaining individual error isolation. This ensures that one malformed input doesn't abort the entire batch.

**Sources:** [api/main.py:154-205](), [api/main.py:242-283]()

---

## Error Handling

### HTTP Exception Patterns

The API uses `fastapi.HTTPException` for all error responses:

| Status Code | Condition | Endpoint(s) |
|-------------|-----------|-------------|
| 400 Bad Request | Empty statement/article after stripping | All prediction endpoints |
| 400 Bad Request | Empty list in batch request | Batch endpoints |
| 400 Bad Request | Batch size exceeds limit (100/50) | Batch endpoints |
| 503 Service Unavailable | Model not loaded (startup failure) | All prediction endpoints |

Example from [api/main.py:114-118]():

```python
if headline_model is None or tokenizer is None:
  raise HTTPException(status_code=503, detail=\"Model not loaded\")

if not request.statement.strip():
  raise HTTPException(status_code=400, detail=\"Statement cannot be empty\")
```

### Startup Errors

Model loading functions raise `RuntimeError` if required files are missing ([api/main.py:44-48]() and [api/main.py:63-67]()):

```python
if not os.path.exists(HEADLINE_MODEL_DIR):
  raise RuntimeError(
    f\"Model directory not found: {HEADLINE_MODEL_DIR}. \"
    \"Please train the model first using the train.ipynb notebook.\"
  )
```

These exceptions propagate to the `lifespan` manager and prevent application startup, ensuring the server never accepts requests with unloaded models.

**Sources:** [api/main.py:44-48](), [api/main.py:63-67](), [api/main.py:114-118](), [api/main.py:210-214]()

---

## Type System

### Request Models

Pydantic models in [api/types.py]() provide automatic validation, serialization, and documentation:

#### `StatementRequest`

Defined at [api/types.py:4-12](), accepts a single field:
- `statement: str` - The text to classify

Includes a `Config` class with `json_schema_extra` for OpenAPI documentation.

#### `ArticleRequest`

Defined at [api/types.py:15-23](), accepts a single field:
- `article: str` - The article text to classify

Also includes example configuration for API documentation.

### Response Models

#### `PredictionResponse`

Defined at [api/types.py:26-40](), returns:
- `statement: str` - The original input statement
- `prediction: str` - Classification result (\"Real\" or \"Fake\")
- `confidence: float` - Confidence score (rounded to 2 decimals)
- `probabilities: dict` - Dictionary with \"fake\" and \"real\" probability values

#### `ArticlePredictionResponse`

Defined at [api/types.py:43-57](), returns the same structure but with `article` instead of `statement`. The article field is truncated to 500 characters in responses.

All response fields are rounded to 2 decimal places for consistent precision ([api/main.py:149-150](), [api/main.py:234-238]()).

**Sources:** [api/types.py:1-58](), [api/main.py:146-151](), [api/main.py:229-239]()

---

## Health Check Endpoint

The root endpoint at [api/main.py:103-109]() provides a simple health check:

```python
@app.get(\"/\")
async def root():
  return {
    \"status\": \"healthy\",
    \"message\": \"Fake News Classifier API is running\",
    \"model_loaded\": headline_model is not None,
  }
```

This endpoint returns:
- `status`: Always \"healthy\" if the server is running
- `message`: Descriptive string
- `model_loaded`: Boolean indicating if the headline model has been successfully loaded

The frontend uses this endpoint to verify backend availability before enabling user interactions.

**Sources:** [api/main.py:103-109]()"])</script><script>self.__next_f.push([1,"1a:T3297,"])</script><script>self.__next_f.push([1,"# API Endpoints

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)

 /details 



## Purpose and Scope

This document provides comprehensive documentation of all REST API endpoints exposed by the UnFake backend API. The FastAPI application in [api/main.py]() implements five endpoints across three functional categories: health monitoring, statement classification, and article classification. Each endpoint accepts JSON payloads validated by Pydantic models and returns structured prediction results.

For detailed information about text preprocessing applied before inference, see [Text Processing](#3.2). For information about model loading and initialization, see [Model Loading and Lifecycle](#3.3). For in-depth documentation of individual endpoint groups, see [Statement Prediction Endpoints](#3.1.1) and [Article Prediction Endpoints](#3.1.2).

**Sources**: [api/main.py:1-289]()

---

## Endpoint Overview

The API exposes five endpoints organized by functionality:

| Endpoint | Method | Purpose | Request Model | Response Model | Batch Size Limit |
|----------|--------|---------|---------------|----------------|------------------|
| `/` | GET | Health check and API status | None | JSON object | N/A |
| `/predict` | POST | Single statement classification | `StatementRequest` | `PredictionResponse` | 1 |
| `/predict/batch` | POST | Batch statement classification | List of strings | JSON with results array | 100 |
| `/predict/article` | POST | Single article classification | `ArticleRequest` | `ArticlePredictionResponse` | 1 |
| `/predict/article/batch` | POST | Batch article classification | List of strings | JSON with results array | 50 |

All endpoints support CORS with wildcard origins configured via middleware [api/main.py:94-100]().

**Sources**: [api/main.py:103-283]()

---

## API Architecture

```mermaid
graph TB
    subgraph \"FastAPI Application [api/main.py]\"
        App[\"FastAPI app br/ (title: 'Fake News Classifier API')\"]
        CORSMid[\"CORSMiddleware br/ [main.py:94-100]\"]
        Lifespan[\"lifespan() br/ [main.py:80-84]\"]
    end
    
    subgraph \"Endpoints [api/main.py]\"
        RootEP[\"GET / br/ root()[main.py:103-109]\"]
        PredictEP[\"POST /predict br/ predict()[main.py:112-151]\"]
        PredictBatchEP[\"POST /predict/batch br/ predict_batch()[main.py:154-205]\"]
        ArticleEP[\"POST /predict/article br/ predict_article()[main.py:208-239]\"]
        ArticleBatchEP[\"POST /predict/article/batch br/ predict_article_batch()[main.py:242-283]\"]
    end
    
    subgraph \"Pydantic Models [api/types.py]\"
        StmtReq[\"StatementRequest br/ [types.py:4-12]\"]
        ArtReq[\"ArticleRequest br/ [types.py:15-23]\"]
        PredResp[\"PredictionResponse br/ [types.py:26-40]\"]
        ArtPredResp[\"ArticlePredictionResponse br/ [types.py:43-57]\"]
    end
    
    subgraph \"Text Processing [api/text_processing.py]\"
        CleanText[\"clean_text() br/ [text_processing.py:4-9]\"]
        CleanArticle[\"clean_article() br/ [text_processing.py:12-21]\"]
    end
    
    subgraph \"ML Models [Global Variables]\"
        HeadlineModelVar[\"headline_model br/ [main.py:26]\"]
        TokenizerVar[\"tokenizer br/ [main.py:27]\"]
        ArticleModelVar[\"article_model br/ [main.py:37]\"]
        VectorizerVar[\"vectorizer br/ [main.py:38]\"]
    end
    
    App --  CORSMid
    App --  Lifespan
    
    Lifespan -.- |\"loads\"| HeadlineModelVar
    Lifespan -.- |\"loads\"| TokenizerVar
    Lifespan -.- |\"loads\"| ArticleModelVar
    Lifespan -.- |\"loads\"| VectorizerVar
    
    PredictEP -- |\"validates\"| StmtReq
    PredictEP -- |\"preprocesses\"| CleanText
    PredictEP -- |\"uses\"| HeadlineModelVar
    PredictEP -- |\"uses\"| TokenizerVar
    PredictEP -- |\"returns\"| PredResp
    
    ArticleEP -- |\"validates\"| ArtReq
    ArticleEP -- |\"preprocesses\"| CleanArticle
    ArticleEP -- |\"uses\"| ArticleModelVar
    ArticleEP -- |\"uses\"| VectorizerVar
    ArticleEP -- |\"returns\"| ArtPredResp
    
    PredictBatchEP -- |\"iterates over\"| PredictEP
    ArticleBatchEP -- |\"iterates over\"| ArticleEP
    
    CORSMid -- |\"protects\"| RootEP
    CORSMid -- |\"protects\"| PredictEP
    CORSMid -- |\"protects\"| PredictBatchEP
    CORSMid -- |\"protects\"| ArticleEP
    CORSMid -- |\"protects\"| ArticleBatchEP
```

**Endpoint-to-Code-Entity Mapping**: This diagram maps each endpoint to its implementing function in the codebase. The `root()` function at [api/main.py:103-109]() handles health checks, while prediction endpoints at [api/main.py:112-283]() implement classification logic. Each prediction endpoint uses specific Pydantic models from [api/types.py]() for validation and preprocessing functions from [api/text_processing.py]() before invoking the appropriate ML model loaded by the `lifespan()` context manager.

**Sources**: [api/main.py:1-289](), [api/types.py:1-57](), [api/text_processing.py:1-21]()

---

## Request/Response Flow

```mermaid
sequenceDiagram
    participant Client
    participant CORS[\"CORSMiddleware br/ [main.py:94]\"]
    participant Endpoint[\"Endpoint Function br/ [main.py:112+]\"]
    participant Pydantic[\"Pydantic Model br/ [types.py]\"]
    participant TextProc[\"Text Processing br/ [text_processing.py]\"]
    participant Model[\"ML Model br/ (headline_model or article_model)\"]
    
    Client-  CORS: \"HTTP POST with JSON\"
    CORS-  Endpoint: \"Forward request\"
    
    Endpoint-  Endpoint: \"Check model loaded br/ [main.py:114-115]\"
    
    alt \"Model not loaded\"
        Endpoint--  Client: \"HTTPException(503)\"
    end
    
    Endpoint-  Pydantic: \"Auto-validate request\"
    
    alt \"Validation fails\"
        Pydantic--  Client: \"HTTPException(422)\"
    end
    
    Endpoint-  Endpoint: \"Check empty input br/ [main.py:117-118]\"
    
    alt \"Empty input\"
        Endpoint--  Client: \"HTTPException(400)\"
    end
    
    Endpoint-  TextProc: \"clean_text() or clean_article()\"
    TextProc--  Endpoint: \"cleaned_text\"
    
    Endpoint-  Model: \"Vectorize/Tokenize + Predict\"
    Model--  Endpoint: \"predictions + probabilities\"
    
    Endpoint-  Endpoint: \"Format response br/ [main.py:146-151]\"
    Endpoint-  CORS: \"Return response model\"
    CORS--  Client: \"JSON with CORS headers\"
```

**Request Processing Pipeline**: All prediction endpoints follow this standardized flow. After CORS validation, endpoints check model availability [api/main.py:114-115, 210-211](), validate inputs using Pydantic models [api/types.py](), verify non-empty content [api/main.py:117-118, 213-214](), preprocess text [api/text_processing.py](), perform inference using the appropriate model, and return structured responses. Error conditions return appropriate HTTP status codes: 503 for model unavailability, 422 for validation errors, and 400 for empty inputs.

**Sources**: [api/main.py:112-151, 208-239](), [api/types.py:1-57](), [api/text_processing.py:4-21]()

---

## Data Models

All request and response payloads are validated using Pydantic models defined in [api/types.py](). These models provide automatic validation, serialization, and documentation generation.

### Request Models

| Model | Field | Type | Description | Example Schema Line |
|-------|-------|------|-------------|---------------------|
| `StatementRequest` | `statement` | `str` | Text statement to classify | [types.py:4-5]() |
| `ArticleRequest` | `article` | `str` | Article text to classify | [types.py:15-16]() |

Both models include `Config` classes with `json_schema_extra` providing example payloads for API documentation [api/types.py:7-12, 18-23]().

### Response Models

| Model | Fields | Description | Example Line |
|-------|--------|-------------|--------------|
| `PredictionResponse` | `statement`, `prediction`, `confidence`, `probabilities` | Statement classification results | [types.py:26-30]() |
| `ArticlePredictionResponse` | `article`, `prediction`, `confidence`, `probabilities` | Article classification results | [types.py:43-47]() |

Both response models share the same structure:
- `prediction`: String value \"Real\" or \"Fake\"
- `confidence`: Float between 0 and 1 representing maximum probability
- `probabilities`: Dictionary with `fake` and `real` keys containing class probabilities

Response construction occurs at [api/main.py:146-151]() for statements and [api/main.py:229-239]() for articles, where probabilities are rounded to 2 decimal places.

**Sources**: [api/types.py:1-57](), [api/main.py:146-151, 229-239]()

---

## Common Patterns

### Input Validation

All prediction endpoints implement multi-stage validation:

1. **Model availability check**: Ensures models loaded at startup [api/main.py:114-115, 156-157, 210-211, 244-245]()
   ```python
   if headline_model is None or tokenizer is None:
       raise HTTPException(status_code=503, detail=\"Model not loaded\")
   ```

2. **Pydantic validation**: Automatic validation via function parameter type hints
3. **Empty content check**: Explicit validation for non-empty strings [api/main.py:117-118, 213-214]()
   ```python
   if not request.statement.strip():
       raise HTTPException(status_code=400, detail=\"Statement cannot be empty\")
   ```

### Batch Processing

Batch endpoints implement size limits and graceful error handling:

| Endpoint | Max Batch Size | Error Handling |
|----------|----------------|----------------|
| `/predict/batch` | 100 statements | Individual failures append error objects [api/main.py:167-169]() |
| `/predict/article/batch` | 50 articles | Individual failures append error objects [api/main.py:255-257]() |

Batch size validation occurs at [api/main.py:162-163, 250-251](). When individual items fail, the response includes an error field instead of predictions:
```python
results.append({\"statement\": statement, \"error\": \"Empty statement\"})
```

This design ensures partial batch completion rather than full batch failure.

### Response Truncation

Article responses truncate long text for performance [api/main.py:230-232, 273]():
```python
article=request.article[:500] + \"...\" if len(request.article)   500 else request.article
```

This limits response payload size while preserving full text for inference.

**Sources**: [api/main.py:114-118, 156-169, 210-214, 230-232, 244-257]()

---

## Health Check Endpoint

**Endpoint**: `GET /`  
**Function**: `root()` at [api/main.py:103-109]()

Returns API status and model loading state:

```json
{
  \"status\": \"healthy\",
  \"message\": \"Fake News Classifier API is running\",
  \"model_loaded\": true
}
```

The `model_loaded` field checks the `headline_model` global variable [api/main.py:108](). This endpoint requires no authentication and is suitable for container health checks and monitoring.

**Sources**: [api/main.py:103-109]()

---

## Prediction Endpoint Groups

The API organizes prediction endpoints into two functional groups based on text type:

### Statement Prediction
- `POST /predict` - Single statement classification
- `POST /predict/batch` - Batch statement classification (up to 100 items)

These endpoints use the RoBERTa transformer model with tokenization at max_length=256 [api/main.py:23, 125](). For detailed documentation including tokenization parameters, inference flow, and probability calculations, see [Statement Prediction Endpoints](#3.1.1).

### Article Prediction
- `POST /predict/article` - Single article classification
- `POST /predict/article/batch` - Batch article classification (up to 50 items)

These endpoints use the Gradient Boosting Classifier with TF-IDF vectorization. For detailed documentation including vectorization parameters, model inference, and batch processing behavior, see [Article Prediction Endpoints](#3.1.2).

**Sources**: [api/main.py:112-283]()

---

## CORS Configuration

CORS middleware configuration at [api/main.py:94-100]() enables cross-origin requests from any domain:

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=[\"*\"],
    allow_credentials=True,
    allow_methods=[\"*\"],
    allow_headers=[\"*\"],
)
```

This permissive configuration supports development and allows the frontend to communicate with the backend regardless of deployment configuration. For production deployments, consider restricting `allow_origins` to specific domains.

**Sources**: [api/main.py:94-100]()

---

## Application Metadata

The FastAPI application initializes with metadata used for automatic documentation generation [api/main.py:87-92]():

- **title**: \"Fake News Classifier API\"
- **description**: \"API for classifying statements as fake or real news using a fine-tuned RoBERTa model.\"
- **version**: \"1.0.0\"
- **lifespan**: `lifespan` context manager for startup/shutdown hooks

These values populate the auto-generated OpenAPI documentation available at `/docs` and `/redoc` endpoints when the server runs.

**Sources**: [api/main.py:87-92]()"])</script><script>self.__next_f.push([1,"1b:T3a4e,"])</script><script>self.__next_f.push([1,"# Statement Prediction Endpoints

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)

 /details 



## Purpose and Scope

This document covers the `/predict` and `/predict/batch` endpoints in the UnFake API, which analyze short statements and headlines for fake news detection using a fine-tuned RoBERTa transformer model. These endpoints are optimized for text inputs up to 256 tokens and return binary classification results (Real/Fake) with confidence scores.

For article analysis using the Gradient Boosting model, see [Article Prediction Endpoints](#3.1.2). For details on text preprocessing functions, see [Text Processing](#3.2). For information about model loading and device selection, see [Model Loading and Lifecycle](#3.3).

**Sources:** [api/main.py:1-289]()

---

## Endpoint Overview

The statement prediction functionality is exposed through two HTTP POST endpoints:

| Endpoint | Purpose | Max Batch Size | Model Used |
|----------|---------|----------------|------------|
| `/predict` | Single statement classification | 1 | RoBERTa (fine-tuned) |
| `/predict/batch` | Batch statement classification | 100 | RoBERTa (fine-tuned) |

Both endpoints use the same underlying inference pipeline but differ in request/response structure and batch processing capabilities.

**Sources:** [api/main.py:112-205]()

---

## Single Statement Prediction

### Endpoint: POST /predict

The `/predict` endpoint analyzes a single statement and returns a structured prediction response.

#### Request Format

The endpoint accepts a JSON request body conforming to the `StatementRequest` model:

```json
{
  \"statement\": \"Scientists confirm that regular exercise improves cardiovascular health.\"
}
```

The `StatementRequest` class is defined in [api/types.py:4-12]() with a single required field:
- `statement` (str): The text to analyze

**Sources:** [api/types.py:4-12](), [api/main.py:112-113]()

#### Response Format

Returns a `PredictionResponse` object with the following structure:

```json
{
  \"statement\": \"Scientists confirm that regular exercise improves cardiovascular health.\",
  \"prediction\": \"Real\",
  \"confidence\": 0.92,
  \"probabilities\": {
    \"fake\": 0.08,
    \"real\": 0.92
  }
}
```

| Field | Type | Description |
|-------|------|-------------|
| `statement` | str | Original input text (echoed back) |
| `prediction` | str | Classification result: \"Real\" or \"Fake\" |
| `confidence` | float | Confidence score (0.0-1.0) for the predicted class |
| `probabilities` | dict | Probability scores for both classes (fake and real) |

The `PredictionResponse` model is defined in [api/types.py:26-40]().

**Sources:** [api/types.py:26-40](), [api/main.py:146-151]()

#### Processing Flow

```mermaid
graph TD
    Request[\"POST /predict br/ StatementRequest\"] --  Validate[\"Validation br/ [main.py:114-118]\"]
    Validate --  CheckModel{\"headline_model br/ is not None?\"}
    CheckModel -- |No| Error503[\"HTTPException br/ 503: Model not loaded\"]
    CheckModel -- |Yes| CheckEmpty{\"statement.strip() br/ not empty?\"}
    CheckEmpty -- |No| Error400[\"HTTPException br/ 400: Empty statement\"]
    CheckEmpty -- |Yes| Clean[\"clean_text() br/ [text_processing.py:4-9]\"]
    Clean --  Tokenize[\"tokenizer() br/ [main.py:122-130] br/ max_length=256\"]
    Tokenize --  MoveDevice[\"input_ids.to(DEVICE) br/ attention_mask.to(DEVICE) br/ [main.py:132-133]\"]
    MoveDevice --  Inference[\"headline_model() br/ torch.no_grad() br/ [main.py:135-138]\"]
    Inference --  Softmax[\"torch.softmax() br/ [main.py:137]\"]
    Softmax --  Argmax[\"torch.argmax() br/ [main.py:138]\"]
    Argmax --  ExtractProbs[\"Extract probabilities br/ [main.py:140-142]\"]
    ExtractProbs --  MapLabel[\"Map to 'Real'/'Fake' br/ [main.py:144]\"]
    MapLabel --  BuildResponse[\"PredictionResponse br/ [main.py:146-151]\"]
    BuildResponse --  Return[\"Return JSON\"]
```

**Processing Steps:**

1. **Model Availability Check** [api/main.py:114-115](): Verifies that `headline_model` and `tokenizer` are loaded, raising HTTP 503 if not
2. **Input Validation** [api/main.py:117-118](): Ensures the statement is not empty (after stripping whitespace), raising HTTP 400 if invalid
3. **Text Cleaning** [api/main.py:120](): Applies `clean_text()` function which lowercases text and normalizes punctuation
4. **Tokenization** [api/main.py:122-130](): Converts cleaned text to token IDs using the RoBERTa tokenizer with:
   - `max_length=256`: Maximum sequence length
   - `padding=\"max_length\"`: Pad sequences to max length
   - `truncation=True`: Truncate sequences exceeding max length
   - `return_attention_mask=True`: Generate attention mask
   - `return_tensors=\"pt\"`: Return PyTorch tensors
5. **Device Transfer** [api/main.py:132-133](): Moves tensors to GPU (if available) or CPU
6. **Model Inference** [api/main.py:135-138](): 
   - Forward pass through RoBERTa model in `torch.no_grad()` context
   - Softmax applied to logits to get probability distribution
   - Argmax determines predicted class (0=Fake, 1=Real)
7. **Probability Extraction** [api/main.py:140-142](): Extracts individual class probabilities and confidence score
8. **Response Construction** [api/main.py:144-151](): Builds `PredictionResponse` with prediction label, confidence, and probabilities (all rounded to 2 decimal places)

**Sources:** [api/main.py:112-151](), [api/text_processing.py:4-9]()

---

## Batch Statement Prediction

### Endpoint: POST /predict/batch

The `/predict/batch` endpoint processes multiple statements in a single request, returning results for each statement.

#### Request Format

Accepts a JSON array of strings (no wrapper object):

```json
[
  \"Breaking: Scientists discover cure for aging\",
  \"Local weather expected to be sunny tomorrow\",
  \"Alien spacecraft lands in New York City\"
]
```

**Constraints:**
- Maximum 100 statements per batch [api/main.py:162-163]()
- Empty statements list returns HTTP 400 [api/main.py:159-160]()

**Sources:** [api/main.py:154-163]()

#### Response Format

Returns a JSON object with a `results` array:

```json
{
  \"results\": [
    {
      \"statement\": \"Breaking: Scientists discover cure for aging\",
      \"prediction\": \"Fake\",
      \"confidence\": 0.87,
      \"probabilities\": {\"fake\": 0.87, \"real\": 0.13}
    },
    {
      \"statement\": \"Local weather expected to be sunny tomorrow\",
      \"prediction\": \"Real\",
      \"confidence\": 0.76,
      \"probabilities\": {\"fake\": 0.24, \"real\": 0.76}
    },
    {
      \"statement\": \"\",
      \"error\": \"Empty statement\"
    }
  ]
}
```

Each result object contains either:
- **Success**: Same fields as single prediction response (`statement`, `prediction`, `confidence`, `probabilities`)
- **Error**: `statement` and `error` fields describing the failure

**Sources:** [api/main.py:196-205]()

#### Batch Processing Logic

```mermaid
sequenceDiagram
    participant Client
    participant Endpoint as \"/predict/batch br/ [main.py:154-205]\"
    participant Validation as \"Input Validation\"
    participant Loop as \"for loop br/ [main.py:166]\"
    participant CleanText as \"clean_text() br/ [text_processing.py:4-9]\"
    participant Tokenizer as \"AutoTokenizer br/ [main.py:172-180]\"
    participant Model as \"headline_model br/ [main.py:186]\"
    
    Client-  Endpoint: POST with list[str]
    Endpoint-  Validation: Check model loaded
    Validation--  Endpoint: OK or 503
    Endpoint-  Validation: Check list not empty
    Validation--  Endpoint: OK or 400
    Endpoint-  Validation: Check len  = 100
    Validation--  Endpoint: OK or 400
    
    Endpoint-  Loop: Iterate statements
    
    loop For each statement
        Loop-  Loop: Check if statement.strip()
        alt Empty statement
            Loop-  Loop: Append error result
        else Valid statement
            Loop-  CleanText: Clean text
            CleanText--  Loop: cleaned_text
            Loop-  Tokenizer: Tokenize
            Tokenizer--  Loop: input_ids, attention_mask
            Loop-  Model: Forward pass (no_grad)
            Model--  Loop: logits
            Loop-  Loop: Softmax + argmax
            Loop-  Loop: Extract probabilities
            Loop-  Loop: Append success result
        end
    end
    
    Endpoint--  Client: {\"results\": [...]}
```

**Processing Details:**

1. **Validation** [api/main.py:156-163]():
   - Model availability check (HTTP 503 if not loaded)
   - Non-empty list check (HTTP 400 if empty)
   - Size limit check (HTTP 400 if   100 statements)

2. **Sequential Processing** [api/main.py:166-203]():
   - Iterates through each statement individually
   - Empty statements produce error objects without halting batch
   - Each valid statement follows the same pipeline as single prediction
   - Results accumulated in `results` list

3. **Error Isolation** [api/main.py:167-169]():
   - Individual statement failures don't abort the batch
   - Error messages captured in result object alongside statement
   - Batch continues processing remaining statements

**Sources:** [api/main.py:154-205]()

---

## Data Models and Validation

### StatementRequest

Defined in [api/types.py:4-12](), this Pydantic model validates incoming requests to `/predict`.

**Fields:**
- `statement` (str): Required string field containing the text to analyze

**Example:**
```python
class StatementRequest(BaseModel):
    statement: str
```

**Sources:** [api/types.py:4-12]()

### PredictionResponse

Defined in [api/types.py:26-40](), this model structures the prediction output.

**Fields:**
- `statement` (str): Original input text
- `prediction` (str): \"Real\" or \"Fake\"
- `confidence` (float): Confidence score for predicted class
- `probabilities` (dict): Mapping of class names to probability scores

**Sources:** [api/types.py:26-40]()

---

## Text Preprocessing

Before inference, statements undergo cleaning via the `clean_text()` function defined in [api/text_processing.py:4-9]().

**Preprocessing Steps:**

1. **Lowercase** [api/text_processing.py:5](): Converts all characters to lowercase
2. **Character Filtering** [api/text_processing.py:6](): Retains only alphanumeric characters, spaces, and punctuation (`,`, `.`, `!`, `?`)
3. **Whitespace Normalization** [api/text_processing.py:7](): Collapses multiple spaces to single space
4. **Strip** [api/text_processing.py:8](): Removes leading/trailing whitespace

**Example Transformation:**
```
Input:  \"BREAKING: @Scientists say climate change is REAL!!! https://example.com\"
Output: \"breaking scientists say climate change is real!\"
```

This preprocessing preserves punctuation (unlike article cleaning) because punctuation can be semantically important in short statements.

**Sources:** [api/text_processing.py:4-9]()

---

## Error Handling

### HTTP Error Codes

| Status Code | Condition | Endpoint | Line Reference |
|-------------|-----------|----------|----------------|
| 400 | Empty statement | `/predict` | [api/main.py:117-118]() |
| 400 | Empty statements list | `/predict/batch` | [api/main.py:159-160]() |
| 400 | Batch size   100 | `/predict/batch` | [api/main.py:162-163]() |
| 503 | Model not loaded | Both | [api/main.py:114-115](), [api/main.py:156-157]() |

### Batch-Specific Error Handling

In batch processing, individual statement failures are captured in the results array rather than aborting the entire request:

```python
if not statement.strip():
    results.append({\"statement\": statement, \"error\": \"Empty statement\"})
    continue
```

This allows partial success in batch operations—some statements may fail validation while others succeed.

**Sources:** [api/main.py:114-118](), [api/main.py:156-163](), [api/main.py:167-169]()

---

## Model Inference Details

### Device Selection

The RoBERTa model is loaded onto the appropriate device during application startup:

```python
DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")
```

Defined at [api/main.py:24](), this device selection enables GPU acceleration when available. Tensors are moved to the selected device before inference at [api/main.py:132-133]().

### Inference Context

All model inference occurs within a `torch.no_grad()` context [api/main.py:135-138]() to disable gradient computation, reducing memory usage and improving inference speed.

### Output Processing

The model outputs logits which are converted to probabilities:

```mermaid
graph LR
    Logits[\"Model Output br/ logits tensor\"] --  Softmax[\"torch.softmax(dim=1) br/ [main.py:137]\"]
    Softmax --  Probs[\"Probability Distribution br/ [prob_fake, prob_real]\"]
    Probs --  Argmax[\"torch.argmax() br/ [main.py:138]\"]
    Argmax --  Class[\"Predicted Class br/ 0 or 1\"]
    Probs --  Extract[\"Extract Individual br/ Probabilities\"]
    Extract --  ProbFake[\"prob_fake br/ [main.py:140]\"]
    Extract --  ProbReal[\"prob_real br/ [main.py:141]\"]
    Class --  MapLabel{\"pred_class == 1?\"}
    MapLabel -- |Yes| Real[\"'Real'\"]
    MapLabel -- |No| Fake[\"'Fake'\"]
```

**Class Mapping:**
- `0` → \"Fake\"
- `1` → \"Real\"

This mapping is applied at [api/main.py:144]().

**Sources:** [api/main.py:135-144]()

---

## Performance Considerations

### Tokenization Parameters

The `MAX_LENGTH` constant is set to 256 tokens [api/main.py:23](), matching the RoBERTa model's training configuration. Statements longer than this are truncated, while shorter ones are padded.

### Batch Size Limits

The batch endpoint enforces a maximum of 100 statements per request [api/main.py:162-163](). This prevents:
- Excessive memory usage
- Request timeout issues
- Resource exhaustion on the server

For larger datasets, clients should split requests into multiple batches.

### Sequential vs. Parallel Processing

The current batch implementation processes statements sequentially [api/main.py:166-203](). While this simplifies error handling and resource management, it means batch processing time scales linearly with batch size.

**Sources:** [api/main.py:23](), [api/main.py:162-163](), [api/main.py:166-203]()

---

## Dependencies and Imports

The statement prediction endpoints rely on the following key imports:

| Import | Purpose | Usage |
|--------|---------|-------|
| `torch` | Deep learning framework | Tensor operations, device management, no_grad context |
| `transformers.AutoTokenizer` | Text tokenization | Convert text to token IDs |
| `transformers.AutoModelForSequenceClassification` | Model architecture | RoBERTa inference |
| `fastapi.FastAPI` | Web framework | Endpoint definitions |
| `fastapi.HTTPException` | Error handling | HTTP error responses |
| `api.types.StatementRequest` | Request validation | Input schema |
| `api.types.PredictionResponse` | Response formatting | Output schema |
| `api.text_processing.clean_text` | Text preprocessing | Statement cleaning |

**Sources:** [api/main.py:1-17]()"])</script><script>self.__next_f.push([1,"1c:T4451,"])</script><script>self.__next_f.push([1,"# Article Prediction Endpoints

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)

 /details 



## Purpose and Scope

This document describes the article prediction endpoints that classify longer news articles as fake or real. These endpoints use a Gradient Boosting Classifier with TF-IDF vectorization to analyze full-length articles (typically multiple paragraphs or complete news stories).

For documentation of endpoints that analyze short statements and headlines using the RoBERTa transformer model, see [Statement Prediction Endpoints](#3.1.1). For general information about the backend API architecture, see [Backend API](#3).

**Sources:** [api/main.py:208-283]()

---

## Endpoints Overview

The article prediction subsystem exposes two REST endpoints:

| Endpoint | Method | Purpose | Max Batch Size |
|----------|--------|---------|----------------|
| `/predict/article` | POST | Classify a single article | N/A |
| `/predict/article/batch` | POST | Classify multiple articles | 50 articles |

Both endpoints require the article model and TF-IDF vectorizer to be loaded at application startup. If models are not loaded, requests return HTTP 503 with error message \"Article model not loaded\".

**Sources:** [api/main.py:208-283]()

---

## Architecture Overview

```mermaid
graph TB
    Client[\"Client Application\"]
    
    subgraph \"FastAPI Endpoints\"
        SingleEP[\"/predict/article br/ predict_article()\"]
        BatchEP[\"/predict/article/batch br/ predict_article_batch()\"]
    end
    
    subgraph \"Request Validation\"
        ArtReq[\"ArticleRequest br/ api/types.py\"]
        ListVal[\"List[str] Validation\"]
    end
    
    subgraph \"Text Processing\"
        CleanArt[\"clean_article() br/ api/text_processing.py\"]
    end
    
    subgraph \"ML Pipeline\"
        Vectorizer[\"TfidfVectorizer br/ vectorizer.transform()\"]
        GBC[\"GradientBoostingClassifier br/ article_model\"]
        PredictProba[\"predict_proba()\"]
        Predict[\"predict()\"]
    end
    
    subgraph \"Response Construction\"
        ArtResp[\"ArticlePredictionResponse br/ api/types.py\"]
        DictResp[\"Dict Response\"]
    end
    
    Client -- |\"POST {article: str}\"| SingleEP
    Client -- |\"POST [str, str, ...]\"| BatchEP
    
    SingleEP --  ArtReq
    BatchEP --  ListVal
    
    ArtReq --  CleanArt
    ListVal --  CleanArt
    
    CleanArt --  Vectorizer
    Vectorizer --  PredictProba
    Vectorizer --  Predict
    
    PredictProba --  ArtResp
    Predict --  ArtResp
    PredictProba --  DictResp
    Predict --  DictResp
    
    ArtResp --  Client
    DictResp --  Client
```

**Sources:** [api/main.py:208-283](), [api/types.py:15-57](), [api/text_processing.py:12-21]()

---

## Single Article Prediction

### Endpoint Details

- **Route:** `/predict/article`
- **Method:** POST
- **Function:** `predict_article()` [api/main.py:208-239]()
- **Response Model:** `ArticlePredictionResponse`

### Request Format

The endpoint accepts JSON payloads conforming to the `ArticleRequest` schema defined in [api/types.py:15-23]():

```json
{
  \"article\": \"A new study published in the Journal of Medicine reveals...\"
}
```

**Field Specifications:**

| Field | Type | Required | Validation |
|-------|------|----------|------------|
| `article` | string | Yes | Must not be empty after stripping whitespace |

Empty articles (containing only whitespace) result in HTTP 400 with error message \"Article cannot be empty\" [api/main.py:213-214]().

**Sources:** [api/types.py:15-23](), [api/main.py:213-214]()

### Processing Pipeline

The article undergoes a multi-stage processing pipeline before classification:

```mermaid
sequenceDiagram
    participant EP as predict_article()
    participant Clean as clean_article()
    participant Vec as vectorizer
    participant Model as article_model
    
    EP-  EP: \"Validate request\"
    EP-  Clean: \"article text\"
    
    Note over Clean: \"Lowercase conversion\"
    Note over Clean: \"Remove URLs (https?://...)\"
    Note over Clean: \"Remove brackets [...]\"
    Note over Clean: \"Remove digits\"
    Note over Clean: \"Remove HTML tags\"
    Note over Clean: \"Replace newlines\"
    Note over Clean: \"Normalize whitespace\"
    Note over Clean: \"Remove non-alphanumeric\"
    
    Clean--  EP: \"cleaned_text\"
    EP-  Vec: \"transform([cleaned_text])\"
    Vec--  EP: \"text_vectorized (sparse matrix)\"
    
    EP-  Model: \"predict_proba(text_vectorized)\"
    Model--  EP: \"probabilities array [fake, real]\"
    
    EP-  Model: \"predict(text_vectorized)\"
    Model--  EP: \"pred_class (0 or 1)\"
    
    Note over EP: \"Extract prob_fake = probabilities[0]\"
    Note over EP: \"Extract prob_real = probabilities[1]\"
    Note over EP: \"confidence = probabilities[pred_class]\"
    Note over EP: \"prediction = 'Real' if pred_class==1 else 'Fake'\"
```

**Text Cleaning Details:**

The `clean_article()` function [api/text_processing.py:12-21]() applies aggressive preprocessing:

1. Convert to lowercase
2. Remove URLs matching patterns `https?://\\S+` or `www.\\S+`
3. Remove bracketed content `\\[.*?\\]`
4. Remove tokens containing digits `\\w*\\d\\w*`
5. Remove HTML tags ` .*? +`
6. Replace newlines with spaces
7. Normalize multiple whitespaces to single space
8. Remove all non-word characters (keeps only alphanumeric and underscores)

**Vectorization:**

The cleaned text is transformed using the pre-fitted `TfidfVectorizer` loaded at startup [api/main.py:53-54](), producing a sparse feature matrix.

**Classification:**

The `GradientBoostingClassifier` performs two operations [api/main.py:220-221]():
- `predict_proba()`: Returns probability distribution `[P(fake), P(real)]`
- `predict()`: Returns predicted class label (0 for fake, 1 for real)

**Sources:** [api/main.py:216-228](), [api/text_processing.py:12-21]()

### Response Format

The endpoint returns an `ArticlePredictionResponse` object [api/types.py:43-57]():

```json
{
  \"article\": \"A new study published in the Journal of Medicine...\",
  \"prediction\": \"Real\",
  \"confidence\": 0.89,
  \"probabilities\": {
    \"fake\": 0.11,
    \"real\": 0.89
  }
}
```

**Field Specifications:**

| Field | Type | Description |
|-------|------|-------------|
| `article` | string | The original article text, truncated to 500 characters if longer |
| `prediction` | string | Classification result: \"Real\" or \"Fake\" |
| `confidence` | float | Confidence score (0.0-1.0), rounded to 2 decimal places |
| `probabilities` | object | Breakdown with `fake` and `real` probabilities, both rounded to 2 decimals |

**Article Truncation:**

For response compactness, articles exceeding 500 characters are truncated with \"...\" appended [api/main.py:230-232](). The original full text is still used for classification; truncation only affects the response payload.

**Sources:** [api/main.py:229-239](), [api/types.py:43-57]()

### Error Handling

The endpoint implements the following error conditions:

| Status Code | Condition | Error Message |
|-------------|-----------|---------------|
| 503 | `article_model` or `vectorizer` is None | \"Article model not loaded\" |
| 400 | Article is empty or whitespace-only | \"Article cannot be empty\" |

Model availability is checked at [api/main.py:210-211](). Empty article validation occurs at [api/main.py:213-214]().

**Sources:** [api/main.py:210-214]()

---

## Batch Article Prediction

### Endpoint Details

- **Route:** `/predict/article/batch`
- **Method:** POST
- **Function:** `predict_article_batch()` [api/main.py:242-283]()
- **Response Model:** Dictionary with `results` key

### Request Format

The endpoint accepts a JSON array of strings:

```json
[
  \"First article text...\",
  \"Second article text...\",
  \"Third article text...\"
]
```

**Validation Rules:**

| Rule | Enforcement | Error |
|------|-------------|-------|
| Non-empty list | Must contain at least 1 article | HTTP 400: \"Articles list cannot be empty\" |
| Maximum size | Cannot exceed 50 articles | HTTP 400: \"Maximum 50 articles per batch\" |

Empty list validation occurs at [api/main.py:247-248](). Batch size limit is enforced at [api/main.py:250-251]().

**Sources:** [api/main.py:243-251]()

### Processing Pipeline

The batch endpoint iterates over the article list, processing each individually:

```mermaid
graph TB
    Start[\"Request: [article1, article2, ...]\"]
    
    subgraph \"Validation\"
        CheckModel[\"Check article_model br/ and vectorizer loaded\"]
        CheckEmpty[\"Check list not empty\"]
        CheckSize[\"Check list ≤ 50\"]
    end
    
    subgraph \"Iteration Loop\"
        ForEach[\"For each article\"]
        CheckArticle{\"Article empty?\"}
        AddError[\"Add error result: br/ {article, error}\"]
        
        subgraph \"Processing\"
            Clean[\"clean_article(article)\"]
            Vec[\"vectorizer.transform()\"]
            PredProba[\"predict_proba()\"]
            Pred[\"predict()\"]
            CalcConf[\"Calculate confidence\"]
            AddResult[\"Add success result: br/ {article, prediction, br/ confidence, probabilities}\"]
        end
    end
    
    Response[\"Return: {results: [...]}\"]
    
    Start --  CheckModel
    CheckModel --  CheckEmpty
    CheckEmpty --  CheckSize
    CheckSize --  ForEach
    
    ForEach --  CheckArticle
    CheckArticle -- |\"Yes\"| AddError
    CheckArticle -- |\"No\"| Clean
    
    Clean --  Vec
    Vec --  PredProba
    Vec --  Pred
    PredProba --  CalcConf
    Pred --  CalcConf
    CalcConf --  AddResult
    
    AddError --  ForEach
    AddResult --  ForEach
    ForEach -- |\"All processed\"| Response
```

**Graceful Error Handling:**

Unlike the single-article endpoint, batch processing continues when individual articles fail. Empty articles are added to results with an error field [api/main.py:255-257]():

```json
{
  \"article\": \"\",
  \"error\": \"Empty article\"
}
```

This design ensures partial batch completion rather than complete failure.

**Sources:** [api/main.py:253-281]()

### Response Format

The endpoint returns a dictionary containing a `results` array:

```json
{
  \"results\": [
    {
      \"article\": \"First article text (truncated to 500 chars)...\",
      \"prediction\": \"Real\",
      \"confidence\": 0.92,
      \"probabilities\": {
        \"fake\": 0.08,
        \"real\": 0.92
      }
    },
    {
      \"article\": \"\",
      \"error\": \"Empty article\"
    },
    {
      \"article\": \"Third article text...\",
      \"prediction\": \"Fake\",
      \"confidence\": 0.78,
      \"probabilities\": {
        \"fake\": 0.78,
        \"real\": 0.22
      }
    }
  ]
}
```

**Result Object Schema:**

Successful predictions include:

| Field | Type | Description |
|-------|------|-------------|
| `article` | string | Original text (truncated to 500 chars) |
| `prediction` | string | \"Real\" or \"Fake\" |
| `confidence` | float | Confidence score (0.0-1.0), 2 decimals |
| `probabilities` | object | `fake` and `real` probabilities, 2 decimals |

Failed predictions include:

| Field | Type | Description |
|-------|------|-------------|
| `article` | string | Original (empty) article text |
| `error` | string | Error description |

Article truncation logic mirrors the single-article endpoint [api/main.py:273]().

**Sources:** [api/main.py:271-281]()

### Error Handling

Batch-level errors (model not loaded, empty list, oversized batch) return HTTP error responses. Article-level errors are included in the results array without failing the entire batch.

| Level | Status Code | Condition | Response |
|-------|-------------|-----------|----------|
| Batch | 503 | Models not loaded | Exception with \"Article model not loaded\" |
| Batch | 400 | Empty list | Exception with \"Articles list cannot be empty\" |
| Batch | 400 | List size   50 | Exception with \"Maximum 50 articles per batch\" |
| Article | 200 | Individual empty article | Result object with `error` field |

**Sources:** [api/main.py:244-257]()

---

## Model Dependencies

Both endpoints depend on global state initialized during application startup:

### Required Artifacts

```mermaid
graph LR
    subgraph \"Startup - lifespan()\"
        LoadFunc[\"load_article_model()\"]
    end
    
    subgraph \"File System\"
        PickleModel[\"data/gradient_boosting_classifier.pkl\"]
        PickleVec[\"data/tfidf_vectorizer.pkl\"]
    end
    
    subgraph \"Global State\"
        ModelVar[\"article_model: GradientBoostingClassifier\"]
        VecVar[\"vectorizer: TfidfVectorizer\"]
    end
    
    subgraph \"Runtime\"
        Endpoints[\"/predict/article br/ /predict/article/batch\"]
    end
    
    LoadFunc -- |\"pickle.load()\"| PickleModel
    LoadFunc -- |\"pickle.load()\"| PickleVec
    
    PickleModel --  ModelVar
    PickleVec --  VecVar
    
    ModelVar --  Endpoints
    VecVar --  Endpoints
```

### Loading Mechanism

The `load_article_model()` function [api/main.py:41-57]() executes during FastAPI's lifespan startup:

1. Validates existence of both pickle files [api/main.py:44-48]()
2. Loads classifier from `ARTICLE_MODEL_DIR` using `pickle.load()` [api/main.py:50-51]()
3. Loads vectorizer from `VECTORIZER_DIR` using `pickle.load()` [api/main.py:53-54]()
4. Assigns to global variables `article_model` and `vectorizer` [api/main.py:42, 37-38]()

**File Paths:**

| Constant | Default Value | Description |
|----------|---------------|-------------|
| `ARTICLE_MODEL_DIR` | `../data/gradient_boosting_classifier.pkl` | Serialized Gradient Boosting Classifier |
| `VECTORIZER_DIR` | `../data/tfidf_vectorizer.pkl` | Fitted TF-IDF vectorizer |

Paths are constructed relative to the `api/` directory [api/main.py:30-35]().

**Failure Mode:**

If either file is missing at startup, the application raises `RuntimeError` with message indicating missing files and instructing to run training notebook [api/main.py:45-48]().

**Sources:** [api/main.py:41-57](), [api/main.py:30-35]()

---

## Comparison with Statement Endpoints

The article prediction endpoints differ from statement endpoints in several key aspects:

| Aspect | Article Endpoints | Statement Endpoints |
|--------|-------------------|---------------------|
| **Model** | Gradient Boosting Classifier | RoBERTa Transformer |
| **Vectorization** | TF-IDF | Tokenization (AutoTokenizer) |
| **Device** | CPU only | CUDA if available, else CPU |
| **Text Cleaning** | Aggressive (removes URLs, HTML, digits) | Light (preserves punctuation) |
| **Max Batch Size** | 50 articles | 100 statements |
| **Request Type** | `ArticleRequest` | `StatementRequest` |
| **Response Type** | `ArticlePredictionResponse` | `PredictionResponse` |
| **Max Input Length** | Unlimited (handled by TF-IDF) | 256 tokens (truncated) |
| **Response Truncation** | 500 characters | No truncation |

**Text Processing Differences:**

The `clean_article()` function [api/text_processing.py:12-21]() is more aggressive than `clean_text()` [api/text_processing.py:4-9](), removing URLs, HTML tags, and digits to focus on pure textual content. Statement cleaning preserves punctuation for better RoBERTa performance.

**Model Selection Rationale:**

Articles use traditional ML (Gradient Boosting + TF-IDF) because:
- Longer text provides sufficient features for bag-of-words approaches
- TF-IDF captures document-level term importance
- Faster inference on CPU compared to transformer models
- Lower memory footprint for batch processing

Statements use RoBERTa because:
- Short text requires contextual understanding
- Transformers excel at capturing semantic meaning in limited context
- Pre-trained language models leverage external knowledge

For details on statement endpoint implementation, see [Statement Prediction Endpoints](#3.1.1).

**Sources:** [api/main.py:208-283](), [api/text_processing.py:1-21]()

---

## Usage Examples

### Single Article Classification

**Request:**
```bash
curl -X POST http://localhost:8000/predict/article \\
  -H \"Content-Type: application/json\" \\
  -d '{
    \"article\": \"Scientists at MIT have developed a new quantum computer that operates at room temperature. The breakthrough could revolutionize computing by making quantum processors more accessible...\"
  }'
```

**Response:**
```json
{
  \"article\": \"Scientists at MIT have developed a new quantum computer that operates at room temperature. The breakthrough could revolutionize computing by making quantum processors more accessible...\",
  \"prediction\": \"Real\",
  \"confidence\": 0.87,
  \"probabilities\": {
    \"fake\": 0.13,
    \"real\": 0.87
  }
}
```

### Batch Article Classification

**Request:**
```bash
curl -X POST http://localhost:8000/predict/article/batch \\
  -H \"Content-Type: application/json\" \\
  -d '[
    \"First article about scientific discovery...\",
    \"Second article with questionable claims...\",
    \"Third article from reputable source...\"
  ]'
```

**Response:**
```json
{
  \"results\": [
    {
      \"article\": \"First article about scientific discovery...\",
      \"prediction\": \"Real\",
      \"confidence\": 0.91,
      \"probabilities\": {\"fake\": 0.09, \"real\": 0.91}
    },
    {
      \"article\": \"Second article with questionable claims...\",
      \"prediction\": \"Fake\",
      \"confidence\": 0.84,
      \"probabilities\": {\"fake\": 0.84, \"real\": 0.16}
    },
    {
      \"article\": \"Third article from reputable source...\",
      \"prediction\": \"Real\",
      \"confidence\": 0.88,
      \"probabilities\": {\"fake\": 0.12, \"real\": 0.88}
    }
  ]
}
```

**Sources:** [api/main.py:208-283]()"])</script><script>self.__next_f.push([1,"1d:T39af,"])</script><script>self.__next_f.push([1,"# Text Processing

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)

 /details 



## Purpose and Scope

This document describes the text preprocessing and data validation infrastructure in the UnFake backend API. It covers two preprocessing pipelines (`clean_text` and `clean_article`) that transform raw user input into model-ready features, and the Pydantic data models that validate API requests and structure responses. For information about how these functions are invoked during API request handling, see [API Endpoints](#3.1). For details about the ML models that consume the preprocessed text, see [Machine Learning Models](#5).

---

## Overview

The text processing system implements two distinct preprocessing strategies optimized for different text types and ML models:

| Function | Target Model | Strategy | Input Type |
|----------|--------------|----------|------------|
| `clean_text()` | RoBERTa Transformer | Conservative cleaning, preserves punctuation | Statements, headlines |
| `clean_article()` | Gradient Boosting + TF-IDF | Aggressive cleaning, removes URLs, HTML, special chars | Articles, long-form text |

Both functions are defined in [api/text_processing.py]() and are invoked by FastAPI endpoints after request validation but before model inference.

**Sources:** [api/text_processing.py:1-22](), [api/main.py:17]()

---

## Text Processing Pipeline

The following diagram illustrates how raw user input flows through validation and preprocessing before reaching the ML models:

```mermaid
graph TB
    UserInput[\"User Input br/ (Raw String)\"]
    
    subgraph \"Data Validation Layer\"
        StatementRequest[\"StatementRequest br/ pydantic model\"]
        ArticleRequest[\"ArticleRequest br/ pydantic model\"]
    end
    
    subgraph \"api/text_processing.py\"
        CleanText[\"clean_text() br/ Conservative Preprocessing\"]
        CleanArticle[\"clean_article() br/ Aggressive Preprocessing\"]
    end
    
    subgraph \"Tokenization/Vectorization\"
        RoBERTaTokenizer[\"AutoTokenizer br/ max_length=256 br/ padding='max_length'\"]
        TfidfVectorizer[\"TfidfVectorizer br/ transform()\"]
    end
    
    subgraph \"Model Inference\"
        RoBERTaModel[\"RoBERTa Model br/ headline_model\"]
        GBCModel[\"GradientBoostingClassifier br/ article_model\"]
    end
    
    UserInput -- |\"Statement Mode\"| StatementRequest
    UserInput -- |\"Article Mode\"| ArticleRequest
    
    StatementRequest -- |\"request.statement\"| CleanText
    ArticleRequest -- |\"request.article\"| CleanArticle
    
    CleanText -- |\"cleaned_text\"| RoBERTaTokenizer
    CleanArticle -- |\"cleaned_text\"| TfidfVectorizer
    
    RoBERTaTokenizer -- |\"input_ids, attention_mask\"| RoBERTaModel
    TfidfVectorizer -- |\"tfidf_features\"| GBCModel
```

**Sources:** [api/main.py:112-151](), [api/main.py:208-239](), [api/text_processing.py:1-22]()

---

## Statement Preprocessing: `clean_text()`

The `clean_text()` function implements a conservative preprocessing strategy designed for the RoBERTa transformer model. It preserves punctuation marks and basic sentence structure, which are important for contextual understanding in transformer models.

### Implementation

[api/text_processing.py:4-9]()

```python
def clean_text(text: str) -  str:
  text = text.lower()
  text = re.sub(r\"[^a-z0-9\\s,.!?]\", \" \", text)
  text = re.sub(r\"\\s+\", \" \", text)
  text = text.strip()
  return text
```

### Processing Steps

```mermaid
graph LR
    Input[\"Input Text\"]
    Lower[\"Lowercase br/ text.lower()\"]
    RemoveChars[\"Remove Non-Alphanumeric br/ except ,.!? br/ Regex: [^a-z0-9\\s,.!?]\"]
    CollapseSpace[\"Collapse Whitespace br/ Regex: \\s+\"]
    Strip[\"Strip Leading/Trailing br/ text.strip()\"]
    Output[\"Cleaned Text\"]
    
    Input --  Lower
    Lower --  RemoveChars
    RemoveChars --  CollapseSpace
    CollapseSpace --  Strip
    Strip --  Output
```

### Transformation Example

| Stage | Example |
|-------|---------|
| Original | `\"The President said: 'We need action NOW!'  #politics\"` |
| Lowercase | `\"the president said: 'we need action now!'  #politics\"` |
| Remove special chars | `\"the president said   we need action now     politics\"` |
| Collapse whitespace | `\"the president said we need action now politics\"` |
| Strip | `\"the president said we need action now politics\"` |

### Design Rationale

- **Preserves punctuation** (`.`, `,`, `!`, `?`) for contextual signals
- **Minimal transformation** to retain transformer-interpretable structure
- **Lowercase normalization** reduces vocabulary size without losing semantic meaning
- **Character whitelist approach** (`[^a-z0-9\\s,.!?]`) is more predictable than blacklist

**Sources:** [api/text_processing.py:4-9](), [api/main.py:120-130]()

---

## Article Preprocessing: `clean_article()`

The `clean_article()` function implements an aggressive preprocessing strategy optimized for TF-IDF vectorization and traditional ML models. It removes all non-word characters and artifacts commonly found in web-scraped articles.

### Implementation

[api/text_processing.py:12-22]()

```python
def clean_article(text: str) -  str:
  text = text.lower().strip()
  text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)
  text = re.sub(r\"\\[.*?\\]\", \"\", text)
  text = re.sub(r\"\\w*\\d\\w*\", \"\", text)
  text = re.sub(r\" .*? +\", \"\", text)
  text = re.sub(r\"\
\", \" \", text)
  text = re.sub(r\"\\s+\", \" \", text)
  text = re.sub(r\"\\W\", \" \", text)
  return text
```

### Processing Steps

```mermaid
graph TB
    Input[\"Input Text\"]
    
    Step1[\"Step 1: Lowercase + Strip br/ text.lower().strip()\"]
    Step2[\"Step 2: Remove URLs br/ Regex: https?://\\S+|www\\.\\S+\"]
    Step3[\"Step 3: Remove Brackets br/ Regex: \\[.*?\\]\"]
    Step4[\"Step 4: Remove Words with Digits br/ Regex: \\w*\\d\\w*\"]
    Step5[\"Step 5: Remove HTML Tags br/ Regex:  .*? +\"]
    Step6[\"Step 6: Replace Newlines br/ Regex: \
 → space\"]
    Step7[\"Step 7: Collapse Whitespace br/ Regex: \\s+\"]
    Step8[\"Step 8: Remove Non-Word Chars br/ Regex: \\W\"]
    
    Output[\"Cleaned Text\"]
    
    Input --  Step1
    Step1 --  Step2
    Step2 --  Step3
    Step3 --  Step4
    Step4 --  Step5
    Step5 --  Step6
    Step6 --  Step7
    Step7 --  Step8
    Step8 --  Output
```

### Transformation Example

| Stage | Example |
|-------|---------|
| Original | `\"BREAKING: [UPDATE] Visit https://news.com for details!\
 p Story2day /p \"` |
| Lowercase + strip | `\"breaking: [update] visit https://news.com for details!\
 p story2day /p \"` |
| Remove URLs | `\"breaking: [update] visit  for details!\
 p story2day /p \"` |
| Remove brackets | `\"breaking:  visit  for details!\
 p story2day /p \"` |
| Remove words with digits | `\"breaking:  visit  for details!\
 p storyday /p \"` |
| Remove HTML tags | `\"breaking:  visit  for details!\
storyday\"` |
| Replace newlines | `\"breaking:  visit  for details! storyday\"` |
| Collapse whitespace | `\"breaking: visit for details! storyday\"` |
| Remove non-word chars | `\"breaking visit for details storyday\"` |

### Cleaned Artifacts

The following artifacts are systematically removed:

- **URLs**: `https?://\\S+|www\\.\\S+` - Removes hyperlinks
- **Bracketed content**: `\\[.*?\\]` - Removes citations, references
- **Alphanumeric tokens**: `\\w*\\d\\w*` - Removes mixed-character tokens like \"covid19\", \"2020election\"
- **HTML tags**: ` .*? +` - Removes markup from web scraping
- **Non-word characters**: `\\W` - Final pass to remove all punctuation and special symbols

### Design Rationale

- **Aggressive cleaning** reduces TF-IDF vocabulary size, improving model generalization
- **URL removal** prevents domain-specific bias in the classifier
- **HTML tag removal** handles web-scraped training data artifacts
- **Digit removal** eliminates noisy numeric tokens that rarely contribute to classification
- **Punctuation removal** aligns with traditional bag-of-words assumptions

**Sources:** [api/text_processing.py:12-22](), [api/main.py:216-218]()

---

## Data Validation Models

The system uses Pydantic models to validate incoming requests and structure outgoing responses. These models enforce type safety and provide automatic API documentation.

### Request Models

#### StatementRequest

[api/types.py:4-13]()

```python
class StatementRequest(BaseModel):
  statement: str
```

Used by endpoints: `/predict`, `/predict/batch`

Validates that the input contains a `statement` field of type `str`. The FastAPI endpoint additionally checks for empty strings at [api/main.py:117-118]().

#### ArticleRequest

[api/types.py:15-24]()

```python
class ArticleRequest(BaseModel):
  article: str
```

Used by endpoints: `/predict/article`, `/predict/article/batch`

Validates that the input contains an `article` field of type `str`. The FastAPI endpoint additionally checks for empty strings at [api/main.py:213-214]().

### Response Models

#### PredictionResponse

[api/types.py:26-41]()

Structure:
```python
{
  \"statement\": str,      # Original or echoed statement
  \"prediction\": str,     # \"Real\" or \"Fake\"
  \"confidence\": float,   # Rounded to 2 decimals
  \"probabilities\": {     # Rounded to 2 decimals
    \"fake\": float,
    \"real\": float
  }
}
```

Returned by: `/predict`, `/predict/batch`

#### ArticlePredictionResponse

[api/types.py:43-58]()

Structure:
```python
{
  \"article\": str,        # Truncated to 500 chars + \"...\"
  \"prediction\": str,     # \"Real\" or \"Fake\"
  \"confidence\": float,   # Rounded to 2 decimals
  \"probabilities\": {     # Rounded to 2 decimals
    \"fake\": float,
    \"real\": float
  }
}
```

Returned by: `/predict/article`, `/predict/article/batch`

Note: Article text is truncated to 500 characters in responses at [api/main.py:230-232]() to reduce payload size.

**Sources:** [api/types.py:1-58](), [api/main.py:146-151](), [api/main.py:229-239]()

---

## Integration with API Endpoints

The following table shows how text processing functions integrate with each API endpoint:

| Endpoint | Request Model | Preprocessing Function | Validation Checks |
|----------|---------------|------------------------|-------------------|
| `POST /predict` | `StatementRequest` | `clean_text()` | Empty string check ([api/main.py:117-118]()) |
| `POST /predict/batch` | `list[str]` | `clean_text()` | Empty list, max 100 items ([api/main.py:159-163]()) |
| `POST /predict/article` | `ArticleRequest` | `clean_article()` | Empty string check ([api/main.py:213-214]()) |
| `POST /predict/article/batch` | `list[str]` | `clean_article()` | Empty list, max 50 items ([api/main.py:247-251]()) |

### Request Processing Sequence

```mermaid
sequenceDiagram
    participant Client
    participant FastAPI
    participant Pydantic
    participant TextProc as \"text_processing\"
    participant Model
    
    Client-  FastAPI: POST /predict {statement}
    FastAPI-  Pydantic: Validate StatementRequest
    
    alt Validation Fails
        Pydantic--  Client: 422 Unprocessable Entity
    end
    
    Pydantic-  FastAPI: Validated request object
    FastAPI-  FastAPI: Check if statement.strip() empty
    
    alt Empty String
        FastAPI--  Client: 400 Bad Request
    end
    
    FastAPI-  TextProc: clean_text(statement)
    TextProc--  FastAPI: cleaned_text
    
    FastAPI-  Model: Tokenize + Inference
    Model--  FastAPI: Prediction results
    
    FastAPI-  Pydantic: Construct PredictionResponse
    Pydantic--  Client: 200 OK with JSON response
```

**Sources:** [api/main.py:112-151](), [api/main.py:208-239]()

---

## Comparison: Statement vs Article Preprocessing

The two preprocessing strategies reflect different assumptions about text characteristics and model requirements:

| Aspect | `clean_text()` | `clean_article()` |
|--------|----------------|-------------------|
| **Target text length** | Short (headlines, statements) | Long (articles, documents) |
| **Punctuation** | Preserved (`.`, `,`, `!`, `?`) | Removed (all non-word chars) |
| **URLs** | Replaced with spaces | Explicitly removed |
| **HTML tags** | Replaced with spaces | Explicitly removed |
| **Digits** | Preserved | Removed |
| **Special characters** | Some preserved | All removed |
| **Regex passes** | 3 | 8 |
| **Processing complexity** | Low | High |
| **Model assumption** | Context-aware transformer | Bag-of-words TF-IDF |

### Why Different Strategies?

1. **Transformer models** (RoBERTa) benefit from punctuation and sentence structure for attention mechanism
2. **TF-IDF models** treat text as unordered token bags; punctuation adds noise without semantic value
3. **Article data** (from web scraping) contains more artifacts (HTML, URLs) than curated statements
4. **Vocabulary size** matters more for TF-IDF; aggressive cleaning reduces dimensionality

**Sources:** [api/text_processing.py:4-22]()

---

## Error Handling

Text processing functions do not raise exceptions. Invalid input is handled at the API endpoint level:

### Validation Errors

| Error Condition | HTTP Status | Endpoint |
|----------------|-------------|----------|
| Missing required field | 422 | All endpoints (Pydantic validation) |
| Empty string after strip | 400 | `/predict`, `/predict/article` |
| Empty list | 400 | Batch endpoints |
| Batch size   100 | 400 | `/predict/batch` |
| Batch size   50 | 400 | `/predict/article/batch` |

### Example Error Response

```json
{
  \"detail\": \"Statement cannot be empty\"
}
```

Returned at [api/main.py:118]() and [api/main.py:214]().

**Sources:** [api/main.py:117-118](), [api/main.py:159-163](), [api/main.py:213-214](), [api/main.py:247-251]()

---

## Usage Examples

### Statement Preprocessing in Context

[api/main.py:120-130]()

```python
# After Pydantic validation
cleaned_text = clean_text(request.statement)

encoding = tokenizer(
  cleaned_text,
  add_special_tokens=True,
  max_length=MAX_LENGTH,
  padding=\"max_length\",
  truncation=True,
  return_attention_mask=True,
  return_tensors=\"pt\",
)
```

### Article Preprocessing in Context

[api/main.py:216-218]()

```python
# After Pydantic validation
cleaned_text = clean_article(request.article)

text_vectorized = vectorizer.transform([cleaned_text])
```

**Sources:** [api/main.py:120-130](), [api/main.py:216-218]()

---

## Dependencies

The text processing module has minimal dependencies:

- **Python standard library**: `re` module for regular expressions
- **Pydantic**: `BaseModel` for data validation ([api/types.py:1]())

No external NLP libraries (NLTK, spaCy) are used in the preprocessing layer. Tokenization is handled by model-specific components (AutoTokenizer for RoBERTa, fitted TfidfVectorizer for articles).

**Sources:** [api/text_processing.py:1](), [api/types.py:1]()"])</script><script>self.__next_f.push([1,"1e:T4082,"])</script><script>self.__next_f.push([1,"# Model Loading and Lifecycle

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [data/gradient_boosting_classifier.pkl](data/gradient_boosting_classifier.pkl)
- [data/tfidf_vectorizer.pkl](data/tfidf_vectorizer.pkl)

 /details 



## Purpose and Scope

This document explains how the UnFake backend loads ML models at application startup and manages them throughout the application lifecycle. It covers the initialization sequence, device selection logic, error handling, and runtime state management for both the RoBERTa headline model and the Gradient Boosting article model.

For details about how these models are used during inference, see [Statement Prediction Endpoints](#3.1.1) and [Article Prediction Endpoints](#3.1.2). For information about the models themselves, see [Headline Model (RoBERTa)](#5.1) and [Article Model (Gradient Boosting)](#5.2).

## Lifecycle Overview

The UnFake backend uses FastAPI's `lifespan` context manager pattern to orchestrate model loading during application startup and cleanup during shutdown. This ensures models are loaded into memory exactly once before any requests are served, minimizing first-request latency.

```mermaid
stateDiagram-v2
    [*] --  Uninitialized: Application Start
    Uninitialized --  Loading: lifespan context enters
    Loading --  HeadlineLoading: load_headline_model()
    HeadlineLoading --  ArticleLoading: load_article_model()
    ArticleLoading --  Ready: Models loaded successfully
    ArticleLoading --  Failed: RuntimeError
    HeadlineLoading --  Failed: RuntimeError
    Ready --  Serving: Accept requests
    Serving --  Serving: Handle predictions
    Serving --  ShuttingDown: Application stop signal
    ShuttingDown --  [*]: lifespan context exits
    Failed --  [*]: Application exits
```

**Lifecycle Stages:**

| Stage | Description | Model State |
|-------|-------------|-------------|
| **Uninitialized** | Application starting, modules imported | `headline_model = None`, `article_model = None` |
| **Loading** | `lifespan()` context manager entered | Models loading sequentially |
| **Ready** | Both models loaded successfully | Models in memory, ready for inference |
| **Serving** | Application accepting requests | Models remain in memory |
| **ShuttingDown** | `lifespan()` context manager exiting | Models still in memory |

**Sources:** [api/main.py:80-84](), [api/main.py:26-38]()

## Model Storage Configuration

The application defines explicit directory paths for all model artifacts at the module level, using relative paths from the `api/` directory:

```mermaid
graph TB
    subgraph \"api/main.py Module Constants\"
        HEADLINE_MODEL_DIR[\"HEADLINE_MODEL_DIR\"]
        ARTICLE_MODEL_DIR[\"ARTICLE_MODEL_DIR\"]
        VECTORIZER_DIR[\"VECTORIZER_DIR\"]
        MAX_LENGTH[\"MAX_LENGTH=256\"]
        DEVICE[\"DEVICE\"]
    end
    
    subgraph \"File System\"
        RoBERTaDir[\"data/RoBERTa_Classifier/ br/ config.json br/ pytorch_model.bin br/ tokenizer files\"]
        GBCFile[\"data/gradient_boosting_classifier.pkl\"]
        VectFile[\"data/tfidf_vectorizer.pkl\"]
    end
    
    HEADLINE_MODEL_DIR -- |\"os.path.join(__file__, '..', 'data', 'RoBERTa_Classifier')\"| RoBERTaDir
    ARTICLE_MODEL_DIR -- |\"os.path.join(__file__, '..', 'data', 'gradient_boosting_classifier.pkl')\"| GBCFile
    VECTORIZER_DIR -- |\"os.path.join(__file__, '..', 'data', 'tfidf_vectorizer.pkl')\"| VectFile
```

**Configuration Constants:**

| Constant | Value | Purpose |
|----------|-------|---------|
| `HEADLINE_MODEL_DIR` | `../data/RoBERTa_Classifier` | Directory containing RoBERTa model and tokenizer files |
| `ARTICLE_MODEL_DIR` | `../data/gradient_boosting_classifier.pkl` | Path to pickled Gradient Boosting model |
| `VECTORIZER_DIR` | `../data/tfidf_vectorizer.pkl` | Path to pickled TF-IDF vectorizer |
| `MAX_LENGTH` | `256` | Maximum token length for headline tokenization |

**Sources:** [api/main.py:19-35]()

## Device Selection Logic

The application automatically selects the optimal compute device for the RoBERTa model at module import time, with transparent fallback from GPU to CPU:

```mermaid
flowchart TD
    Start[\"Module Import: br/ api/main.py loaded\"] --  CheckCUDA{\"torch.cuda.is_available()\"}
    CheckCUDA -- |\"True\"| SetGPU[\"DEVICE = torch.device('cuda')\"]
    CheckCUDA -- |\"False\"| SetCPU[\"DEVICE = torch.device('cpu')\"]
    SetGPU --  UsedInLoading[\"Used in load_headline_model(): br/ headline_model.to(DEVICE)\"]
    SetCPU --  UsedInLoading
    UsedInLoading --  PrintDevice[\"Print: 'Using device: {DEVICE}'\"]
```

**Device Selection Details:**

- **Module-level initialization:** The `DEVICE` variable is set once when `api/main.py` is imported
- **GPU detection:** Uses `torch.cuda.is_available()` to check for CUDA-capable devices
- **Model placement:** Only the RoBERTa headline model uses GPU acceleration
- **Article model:** Always runs on CPU (scikit-learn models don't support GPU)
- **Input tensor placement:** All input tensors are moved to the selected device during inference

**Sources:** [api/main.py:24](), [api/main.py:73](), [api/main.py:132-133](), [api/main.py:182-183]()

## Headline Model Loading

The `load_headline_model()` function loads the fine-tuned RoBERTa transformer model and its tokenizer from disk, configures the device, and sets the model to evaluation mode:

```mermaid
sequenceDiagram
    participant LF as load_headline_model()
    participant FS as File System
    participant AT as AutoTokenizer
    participant AM as AutoModelForSequenceClassification
    participant Device as DEVICE (cuda/cpu)
    participant Global as Global Variables

    LF-  FS: Check os.path.exists(HEADLINE_MODEL_DIR)
    alt Directory not found
        FS--  LF: False
        LF-  LF: raise RuntimeError
    else Directory exists
        FS--  LF: True
        LF-  AT: AutoTokenizer.from_pretrained(HEADLINE_MODEL_DIR)
        AT--  LF: tokenizer instance
        LF-  AM: AutoModelForSequenceClassification.from_pretrained(HEADLINE_MODEL_DIR)
        AM--  LF: headline_model instance
        LF-  Device: headline_model.to(DEVICE)
        Device--  LF: model on GPU/CPU
        LF-  LF: headline_model.eval()
        LF-  Global: Update headline_model, tokenizer
        LF-  LF: Print confirmation messages
    end
```

**Loading Steps:**

1. **Directory validation:** Checks if `HEADLINE_MODEL_DIR` exists, raises `RuntimeError` if missing
2. **Tokenizer loading:** Uses `AutoTokenizer.from_pretrained()` to load tokenizer configuration
3. **Model loading:** Uses `AutoModelForSequenceClassification.from_pretrained()` to load model weights
4. **Device placement:** Moves model to GPU or CPU using `.to(DEVICE)`
5. **Evaluation mode:** Sets model to evaluation mode with `.eval()` (disables dropout, batch normalization)
6. **Global state update:** Assigns loaded objects to global `headline_model` and `tokenizer` variables
7. **Logging:** Prints confirmation messages with model path and device

**Sources:** [api/main.py:60-77]()

## Article Model Loading

The `load_article_model()` function deserializes the scikit-learn Gradient Boosting Classifier and TF-IDF vectorizer from pickle files:

```mermaid
sequenceDiagram
    participant LF as load_article_model()
    participant FS as File System
    participant Pickle as pickle module
    participant Global as Global Variables

    LF-  FS: Check os.path.exists(ARTICLE_MODEL_DIR)
    LF-  FS: Check os.path.exists(VECTORIZER_DIR)
    alt Files not found
        FS--  LF: False for either
        LF-  LF: raise RuntimeError
    else Files exist
        FS--  LF: True for both
        LF-  FS: open(ARTICLE_MODEL_DIR, 'rb')
        FS--  LF: file handle
        LF-  Pickle: pickle.load(f)
        Pickle--  LF: GradientBoostingClassifier instance
        LF-  Global: article_model = loaded model
        LF-  FS: open(VECTORIZER_DIR, 'rb')
        FS--  LF: file handle
        LF-  Pickle: pickle.load(f)
        Pickle--  LF: TfidfVectorizer instance
        LF-  Global: vectorizer = loaded vectorizer
        LF-  LF: Print confirmation messages
    end
```

**Loading Steps:**

1. **File validation:** Checks existence of both pickle files, raises `RuntimeError` if either is missing
2. **Model deserialization:** Opens `ARTICLE_MODEL_DIR` in binary read mode and uses `pickle.load()`
3. **Vectorizer deserialization:** Opens `VECTORIZER_DIR` in binary read mode and uses `pickle.load()`
4. **Global state update:** Assigns loaded objects to global `article_model` and `vectorizer` variables
5. **Logging:** Prints confirmation messages with file paths

**Pickle Format Details:**

- Both files are Python pickle (`.pkl`) format
- The Gradient Boosting Classifier contains the trained tree ensemble
- The TF-IDF vectorizer contains fitted vocabulary and IDF weights
- These artifacts are generated by the training notebook (see [Training the Article Model](#6.2))

**Sources:** [api/main.py:41-57]()

## Startup Sequence

The `lifespan()` async context manager orchestrates the complete model loading sequence during FastAPI application startup:

```mermaid
sequenceDiagram
    participant Uvicorn as Uvicorn Server
    participant FastAPI as FastAPI(lifespan=lifespan)
    participant LCM as lifespan() context manager
    participant LH as load_headline_model()
    participant LA as load_article_model()
    participant App as Application Ready

    Uvicorn-  FastAPI: Start application
    FastAPI-  LCM: Enter context (__aenter__)
    LCM-  LH: Call load_headline_model()
    LH-  LH: Load RoBERTa + tokenizer
    LH-  LH: Move to DEVICE
    LH--  LCM: Models loaded
    LCM-  LA: Call load_article_model()
    LA-  LA: Load GradientBoostingClassifier
    LA-  LA: Load TfidfVectorizer
    LA--  LCM: Models loaded
    LCM-  App: yield (startup complete)
    Note over App: Application serving requests
    Note over App: Models remain in memory
    Uvicorn-  FastAPI: Shutdown signal
    FastAPI-  LCM: Exit context (__aexit__)
    LCM--  FastAPI: Cleanup complete
```

**Context Manager Implementation:**

The `lifespan()` function is defined as an async context manager using the `@asynccontextmanager` decorator:

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    load_headline_model()
    load_article_model()
    yield
```

**Execution Flow:**

1. **Application initialization:** FastAPI created with `FastAPI(lifespan=lifespan)` parameter
2. **Startup phase:** Before accepting requests, FastAPI enters the `lifespan` context
3. **Sequential loading:** Headline model loads first, then article model
4. **Yield point:** After successful loading, context manager yields control to FastAPI
5. **Serving phase:** Application accepts and processes requests
6. **Shutdown phase:** On application termination, context exits (cleanup if needed)

**Error Behavior:**

- If either loading function raises `RuntimeError`, the application fails to start
- FastAPI will not accept any requests until the `yield` point is reached
- Any exception before `yield` prevents application startup

**Sources:** [api/main.py:80-84](), [api/main.py:87-92]()

## Runtime State Management

During request processing, endpoints access the globally loaded models through direct variable references and perform null checks to ensure models are available:

```mermaid
graph TB
    subgraph \"Global Module State\"
        HM[\"headline_model: AutoModelForSequenceClassification | None\"]
        TOK[\"tokenizer: AutoTokenizer | None\"]
        AM[\"article_model: GradientBoostingClassifier | None\"]
        VEC[\"vectorizer: TfidfVectorizer | None\"]
    end
    
    subgraph \"Endpoints\"
        EP1[\"/predict\"]
        EP2[\"/predict/batch\"]
        EP3[\"/predict/article\"]
        EP4[\"/predict/article/batch\"]
        ROOT[\"/\"]
    end
    
    EP1 --  CheckHM{\"if headline_model is None br/ or tokenizer is None\"}
    EP2 --  CheckHM
    CheckHM -- |\"True\"| HTTP503[\"HTTPException(503, br/ 'Model not loaded')\"]
    CheckHM -- |\"False\"| UseHM[\"Use headline_model br/ and tokenizer\"]
    
    EP3 --  CheckAM{\"if article_model is None br/ or vectorizer is None\"}
    EP4 --  CheckAM
    CheckAM -- |\"True\"| HTTP503_2[\"HTTPException(503, br/ 'Article model not loaded')\"]
    CheckAM -- |\"False\"| UseAM[\"Use article_model br/ and vectorizer\"]
    
    ROOT --  CheckRoot{\"headline_model is not None\"}
    CheckRoot -- |\"True\"| Return200[\"return {'model_loaded': True}\"]
    CheckRoot -- |\"False\"| ReturnFalse[\"return {'model_loaded': False}\"]
```

**State Access Pattern:**

All prediction endpoints follow this pattern:

1. **Null check:** Verify model and associated objects are not `None`
2. **Error response:** Return HTTP 503 if models unavailable
3. **Direct access:** Use global variables directly for inference

**Global Variable Types:**

| Variable | Type Annotation | Initial Value | Set By |
|----------|----------------|---------------|--------|
| `headline_model` | `AutoModelForSequenceClassification \\| None` | `None` | `load_headline_model()` |
| `tokenizer` | `AutoTokenizer \\| None` | `None` | `load_headline_model()` |
| `article_model` | `GradientBoostingClassifier \\| None` | `None` | `load_article_model()` |
| `vectorizer` | `TfidfVectorizer \\| None` | `None` | `load_article_model()` |

**Thread Safety:**

- Models are loaded once during startup before any requests are processed
- All endpoints only **read** from global variables (no writes during serving)
- Python's GIL ensures thread-safe reads of object references
- Model inference methods (`.forward()`, `.predict()`) are called in read-only mode

**Sources:** [api/main.py:26-38](), [api/main.py:114-115](), [api/main.py:156-157](), [api/main.py:210-211](), [api/main.py:244-245](), [api/main.py:104-109]()

## Error Handling

The model loading system implements defensive error checking with clear failure messages:

**Model Loading Errors:**

| Error Condition | Function | Exception Type | Message |
|----------------|----------|----------------|---------|
| Headline model directory missing | `load_headline_model()` | `RuntimeError` | \"Model directory not found: {path}. Please train the model first...\" |
| Article model file missing | `load_article_model()` | `RuntimeError` | \"Model or vectorizer file not found: {paths}. Please train the model first...\" |
| Vectorizer file missing | `load_article_model()` | `RuntimeError` | (same as above) |

**Runtime Errors:**

| Error Condition | Endpoint | HTTP Status | Detail Message |
|----------------|----------|-------------|----------------|
| Headline model not loaded | `/predict`, `/predict/batch` | 503 | \"Model not loaded\" |
| Article model not loaded | `/predict/article`, `/predict/article/batch` | 503 | \"Article model not loaded\" |
| Empty statement | `/predict` | 400 | \"Statement cannot be empty\" |
| Empty article | `/predict/article` | 400 | \"Article cannot be empty\" |
| Batch too large (statements) | `/predict/batch` | 400 | \"Maximum 100 statements per batch\" |
| Batch too large (articles) | `/predict/article/batch` | 400 | \"Maximum 50 articles per batch\" |

**Error Flow:**

```mermaid
flowchart TD
    Start[\"Application Start\"] --  Lifespan[\"lifespan() entered\"]
    Lifespan --  LoadH[\"load_headline_model()\"]
    LoadH --  CheckHDir{\"HEADLINE_MODEL_DIR br/ exists?\"}
    CheckHDir -- |\"No\"| RaiseH[\"raise RuntimeError\"]
    RaiseH --  FailStart[\"Application fails to start\"]
    CheckHDir -- |\"Yes\"| LoadHSuccess[\"Load RoBERTa model\"]
    LoadHSuccess --  LoadA[\"load_article_model()\"]
    LoadA --  CheckAFiles{\"Both pickle files br/ exist?\"}
    CheckAFiles -- |\"No\"| RaiseA[\"raise RuntimeError\"]
    RaiseA --  FailStart
    CheckAFiles -- |\"Yes\"| LoadASuccess[\"Load GB model + vectorizer\"]
    LoadASuccess --  Yield[\"yield (startup complete)\"]
    Yield --  Serving[\"Accept requests\"]
    Serving --  Request[\"Incoming request\"]
    Request --  CheckModels{\"Models loaded?\"}
    CheckModels -- |\"No\"| Return503[\"HTTP 503\"]
    CheckModels -- |\"Yes\"| ProcessRequest[\"Process request\"]
```

**Recovery:**

- **Startup failures:** Application will not start; user must train models or fix file paths
- **Runtime failures:** If models somehow become `None` after startup, returns 503 errors
- **No automatic retry:** System does not attempt to reload models during runtime

**Sources:** [api/main.py:43-48](), [api/main.py:63-67](), [api/main.py:114-115](), [api/main.py:117-118](), [api/main.py:210-211](), [api/main.py:213-214]()"])</script><script>self.__next_f.push([1,"1f:T5f7d,"])</script><script>self.__next_f.push([1,"# Frontend Application

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [frontend/app.js](frontend/app.js)
- [frontend/index.html](frontend/index.html)
- [frontend/styles.css](frontend/styles.css)

 /details 



## Purpose and Scope

This document covers the web-based user interface of the UnFake system, implemented as a single-page application (SPA) using vanilla HTML, CSS, and JavaScript. The frontend provides an interactive interface for users to analyze news statements and articles, displays prediction results with animated visualizations, and handles communication with the backend API.

For details on the backend API that this frontend communicates with, see [Backend API](#3). For information about the machine learning models that power the predictions, see [Machine Learning Models](#5).

**Sources:** [frontend/app.js:1-227](), [frontend/index.html:1-173](), [frontend/styles.css:1-686]()

---

## Architecture Overview

The frontend implements a three-layer architecture with clear separation of concerns:

```mermaid
graph TB
    subgraph \"Structure Layer\"
        HTML[index.html]
        Header[\"Header Section\"]
        InputSection[\"Input Section br/ .input-section\"]
        ResultsSection[\"Results Section br/ #results\"]
        ErrorSection[\"Error Section br/ #error\"]
    end
    
    subgraph \"Logic Layer\"
        AppJS[app.js]
        Init[\"Initialization br/ DOMContentLoaded\"]
        EventHandlers[\"Event Handlers\"]
        APIComm[\"API Communication br/ fetch()\"]
        StateManagement[\"State Management br/ currentMode\"]
    end
    
    subgraph \"Presentation Layer\"
        CSS[styles.css]
        Variables[\"CSS Variables br/ :root\"]
        Animations[\"Keyframe Animations br/ @keyframes\"]
        Responsive[\"Media Queries br/ @media\"]
    end
    
    subgraph \"Backend\"
        FastAPI[\"FastAPI Backend br/ localhost:8000\"]
    end
    
    HTML --  Header
    HTML --  InputSection
    HTML --  ResultsSection
    HTML --  ErrorSection
    
    AppJS --  Init
    AppJS --  EventHandlers
    AppJS --  APIComm
    AppJS --  StateManagement
    
    CSS --  Variables
    CSS --  Animations
    CSS --  Responsive
    
    EventHandlers --  APIComm
    APIComm --  FastAPI
    FastAPI --  StateManagement
    StateManagement --  EventHandlers
    
    Init --  HTML
    EventHandlers --  HTML
    Variables --  HTML
    Animations --  HTML
    Responsive --  HTML
```

**Sources:** [frontend/index.html:1-173](), [frontend/app.js:1-227](), [frontend/styles.css:1-686]()

---

## DOM Structure and Key Elements

The HTML file defines a semantic structure with major sections that toggle visibility based on application state. All DOM elements are accessed and manipulated by `app.js` through their IDs.

### Major Sections

| Section | Element ID | Purpose | Initial State |
|---------|-----------|---------|---------------|
| Header | `.header` | Logo, title, and tagline | Visible |
| Input Section | `.input-section` | Mode toggle, textarea, analyze button | Visible |
| Results Section | `#results` | Verdict, confidence ring, probability bars, content preview | Hidden |
| Error Section | `#error` | Error message and dismiss button | Hidden |

### Input Section Components

```mermaid
graph LR
    InputSection[\".input-section\"]
    InputCard[\".input-card\"]
    ModeToggle[\".mode-toggle\"]
    ModeStatement[\"#mode-statement br/ Button\"]
    ModeArticle[\"#mode-article br/ Button\"]
    InputLabel[\"#input-label br/ Label\"]
    ContentInput[\"#content-input br/ Textarea\"]
    AnalyzeBtn[\"#analyze-btn br/ Button\"]
    
    InputSection --  InputCard
    InputCard --  ModeToggle
    ModeToggle --  ModeStatement
    ModeToggle --  ModeArticle
    InputCard --  InputLabel
    InputCard --  ContentInput
    InputCard --  AnalyzeBtn
```

**Sources:** [frontend/index.html:30-65](), [frontend/app.js:5-13]()

### Results Section Components

```mermaid
graph TB
    Results[\"#results\"]
    ResultsCard[\".results-card\"]
    
    VerdictContainer[\".verdict-container\"]
    IconFake[\"#icon-fake br/ SVG X Circle\"]
    IconReal[\"#icon-real br/ SVG Check Circle\"]
    VerdictText[\"#verdict-text br/ H2\"]
    
    ProbRing[\".probability-ring\"]
    ProgressCircle[\"#progress-circle br/ SVG Circle\"]
    ConfidenceValue[\"#confidence-value br/ Span\"]
    
    ProbBars[\".probability-bars\"]
    FakeBar[\"#fake-bar br/ Div\"]
    RealBar[\"#real-bar br/ Div\"]
    FakePercent[\"#fake-percent br/ Span\"]
    RealPercent[\"#real-percent br/ Span\"]
    
    Preview[\".statement-preview\"]
    PreviewLabel[\"#preview-label br/ P\"]
    AnalyzedContent[\"#analyzed-content br/ P\"]
    
    ResetBtn[\"#reset-btn br/ Button\"]
    
    Results --  ResultsCard
    ResultsCard --  VerdictContainer
    VerdictContainer --  IconFake
    VerdictContainer --  IconReal
    VerdictContainer --  VerdictText
    ResultsCard --  ProbRing
    ProbRing --  ProgressCircle
    ProbRing --  ConfidenceValue
    ResultsCard --  ProbBars
    ProbBars --  FakeBar
    ProbBars --  RealBar
    ProbBars --  FakePercent
    ProbBars --  RealPercent
    ResultsCard --  Preview
    Preview --  PreviewLabel
    Preview --  AnalyzedContent
    ResultsCard --  ResetBtn
```

**Sources:** [frontend/index.html:67-148](), [frontend/app.js:15-28]()

---

## Application State and Flow

### Global State Variables

The application maintains minimal global state through JavaScript variables defined at [frontend/app.js:1-3]():

- `API_URL`: Backend API endpoint (`\"http://localhost:8000\"`)
- `currentMode`: Current analysis mode (`\"statement\"` or `\"article\"`)

### Application Lifecycle

```mermaid
stateDiagram-v2
    [*] --  DOMLoaded: Document Ready
    DOMLoaded --  HealthCheck: checkApiHealth()
    HealthCheck --  InitialFocus: Focus content-input
    InitialFocus --  Idle: Ready for Input
    
    Idle --  ModeSwitch: Click mode button
    ModeSwitch --  Idle: switchMode()
    
    Idle --  Validating: Click analyze / Press Enter
    Validating --  ShowError: Input empty
    ShowError --  Idle: Click dismiss
    
    Validating --  Loading: Input valid
    Loading --  FetchAPI: POST request
    FetchAPI --  ProcessResponse: Success
    FetchAPI --  ShowError: Error
    
    ProcessResponse --  ShowResults: Display results
    ShowResults --  ResultsDisplayed: Animations complete
    
    ResultsDisplayed --  Idle: Click reset
```

**Sources:** [frontend/app.js:67-115](), [frontend/app.js:177-194](), [frontend/app.js:224-227]()

---

## Event Handling and User Interactions

### Event Listener Registration

All event listeners are registered during initial script execution at [frontend/app.js:32-44]():

| Element | Event | Handler | Description |
|---------|-------|---------|-------------|
| `analyzeBtn` | `click` | `handleAnalyze()` | Triggers analysis request |
| `resetBtn` | `click` | `handleReset()` | Clears results and returns to input |
| `errorDismiss` | `click` | `handleReset()` | Dismisses error message |
| `modeStatementBtn` | `click` | `switchMode(\"statement\")` | Switches to statement mode |
| `modeArticleBtn` | `click` | `switchMode(\"article\")` | Switches to article mode |
| `contentInput` | `keydown` | Enter key handler | Submits statement on Enter (statement mode only) |

### Mode Switching

The `switchMode()` function at [frontend/app.js:46-65]() handles toggling between statement and article analysis modes:

**Mode-Specific UI Changes:**

| Property | Statement Mode | Article Mode |
|----------|---------------|--------------|
| `currentMode` | `\"statement\"` | `\"article\"` |
| `inputLabel.textContent` | \"Enter a statement to verify\" | \"Enter an article to verify\" |
| `contentInput.placeholder` | \"Paste or type a news statement here...\" | \"Paste or type a news article here...\" |
| `contentInput.rows` | 4 | 8 |
| `analyzeBtn` text | \"Analyze Statement\" | \"Analyze Article\" |
| Active button class | `modeStatementBtn.active` | `modeArticleBtn.active` |

**Sources:** [frontend/app.js:46-65]()

---

## API Communication

### Request Flow

```mermaid
sequenceDiagram
    participant User
    participant handleAnalyze
    participant setLoading
    participant fetch
    participant Backend as \"FastAPI br/ (localhost:8000)\"
    participant showResults
    participant showError
    
    User-  handleAnalyze: Click Analyze
    handleAnalyze-  handleAnalyze: Validate input (trim)
    
    alt Input empty
        handleAnalyze-  showError: Display validation error
    else Input valid
        handleAnalyze-  setLoading: setLoading(true)
        handleAnalyze-  fetch: POST /predict or /predict/article
        
        alt Response OK
            fetch-  Backend: HTTP POST with JSON body
            Backend--  fetch: 200 OK + PredictionResponse
            fetch-  showResults: Display results
        else Response Error
            Backend--  fetch: 4xx/5xx + error detail
            fetch-  showError: Display error message
        else Network Error
            fetch-  showError: \"Unable to connect to server\"
        end
        
        handleAnalyze-  setLoading: setLoading(false)
    end
```

**Sources:** [frontend/app.js:67-115]()

### Endpoint Selection

The `handleAnalyze()` function dynamically selects the appropriate endpoint based on `currentMode`:

```javascript
// Statement mode
endpoint = \"/predict\"
body = { statement: content }

// Article mode  
endpoint = \"/predict/article\"
body = { article: content }
```

Implementation at [frontend/app.js:82-83]().

### Request Configuration

All API requests use the following configuration:

- **Method:** `POST`
- **Headers:** `Content-Type: application/json`
- **Body:** JSON-stringified object with `statement` or `article` field
- **Base URL:** `API_URL` constant (`http://localhost:8000`)

**Sources:** [frontend/app.js:85-91]()

### Response Handling

The application expects JSON responses conforming to backend API types (see [API Endpoints](#3.1)):

**Expected Response Structure:**

```javascript
{
  prediction: \"Fake\" | \"Real\",
  confidence: 0.0 - 1.0,
  probabilities: {
    fake: 0.0 - 1.0,
    real: 0.0 - 1.0
  },
  statement: string,  // or article: string
  processing_time: number
}
```

**Sources:** [frontend/app.js:98-99](), [frontend/app.js:117-119]()

### Error Handling

The application handles three error scenarios at [frontend/app.js:100-114]():

1. **HTTP Errors:** Response not OK (status 4xx/5xx) - displays `errorData.detail` or generic server error
2. **Network Errors:** `Failed to fetch` - displays \"Unable to connect to the server\"
3. **Unexpected Errors:** Other exceptions - displays generic error message

---

## Results Display and Animations

### Results Rendering Function

The `showResults()` function at [frontend/app.js:117-155]() orchestrates the display of analysis results through multiple coordinated animations.

```mermaid
graph TB
    showResults[\"showResults(data, isArticle)\"]
    
    Extract[\"Extract data properties br/ prediction, confidence, probabilities\"]
    DetermineVerdict[\"Determine verdict br/ isFake = prediction === 'Fake'\"]
    
    ToggleSections[\"Hide input-section br/ Show results section\"]
    
    UpdateVerdict[\"Update verdict icon   text br/ iconFake/iconReal, verdictText\"]
    UpdateContent[\"Update preview br/ previewLabel, analyzedContent\"]
    
    AnimateRing[\"Animate progress ring br/ strokeDashoffset transition\"]
    AnimateConfidence[\"Animate confidence number br/ animateValue()\"]
    AnimateBars[\"Animate probability bars br/ width transition\"]
    AnimatePercents[\"Animate percent values br/ animateValue()\"]
    
    showResults --  Extract
    Extract --  DetermineVerdict
    DetermineVerdict --  ToggleSections
    ToggleSections --  UpdateVerdict
    ToggleSections --  UpdateContent
    ToggleSections --  AnimateRing
    AnimateRing --  AnimateConfidence
    AnimateConfidence --  AnimateBars
    AnimateBars --  AnimatePercents
```

**Sources:** [frontend/app.js:117-155]()

### Animation Sequence

All animations are triggered after a 100ms delay ([frontend/app.js:140]()) to ensure DOM updates complete before animations begin.

**Animation Timeline:**

| Element | Property | Duration | Easing | Start Value | End Value |
|---------|----------|----------|--------|-------------|-----------|
| Progress Circle | `strokeDashoffset` | 1500ms | `cubic-bezier(0.4, 0, 0.2, 1)` | `CIRCLE_CIRCUMFERENCE` | `CIRCLE_CIRCUMFERENCE - (confidence * CIRCLE_CIRCUMFERENCE)` |
| Confidence Number | `textContent` | 1200ms | Custom ease-out | 0 | `confidence * 100` |
| Fake Bar | `width` | 1200ms | `cubic-bezier(0.4, 0, 0.2, 1)` | 0% | `probabilities.fake * 100%` |
| Real Bar | `width` | 1200ms | `cubic-bezier(0.4, 0, 0.2, 1)` | 0% | `probabilities.real * 100%` |
| Fake Percent | `textContent` | 1000ms | Custom ease-out | 0% | `probabilities.fake * 100%` |
| Real Percent | `textContent` | 1000ms | Custom ease-out | 0% | `probabilities.real * 100%` |

**Sources:** [frontend/app.js:140-154](), [frontend/styles.css:370-374](), [frontend/styles.css:471]()

### Custom Animation Function

The `animateValue()` function at [frontend/app.js:157-175]() provides smooth number counting animations using `requestAnimationFrame`:

**Algorithm:**
1. Calculate elapsed time since animation start
2. Compute progress ratio (0.0 to 1.0)
3. Apply cubic ease-out easing: `1 - (1 - progress)³`
4. Interpolate between start and end values
5. Update element text content
6. Continue until progress reaches 1.0

This creates a decelerating animation that appears natural for numeric transitions.

**Sources:** [frontend/app.js:157-175]()

### Circle Progress Ring Calculation

The progress circle uses SVG `stroke-dasharray` and `stroke-dashoffset` to create an animated ring. The circumference is pre-calculated at [frontend/app.js:30]():

```javascript
const CIRCLE_CIRCUMFERENCE = 2 * Math.PI * 52; // 326.73
```

The circle has a radius of 52 pixels ([frontend/index.html:93]()). The offset is calculated at [frontend/app.js:141]():

```javascript
const offset = CIRCLE_CIRCUMFERENCE - confidence * CIRCLE_CIRCUMFERENCE;
```

This creates a clockwise fill from 0% (full offset) to 100% (zero offset).

**Sources:** [frontend/app.js:30](), [frontend/app.js:141-142](), [frontend/styles.css:365-374]()

---

## State Reset and Navigation

The `handleReset()` function at [frontend/app.js:177-194]() returns the application to its initial state:

**Reset Operations:**

1. **Reset Animations:** Set progress circle offset to full, bar widths to 0, numeric displays to 0
2. **Hide Icons:** Add `.hidden` class to both `iconFake` and `iconReal`
3. **Toggle Sections:** Hide results and error sections, show input section
4. **Clear Input:** Empty the `contentInput` textarea
5. **Restore Focus:** Focus the textarea for immediate user input

**Sources:** [frontend/app.js:177-194]()

---

## Loading States

The `setLoading()` function at [frontend/app.js:206-209]() manages the analyze button's loading state:

**Loading State Changes:**

| State | Button Disabled | CSS Class | Visual Effect |
|-------|----------------|-----------|---------------|
| Loading | `true` | `.loading` | Hides button text, shows spinning loader |
| Idle | `false` | (none) | Shows button text, hides loader |

The loading spinner is positioned absolutely and animated with CSS at [frontend/styles.css:271-283](). The spin animation is defined at [frontend/styles.css:638-640]().

**Sources:** [frontend/app.js:206-209](), [frontend/styles.css:263-283](), [frontend/styles.css:638-640]()

---

## Health Check

The `checkApiHealth()` function at [frontend/app.js:211-222]() performs a non-blocking health check on page load:

**Behavior:**
- Sends GET request to `${API_URL}/` (root endpoint)
- Logs warning if API is unreachable or model is not loaded
- Does not block user interaction
- Errors are caught and logged without displaying to user

This provides diagnostic information in the browser console without interrupting the user experience.

**Sources:** [frontend/app.js:211-222](), [frontend/app.js:224-227]()

---

## CSS Architecture and Design System

### CSS Variables

The design system is built on CSS custom properties defined at [frontend/styles.css:2-19]():

**Color Palette:**

| Variable | Value | Usage |
|----------|-------|-------|
| `--primary` | `#6366f1` | Primary brand color, buttons |
| `--primary-dark` | `#4f46e5` | Button hover states |
| `--primary-light` | `#818cf8` | Accents, logo |
| `--fake-color` | `#ef4444` | Fake news indicators |
| `--fake-light` | `#fca5a5` | Fake labels, gradients |
| `--real-color` | `#22c55e` | Real news indicators |
| `--real-light` | `#86efac` | Real labels, gradients |
| `--bg-dark` | `#0f0f1a` | Body background |
| `--bg-card` | `#1a1a2e` | Card backgrounds |
| `--text-primary` | `#ffffff` | Primary text |
| `--text-secondary` | `#a1a1aa` | Secondary text |
| `--text-muted` | `#71717a` | Muted text |
| `--border-color` | `#2d2d44` | Borders, dividers |

**Sources:** [frontend/styles.css:2-19]()

### Animation System

The CSS defines seven keyframe animations at [frontend/styles.css:585-651]():

```mermaid
graph LR
    Animations[\"CSS Animations\"]
    
    FadeIn[\"fadeIn br/ Opacity 0→1\"]
    FadeInUp[\"fadeInUp br/ Translate Y + Fade\"]
    FadeInDown[\"fadeInDown br/ Translate -Y + Fade\"]
    BounceIn[\"bounceIn br/ Scale + Bounce\"]
    Pulse[\"pulse br/ Scale 1→1.1→1\"]
    Spin[\"spin br/ Rotate 360°\"]
    Shake[\"shake br/ Translate X oscillation\"]
    
    Animations --  FadeIn
    Animations --  FadeInUp
    Animations --  FadeInDown
    Animations --  BounceIn
    Animations --  Pulse
    Animations --  Spin
    Animations --  Shake
```

**Animation Applications:**

| Animation | Target | CSS Declaration | Duration |
|-----------|--------|----------------|----------|
| `fadeInDown` | `.header` | Line 72 | 0.8s |
| `fadeInUp` | `.input-section` | Line 121 | 0.8s (0.2s delay) |
| `fadeInUp` | `.results-section` | Line 287 | 0.6s |
| `bounceIn` | `.verdict-icon` | Line 306 | 0.6s |
| `fadeIn` | `.verdict-text` | Line 328 | 0.6s (0.2s delay) |
| `pulse` | `.logo-icon` | Line 86 | 2s (infinite) |
| `spin` | `.btn-loader` | Line 282 | 0.8s (infinite) |
| `shake` | `.error-section` | Line 527 | 0.5s |

**Sources:** [frontend/styles.css:585-651](), [frontend/styles.css:72](), [frontend/styles.css:121](), [frontend/styles.css:287](), [frontend/styles.css:306](), [frontend/styles.css:328](), [frontend/styles.css:86](), [frontend/styles.css:282](), [frontend/styles.css:527]()

### Responsive Design

Media queries at [frontend/styles.css:654-685]() adapt the layout for mobile devices (max-width: 640px):

**Mobile Adaptations:**

| Element | Desktop | Mobile |
|---------|---------|--------|
| `.container` padding | `2rem 1.5rem` | `1.5rem 1rem` |
| `.logo-text` font-size | `3rem` | `2.25rem` |
| `.logo-icon` font-size | `2.5rem` | `2rem` |
| `.input-card`, `.results-card` padding | `2rem` / `2.5rem 2rem` | `1.5rem` |
| `.input-card`, `.results-card` border-radius | `1.5rem` | `1rem` |
| `.verdict-text` font-size | `2rem` | `1.5rem` |
| `.probability-ring` dimensions | `140px × 140px` | `120px × 120px` |
| `.confidence-number` font-size | `2.5rem` | `2rem` |

**Sources:** [frontend/styles.css:654-685]()

### Interactive States

The CSS implements comprehensive hover, focus, and active states for all interactive elements:

**Button Interactions:**

- **Analyze Button** ([frontend/styles.css:217-255]()): 
  - Hover: `translateY(-2px)` + shadow
  - Shimmer effect on hover (pseudo-element animation)
  - Active: `translateY(0)`
  
- **Mode Buttons** ([frontend/styles.css:157-190]()): 
  - Active state: Primary background color
  - Hover (inactive): Subtle primary tint background

**Input States:**

- **Textarea Focus** ([frontend/styles.css:210-214]()): Primary border + 4px focus ring
- **Card Focus-Within** ([frontend/styles.css:133-136]()): Primary border + shadow + focus ring

**Sources:** [frontend/styles.css:133-136](), [frontend/styles.css:157-190](), [frontend/styles.css:210-214](), [frontend/styles.css:217-255]()

---

## Background Gradient Effect

The `.background-gradient` element at [frontend/styles.css:43-55]() creates an atmospheric background using layered radial gradients:

```css
background: 
  radial-gradient(ellipse at 20% 20%, rgba(99, 102, 241, 0.15), transparent 50%),
  radial-gradient(ellipse at 80% 80%, rgba(139, 92, 246, 0.1), transparent 50%),
  radial-gradient(ellipse at 50% 50%, rgba(99, 102, 241, 0.05), transparent 70%);
```

This creates subtle blue-purple glows positioned strategically across the viewport. The element is positioned fixed with `pointer-events: none` to prevent interference with user interactions.

**Sources:** [frontend/styles.css:43-55](), [frontend/index.html:13]()

---

## Error Display

The `showError()` function at [frontend/app.js:196-200]() and `hideError()` at [frontend/app.js:202-204]() manage error message display:

**Error Display Behavior:**

1. Updates `errorMessage.textContent` with error string
2. Removes `.hidden` class from `errorSection`
3. Adds `.hidden` class to `resultsSection` (hides results if shown)
4. Error section animates in with shake animation ([frontend/styles.css:527]())

**Error Section Structure:**

The error card at [frontend/index.html:150-163]() includes:
- Warning triangle icon (SVG)
- Error message text
- \"Try Again\" dismiss button

**Sources:** [frontend/app.js:196-204](), [frontend/index.html:150-163](), [frontend/styles.css:526-569]()

---

## Font Loading and Typography

The application uses the Inter font family from Google Fonts with selective weight loading for performance:

**Loaded Font Weights:**

- 300 (Light)
- 400 (Regular)
- 500 (Medium)
- 600 (Semi-Bold)
- 700 (Bold)
- 800 (Extra-Bold)

Font declaration at [frontend/index.html:7-9]() uses `preconnect` for DNS optimization. The `display=swap` parameter ensures text remains visible during font loading.

**Typography Scale:**

| Element | Font Size | Font Weight |
|---------|-----------|-------------|
| `.logo-text` | 3rem (48px) | 800 |
| `.tagline` | 1.1rem | 400 |
| `.verdict-text` | 2rem | 700 |
| `.confidence-number` | 2.5rem | 700 |
| Body text | 1rem (16px) | 400 |
| Labels | 0.875rem (14px) | 500 |

**Sources:** [frontend/index.html:7-9](), [frontend/styles.css:28-40](), [frontend/styles.css:97-111](), [frontend/styles.css:325-337](), [frontend/styles.css:392-396]()

---

## Performance Considerations

### Animation Performance

- **GPU Acceleration:** Transform and opacity animations leverage GPU for smooth 60fps rendering
- **RequestAnimationFrame:** Custom number animations use `requestAnimationFrame` for optimal timing ([frontend/app.js:157-175]())
- **Cubic-Bezier Easing:** CSS transitions use `cubic-bezier(0.4, 0, 0.2, 1)` for natural deceleration
- **Staggered Timing:** Result animations are staggered with 100ms delay to prevent layout thrashing ([frontend/app.js:140]())

### DOM Manipulation

- **Minimal Reflows:** Batch DOM updates before triggering animations
- **Class Toggles:** Use `.hidden` class instead of inline style changes for better performance
- **Element Reuse:** DOM elements persist and toggle visibility rather than being created/destroyed

### Network Optimization

- **Font Preconnect:** DNS prefetch for Google Fonts at [frontend/index.html:7-8]()
- **Error Recovery:** Graceful degradation when API is unavailable with user-friendly error messages
- **Health Check:** Non-blocking async health check doesn't delay page interaction

**Sources:** [frontend/app.js:140](), [frontend/app.js:157-175](), [frontend/styles.css:18](), [frontend/styles.css:370-374]()

---

## Browser Compatibility

The codebase uses modern web standards with broad browser support:

**JavaScript APIs Used:**

- `fetch()` - Supported in all modern browsers
- `async/await` - ES2017, widely supported
- `requestAnimationFrame()` - Universal support
- Template literals - ES2015, universal support
- Arrow functions - ES2015, universal support

**CSS Features:**

- CSS Custom Properties (variables) - Supported in all modern browsers
- CSS Grid - Universal modern support
- Flexbox - Universal support
- CSS Animations - Universal support
- `backdrop-filter` - Not used (good cross-browser compatibility)

**Target Support:** Chrome 90+, Firefox 88+, Safari 14+, Edge 90+

**Sources:** [frontend/app.js:1-227](), [frontend/styles.css:1-686]()"])</script><script>self.__next_f.push([1,"20:T416e,"])</script><script>self.__next_f.push([1,"# User Interface Structure

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [frontend/index.html](frontend/index.html)
- [frontend/styles.css](frontend/styles.css)

 /details 



## Purpose and Scope

This document describes the HTML structure and component organization of the UnFake web interface. It covers the DOM hierarchy, element identifiers, semantic markup, and how major UI sections are organized in [frontend/index.html](). 

For information about the JavaScript that drives interactivity, API communication, and state management, see [Application Logic and Interactivity](#4.2). For details about CSS styling, animations, and the design system, see [Styling and Design System](#4.3).

**Sources:** [frontend/index.html:1-173]()

---

## Overall DOM Structure

The interface is organized as a single-page application with five primary sections, all contained within a `main.container` element. The design uses a show/hide pattern where the `.hidden` utility class controls visibility rather than dynamically creating/destroying DOM elements.

```mermaid
graph TB
    body[\"body\"]
    gradient[\"div.background-gradient\"]
    main[\"main.container\"]
    footer[\"footer.footer\"]
    
    header[\"header.header\"]
    inputSection[\"section.input-section\"]
    resultsSection[\"section#results.results-section\"]
    errorSection[\"section#error.error-section\"]
    
    body --  gradient
    body --  main
    body --  footer
    
    main --  header
    main --  inputSection
    main --  resultsSection
    main --  errorSection
    
    style main fill:#f9f9f9
    style resultsSection fill:#e8e8e8
    style errorSection fill:#e8e8e8
```

| Element | ID/Class | Initial State | Purpose |
|---------|----------|---------------|---------|
| Background | `.background-gradient` | Visible | Fixed gradient overlay |
| Container | `main.container` | Visible | Main content wrapper (max-width: 700px) |
| Header | `header.header` | Visible | Logo and tagline |
| Input Section | `section.input-section` | Visible | User input interface |
| Results Section | `section#results` | Hidden | Analysis results display |
| Error Section | `section#error` | Hidden | Error message display |
| Footer | `footer.footer` | Visible | Attribution text |

**Sources:** [frontend/index.html:12-173]()

---

## Header Component

The header provides branding and context for the application. It uses a flex layout to center the logo icon and text horizontally.

```mermaid
graph LR
    header[\"header.header\"]
    logo[\"div.logo\"]
    logoIcon[\"span.logo-icon\"]
    svg[\"svg (magnifying glass)\"]
    logoText[\"h1.logo-text\"]
    highlight[\"span.highlight\"]
    tagline[\"p.tagline\"]
    
    header --  logo
    header --  tagline
    logo --  logoIcon
    logo --  logoText
    logoIcon --  svg
    logoText --  highlight
```

### Header Elements

| Element | Selector | Content | Line Reference |
|---------|----------|---------|----------------|
| Logo Icon | `.logo-icon svg` | Magnifying glass SVG (24x24 viewBox) | [frontend/index.html:19-24]() |
| Logo Text | `h1.logo-text` | \"Un**Fake**\" | [frontend/index.html:25]() |
| Highlight | `span.highlight` | \"Fake\" (styled differently) | [frontend/index.html:25]() |
| Tagline | `p.tagline` | \"AI-Powered Fake News Detection\" | [frontend/index.html:27]() |

The header uses semantic HTML5 with a ` header ` element containing heading and paragraph tags. The magnifying glass icon is an inline SVG with stroke-based rendering.

**Sources:** [frontend/index.html:16-28]()

---

## Input Section Structure

The input section contains all interactive elements for submitting content for analysis. It is wrapped in a card layout (`div.input-card`) that provides visual containment and focus indication.

```mermaid
graph TB
    inputSection[\"section.input-section\"]
    inputCard[\"div.input-card\"]
    
    modeToggle[\"div.mode-toggle\"]
    modeStmt[\"button#mode-statement.mode-btn.active\"]
    modeArt[\"button#mode-article.mode-btn\"]
    
    label[\"label#input-label\"]
    textarea[\"textarea#content-input\"]
    analyzeBtn[\"button#analyze-btn.analyze-btn\"]
    btnText[\"span.btn-text\"]
    btnLoader[\"span.btn-loader\"]
    
    inputSection --  inputCard
    inputCard --  modeToggle
    inputCard --  label
    inputCard --  textarea
    inputCard --  analyzeBtn
    
    modeToggle --  modeStmt
    modeToggle --  modeArt
    analyzeBtn --  btnText
    analyzeBtn --  btnLoader
```

### Mode Toggle Component

The mode toggle allows users to switch between statement and article analysis modes. Both buttons contain an SVG icon and text label.

| Element | ID | Data Attribute | Initial State | Line Reference |
|---------|----|----|---------------|----------------|
| Statement Button | `#mode-statement` | `data-mode=\"statement\"` | Active (`.active` class) | [frontend/index.html:35-40]() |
| Article Button | `#mode-article` | `data-mode=\"article\"` | Inactive | [frontend/index.html:41-50]() |

**Icon Details:**
- Statement: Chat bubble SVG (path element with message box shape)
- Article: Document SVG (path elements forming a paper with folded corner)

**Sources:** [frontend/index.html:34-51]()

### Input Controls

| Element | ID | Type | Key Attributes | Line Reference |
|---------|----|----|----------------|----------------|
| Label | `#input-label` | ` label ` | `for=\"content-input\"` | [frontend/index.html:53]() |
| Textarea | `#content-input` | ` textarea ` | `rows=\"4\"`, resizable | [frontend/index.html:54-59]() |
| Analyze Button | `#analyze-btn` | ` button ` | Contains text and loader spans | [frontend/index.html:60-63]() |

The textarea has a `placeholder` attribute that changes based on mode (\"Paste or type a news statement here...\" initially). The analyze button contains two child spans:
- `span.btn-text`: Displays \"Analyze Statement\" (text changes with mode)
- `span.btn-loader`: Loading spinner (hidden by default via CSS opacity)

**Sources:** [frontend/index.html:53-63]()

---

## Results Section Structure

The results section displays analysis outcomes with multiple visualization components. It remains hidden (`.hidden` class) until analysis completes successfully.

### Results Section Hierarchy

```mermaid
graph TB
    resultsSection[\"section#results.results-section.hidden\"]
    resultsCard[\"div.results-card\"]
    
    verdictContainer[\"div.verdict-container\"]
    verdictIcon[\"div#verdict-icon.verdict-icon\"]
    iconFake[\"svg#icon-fake.verdict-svg.fake.hidden\"]
    iconReal[\"svg#icon-real.verdict-svg.real.hidden\"]
    verdictText[\"h2#verdict-text.verdict-text\"]
    
    probContainer[\"div.probability-container\"]
    probRing[\"div.probability-ring\"]
    svg[\"svg.progress-ring\"]
    bgCircle[\"circle.progress-ring-bg\"]
    progressCircle[\"circle#progress-circle.progress-ring-circle\"]
    probValue[\"div.probability-value\"]
    confNumber[\"span#confidence-value.confidence-number\"]
    probLabel[\"p.probability-label\"]
    
    probBars[\"div.probability-bars\"]
    
    preview[\"div.statement-preview\"]
    previewLabel[\"p#preview-label.preview-label\"]
    previewText[\"p#analyzed-content.preview-text\"]
    
    resetBtn[\"button#reset-btn.reset-btn\"]
    
    resultsSection --  resultsCard
    resultsCard --  verdictContainer
    resultsCard --  probContainer
    resultsCard --  probBars
    resultsCard --  preview
    resultsCard --  resetBtn
    
    verdictContainer --  verdictIcon
    verdictContainer --  verdictText
    verdictIcon --  iconFake
    verdictIcon --  iconReal
    
    probContainer --  probRing
    probContainer --  probLabel
    probRing --  svg
    probRing --  probValue
    svg --  bgCircle
    svg --  progressCircle
    probValue --  confNumber
    
    preview --  previewLabel
    preview --  previewText
```

**Sources:** [frontend/index.html:67-148]()

### Verdict Display Component

The verdict component shows the classification result with an icon and text label. Both \"fake\" and \"real\" icons exist in the DOM simultaneously; visibility is controlled by the `.hidden` class.

| Element | ID | Purpose | Icon Type | Line Reference |
|---------|----|----|-----------|----------------|
| Icon Container | `#verdict-icon` | Wrapper for both icons | N/A | [frontend/index.html:72]() |
| Fake Icon | `#icon-fake` | X-circle SVG | Circle with X paths | [frontend/index.html:74-78]() |
| Real Icon | `#icon-real` | Check-circle SVG | Circle with checkmark polyline | [frontend/index.html:80-83]() |
| Verdict Text | `#verdict-text` | Heading for verdict | ` h2 ` | [frontend/index.html:85]() |

Both SVG icons use a 24x24 viewBox with stroke-based rendering (no fill), rendered at 64x64 pixels.

**Sources:** [frontend/index.html:71-86]()

### Probability Ring Component

The probability ring provides a circular progress indicator using SVG. The circle's `stroke-dashoffset` property is animated to fill clockwise from the top.

#### SVG Structure

```mermaid
graph LR
    ring[\"div.probability-ring\"]
    svg[\"svg.progress-ring viewBox='0 0 120 120'\"]
    bgCircle[\"circle.progress-ring-bg cx=60 cy=60 r=52\"]
    progressCircle[\"circle#progress-circle cx=60 cy=60 r=52\"]
    value[\"div.probability-value\"]
    number[\"span#confidence-value\"]
    
    ring --  svg
    ring --  value
    svg --  bgCircle
    svg --  progressCircle
    value --  number
```

| Circle Element | Class | Styling Role | Stroke Properties |
|----------------|-------|--------------|-------------------|
| Background | `.progress-ring-bg` | Static background track | `stroke-width: 8` |
| Progress | `.progress-ring-circle` (ID: `#progress-circle`) | Animated progress indicator | `stroke-width: 8`, `stroke-dasharray: 326.73`, `stroke-dashoffset: 326.73` |

The `stroke-dasharray` value of 326.73 represents the circle's circumference (2πr = 2π×52 ≈ 326.73). The progress circle is rotated -90° via CSS transform to start from the top.

**Sources:** [frontend/index.html:88-101]()

### Probability Bars Component

The probability bars show individual class probabilities as horizontal bars. Each bar has a header with label/icon and percentage value, plus a track/fill structure.

```mermaid
graph TB
    bars[\"div.probability-bars\"]
    
    fakeContainer[\"div.prob-bar-container (Fake)\"]
    fakeHeader[\"div.prob-bar-header\"]
    fakeLabel[\"span.prob-label.fake-label\"]
    fakeSvg[\"svg (X icon)\"]
    fakePercent[\"span#fake-percent.prob-value\"]
    fakeTrack[\"div.prob-bar-track\"]
    fakeBar[\"div#fake-bar.prob-bar.fake-bar\"]
    
    realContainer[\"div.prob-bar-container (Real)\"]
    realHeader[\"div.prob-bar-header\"]
    realLabel[\"span.prob-label.real-label\"]
    realSvg[\"svg (Check icon)\"]
    realPercent[\"span#real-percent.prob-value\"]
    realTrack[\"div.prob-bar-track\"]
    realBar[\"div#real-bar.prob-bar.real-bar\"]
    
    bars --  fakeContainer
    bars --  realContainer
    
    fakeContainer --  fakeHeader
    fakeContainer --  fakeTrack
    fakeHeader --  fakeLabel
    fakeHeader --  fakePercent
    fakeLabel --  fakeSvg
    fakeTrack --  fakeBar
    
    realContainer --  realHeader
    realContainer --  realTrack
    realHeader --  realLabel
    realHeader --  realPercent
    realLabel --  realSvg
    realTrack --  realBar
```

#### Bar Elements

| Bar | IDs | Purpose | Line Reference |
|-----|-----|---------|----------------|
| Fake | `#fake-percent` (value), `#fake-bar` (fill) | Shows probability of fake classification | [frontend/index.html:105-120]() |
| Real | `#real-percent` (value), `#real-bar` (fill) | Shows probability of real classification | [frontend/index.html:121-135]() |

Each bar uses a track-and-fill pattern:
- `.prob-bar-track`: Fixed-width container (background)
- `.prob-bar`: Inner element with animated `width` property (0 to percentage)

**Sources:** [frontend/index.html:103-135]()

### Content Preview Component

The preview component displays the analyzed text back to the user for reference.

| Element | ID | Purpose | Line Reference |
|---------|----|----|----------------|
| Preview Label | `#preview-label` | Section heading (\"Analyzed Statement\") | [frontend/index.html:139]() |
| Preview Text | `#analyzed-content` | Displays submitted content | [frontend/index.html:140]() |

The label text changes between \"Analyzed Statement\" and \"Analyzed Article\" based on mode.

**Sources:** [frontend/index.html:137-141]()

### Reset Button

| Element | ID | Text Content | Line Reference |
|---------|----|----|----------------|
| Reset Button | `#reset-btn` | \"Analyze Another Statement\" | [frontend/index.html:144-146]() |

The button text changes with mode (\"Analyze Another Statement\" or \"Analyze Another Article\").

**Sources:** [frontend/index.html:143-146]()

---

## Error Section Structure

The error section displays error messages with a warning icon and dismiss button. Like the results section, it is hidden by default.

```mermaid
graph TB
    errorSection[\"section#error.error-section.hidden\"]
    errorCard[\"div.error-card\"]
    errorIcon[\"span.error-icon\"]
    svg[\"svg (warning triangle)\"]
    errorMsg[\"p#error-message.error-message\"]
    dismissBtn[\"button#error-dismiss.error-dismiss\"]
    
    errorSection --  errorCard
    errorCard --  errorIcon
    errorCard --  errorMsg
    errorCard --  dismissBtn
    errorIcon --  svg
```

### Error Components

| Element | ID/Class | Content/Purpose | Line Reference |
|---------|----------|-----------------|----------------|
| Error Icon | `.error-icon` | Warning triangle SVG (48x48) | [frontend/index.html:153-159]() |
| Error Message | `#error-message` | Dynamic error text | [frontend/index.html:160]() |
| Dismiss Button | `#error-dismiss` | \"Try Again\" button | [frontend/index.html:161]() |

The warning icon SVG includes:
- Triangle path (alert symbol outline)
- Vertical line path (exclamation mark stem)
- Dot path (exclamation mark dot)

**Sources:** [frontend/index.html:150-163]()

---

## Footer Component

The footer provides attribution and model information.

| Element | Class | Content | Line Reference |
|---------|-------|---------|----------------|
| Footer | `footer.footer` | \"Powered by RoBERTa AI Model • Built with ❤️\" | [frontend/index.html:167-169]() |

**Sources:** [frontend/index.html:166-169]()

---

## External Dependencies

The HTML includes external resources loaded via CDN:

### Fonts
- **Google Fonts**: Inter font family (weights: 300, 400, 500, 600, 700, 800)
- Preconnect directives for `fonts.googleapis.com` and `fonts.gstatic.com` optimize loading

**Sources:** [frontend/index.html:7-9]()

### Stylesheets and Scripts

| Resource | Path | Purpose | Line Reference |
|----------|------|---------|----------------|
| Stylesheet | `styles.css` | Application styling | [frontend/index.html:10]() |
| Script | `app.js` | Application logic | [frontend/index.html:171]() |

Both resources are loaded from relative paths, indicating they are served from the same origin as the HTML.

**Sources:** [frontend/index.html:10](), [frontend/index.html:171]()

---

## Element Identification Summary

This table provides a quick reference for all interactive elements with IDs that are accessed by JavaScript:

| ID | Element Type | Parent Section | Purpose |
|----|--------------|----------------|---------|
| `mode-statement` | ` button ` | Input Section | Switch to statement mode |
| `mode-article` | ` button ` | Input Section | Switch to article mode |
| `input-label` | ` label ` | Input Section | Dynamic label for textarea |
| `content-input` | ` textarea ` | Input Section | Text input field |
| `analyze-btn` | ` button ` | Input Section | Submit for analysis |
| `results` | ` section ` | Results Section | Container for all results |
| `icon-fake` | ` svg ` | Results Section | Fake verdict icon |
| `icon-real` | ` svg ` | Results Section | Real verdict icon |
| `verdict-text` | ` h2 ` | Results Section | Verdict label text |
| `progress-circle` | ` circle ` | Results Section | SVG progress indicator |
| `confidence-value` | ` span ` | Results Section | Confidence percentage |
| `fake-percent` | ` span ` | Results Section | Fake probability text |
| `fake-bar` | ` div ` | Results Section | Fake probability bar fill |
| `real-percent` | ` span ` | Results Section | Real probability text |
| `real-bar` | ` div ` | Results Section | Real probability bar fill |
| `preview-label` | ` p ` | Results Section | Preview heading |
| `analyzed-content` | ` p ` | Results Section | Content display |
| `reset-btn` | ` button ` | Results Section | Return to input |
| `error` | ` section ` | Error Section | Error container |
| `error-message` | ` p ` | Error Section | Error text |
| `error-dismiss` | ` button ` | Error Section | Dismiss error |

**Sources:** [frontend/index.html:1-173]()"])</script><script>self.__next_f.push([1,"21:T47f8,"])</script><script>self.__next_f.push([1,"# Application Logic and Interactivity

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [frontend/app.js](frontend/app.js)

 /details 



## Purpose and Scope

This document covers the JavaScript implementation in [frontend/app.js]() that powers the UnFake web interface. It explains event handling, state management, API communication, result display logic, and animations. For UI structure and HTML elements, see [4.1](#4.1). For visual styling and CSS animations, see [4.3](#4.3). For backend API endpoints and their specifications, see [3.1](#3.1).

**Sources**: [frontend/app.js:1-228]()

---

## Application State and Global Variables

The application maintains minimal global state using JavaScript variables and DOM element references. State management is explicit and mutation-based rather than using a reactive framework.

### State Variables

| Variable | Type | Purpose |
|----------|------|---------|
| `currentMode` | `string` | Tracks whether user is analyzing a \"statement\" or \"article\" |
| `API_URL` | `string` | Backend API base URL (`http://localhost:8000`) |
| `CIRCLE_CIRCUMFERENCE` | `number` | SVG circle circumference constant for progress animation (2π × 52) |

**Sources**: [frontend/app.js:1-30]()

### DOM Element References

The application caches references to all interactive DOM elements at initialization to avoid repeated DOM queries:

```mermaid
graph TB
    subgraph \"Input Controls\"
        contentInput[\"contentInput br/ #content-input\"]
        inputLabel[\"inputLabel br/ #input-label\"]
        analyzeBtn[\"analyzeBtn br/ #analyze-btn\"]
        inputSection[\"inputSection br/ .input-section\"]
    end
    
    subgraph \"Mode Toggle\"
        modeStatementBtn[\"modeStatementBtn br/ #mode-statement\"]
        modeArticleBtn[\"modeArticleBtn br/ #mode-article\"]
    end
    
    subgraph \"Results Display\"
        resultsSection[\"resultsSection br/ #results\"]
        iconFake[\"iconFake br/ #icon-fake\"]
        iconReal[\"iconReal br/ #icon-real\"]
        verdictText[\"verdictText br/ #verdict-text\"]
        confidenceValue[\"confidenceValue br/ #confidence-value\"]
        progressCircle[\"progressCircle br/ #progress-circle\"]
        analyzedContent[\"analyzedContent br/ #analyzed-content\"]
        resetBtn[\"resetBtn br/ #reset-btn\"]
    end
    
    subgraph \"Probability Bars\"
        fakeBar[\"fakeBar br/ #fake-bar\"]
        realBar[\"realBar br/ #real-bar\"]
        fakePercent[\"fakePercent br/ #fake-percent\"]
        realPercent[\"realPercent br/ #real-percent\"]
    end
    
    subgraph \"Error Display\"
        errorSection[\"errorSection br/ #error\"]
        errorMessage[\"errorMessage br/ #error-message\"]
        errorDismiss[\"errorDismiss br/ #error-dismiss\"]
    end
```

**Sources**: [frontend/app.js:5-28]()

---

## Event Handling Architecture

The application uses a centralized event listener pattern, attaching handlers during initialization and routing events to dedicated handler functions.

### Event Listener Registration

```mermaid
graph LR
    subgraph \"Event Listeners\"
        ClickAnalyze[\"analyzeBtn.click\"]
        ClickReset[\"resetBtn.click\"]
        ClickDismiss[\"errorDismiss.click\"]
        ClickStatement[\"modeStatementBtn.click\"]
        ClickArticle[\"modeArticleBtn.click\"]
        KeydownInput[\"contentInput.keydown\"]
    end
    
    subgraph \"Handler Functions\"
        handleAnalyze[\"handleAnalyze()\"]
        handleReset[\"handleReset()\"]
        switchMode[\"switchMode(mode)\"]
    end
    
    ClickAnalyze -- |\"direct call\"| handleAnalyze
    ClickReset -- |\"direct call\"| handleReset
    ClickDismiss -- |\"direct call\"| handleReset
    ClickStatement -- |\"switchMode('statement')\"| switchMode
    ClickArticle -- |\"switchMode('article')\"| switchMode
    KeydownInput -- |\"if Enter    !Shift    statement mode\"| handleAnalyze
```

The event listeners are registered at [frontend/app.js:32-44](). Key behaviors:

- **Enter key in statement mode**: Triggers analysis, but only if Shift is not held (allowing multiline input in article mode)
- **Mode buttons**: Call `switchMode()` with the appropriate mode string
- **Error dismiss**: Reuses the `handleReset()` function for cleanup

**Sources**: [frontend/app.js:32-44]()

---

## Mode Switching Logic

The `switchMode(mode)` function [frontend/app.js:46-65]() handles toggling between statement and article analysis modes. This affects UI labels, input dimensions, and button text.

### Mode Switching Flow

```mermaid
stateDiagram-v2
    [*] --  statement: Initial state
    statement --  article: switchMode(\"article\")
    article --  statement: switchMode(\"statement\")
    
    state statement {
        [*] --  UpdateGlobal: currentMode = \"statement\"
        UpdateGlobal --  ToggleButtons: modeStatementBtn.active
        ToggleButtons --  UpdateLabels: inputLabel = \"Enter a statement...\"
        UpdateLabels --  UpdatePlaceholder: placeholder = \"Paste or type...\"
        UpdatePlaceholder --  SetRows: contentInput.rows = 4
        SetRows --  SetBtnText: btn.text = \"Analyze Statement\"
        SetBtnText --  [*]: focus input
    }
    
    state article {
        [*] --  UpdateGlobal: currentMode = \"article\"
        UpdateGlobal --  ToggleButtons: modeArticleBtn.active
        ToggleButtons --  UpdateLabels: inputLabel = \"Enter an article...\"
        UpdateLabels --  UpdatePlaceholder: placeholder = \"Paste or type...\"
        UpdatePlaceholder --  SetRows: contentInput.rows = 8
        SetRows --  SetBtnText: btn.text = \"Analyze Article\"
        SetBtnText --  [*]: focus input
    }
```

### Mode-Specific Configuration

| Property | Statement Mode | Article Mode |
|----------|---------------|--------------|
| Label | \"Enter a statement to verify\" | \"Enter an article to verify\" |
| Placeholder | \"Paste or type a news statement here...\" | \"Paste or type a news article here...\" |
| Textarea Rows | 4 | 8 |
| Button Text | \"Analyze Statement\" | \"Analyze Article\" |
| API Endpoint | `/predict` | `/predict/article` |
| Request Body Key | `statement` | `article` |

The mode is stored in the `currentMode` global variable and influences behavior in `handleAnalyze()`.

**Sources**: [frontend/app.js:46-65]()

---

## API Communication

The `handleAnalyze()` function [frontend/app.js:67-115]() orchestrates the entire analysis request lifecycle.

### Analysis Request Flow

```mermaid
sequenceDiagram
    participant User
    participant handleAnalyze
    participant contentInput
    participant setLoading
    participant fetch
    participant API as \"FastAPI Backend\"
    participant showResults
    participant showError
    
    User-  handleAnalyze: Click Analyze or Press Enter
    handleAnalyze-  contentInput: contentInput.value.trim()
    
    alt Empty content
        handleAnalyze-  showError: \"Please enter...\"
        showError-  User: Display error with shake
    else Content provided
        handleAnalyze-  setLoading: setLoading(true)
        handleAnalyze-  hideError: hideError()
        handleAnalyze-  handleAnalyze: Determine endpoint   body
        
        alt Article mode
            handleAnalyze-  fetch: POST /predict/article br/ {article: content}
        else Statement mode
            handleAnalyze-  fetch: POST /predict br/ {statement: content}
        end
        
        fetch-  API: HTTP POST with JSON
        
        alt Success (200 OK)
            API--  fetch: PredictionResponse JSON
            fetch--  handleAnalyze: Response data
            handleAnalyze-  showResults: showResults(data, isArticle)
            showResults-  User: Display results with animations
        else Error (4xx/5xx)
            API--  fetch: Error response
            fetch--  handleAnalyze: Throw Error
            handleAnalyze-  showError: showError(error.message)
            showError-  User: Display error
        else Network failure
            fetch--  handleAnalyze: Throw \"Failed to fetch\"
            handleAnalyze-  showError: \"Unable to connect...\"
            showError-  User: Display error
        end
        
        handleAnalyze-  setLoading: setLoading(false)
    end
```

### Request Construction

The function dynamically constructs the API request based on `currentMode` [frontend/app.js:82-91]():

```javascript
const endpoint = isArticle ? \"/predict/article\" : \"/predict\";
const body = isArticle ? { article: content } : { statement: content };

const response = await fetch(`${API_URL}${endpoint}`, {
  method: \"POST\",
  headers: { \"Content-Type\": \"application/json\" },
  body: JSON.stringify(body),
});
```

### Error Handling Strategy

The try-catch block [frontend/app.js:100-114]() implements granular error handling:

- **Network errors**: Detected via `error.message.includes(\"Failed to fetch\")`, displays connection message
- **Server errors**: Extracted from response JSON's `detail` field or falls back to status code
- **Unknown errors**: Generic message displayed as last resort

**Sources**: [frontend/app.js:67-115]()

---

## Results Display and Animation

The `showResults(data, isArticle)` function [frontend/app.js:117-155]() coordinates multiple animations and DOM updates to present prediction results.

### Results Display Flow

```mermaid
graph TB
    showResults[\"showResults(data, isArticle)\"]
    
    showResults --  ExtractData[\"Extract: br/ prediction, confidence, probabilities\"]
    ExtractData --  DetermineVerdict[\"isFake = prediction === 'Fake'\"]
    
    DetermineVerdict --  HideInput[\"inputSection.classList.add('hidden')\"]
    HideInput --  ShowResults[\"resultsSection.classList.remove('hidden')\"]
    
    ShowResults --  ToggleIcons[\"Toggle iconFake/iconReal visibility\"]
    ToggleIcons --  SetVerdict[\"Set verdictText.textContent   className\"]
    SetVerdict --  SetPreview[\"Set previewLabel   analyzedContent\"]
    
    SetPreview --  SetProgressClass[\"progressCircle.className = fake/real\"]
    
    SetProgressClass --  Delay[\"setTimeout(..., 100ms)\"]
    
    Delay --  AnimCircle[\"progressCircle.strokeDashoffset br/ = offset calculation\"]
    AnimCircle --  AnimConfidence[\"animateValue(confidenceValue, 0, %, 1200)\"]
    AnimConfidence --  SetBars[\"fakeBar.width = x% br/ realBar.width = y%\"]
    SetBars --  AnimFake[\"animateValue(fakePercent, 0, x, 1000)\"]
    AnimFake --  AnimReal[\"animateValue(realPercent, 0, y, 1000)\"]
```

### Animation Coordination

The function uses a 100ms `setTimeout` [frontend/app.js:140-154]() to trigger animations after the DOM has updated, ensuring smooth visual transitions.

**Progress Circle Animation**: The SVG circle's `stroke-dashoffset` is calculated to represent confidence:

```javascript
const offset = CIRCLE_CIRCUMFERENCE - confidence * CIRCLE_CIRCUMFERENCE;
progressCircle.style.strokeDashoffset = offset;
```

The CSS transition property (defined in styles.css) animates this change over 1.5 seconds with cubic-bezier easing.

**Value Counter Animation**: The `animateValue()` function [frontend/app.js:157-175]() implements a smooth counting animation:

```mermaid
graph LR
    Start[\"animateValue(element, start, end, duration)\"]
    Start --  GetStartTime[\"startTime = performance.now()\"]
    GetStartTime --  DefineUpdate[\"Define update(currentTime) function\"]
    DefineUpdate --  CalcProgress[\"elapsed / duration\"]
    CalcProgress --  EaseOut[\"easeOut = 1 - (1-progress)³\"]
    EaseOut --  CalcCurrent[\"current = start + (end-start) * easeOut\"]
    CalcCurrent --  SetText[\"element.textContent = current + suffix\"]
    SetText --  CheckProgress{progress   1?}
    CheckProgress -- |Yes| NextFrame[\"requestAnimationFrame(update)\"]
    NextFrame --  CalcProgress
    CheckProgress -- |No| End[\"Animation complete\"]
```

This creates a smooth, ease-out animation where numbers count up from 0 to their target values. The cubic ease-out formula (`1 - (1-progress)³`) provides natural deceleration.

**Sources**: [frontend/app.js:117-175]()

---

## Reset and Cleanup Logic

The `handleReset()` function [frontend/app.js:177-194]() returns the application to its initial state by resetting all visual elements and clearing input.

### Reset Operations

| Element | Reset Action | Value |
|---------|--------------|-------|
| `progressCircle.strokeDashoffset` | Full circle | `CIRCLE_CIRCUMFERENCE` |
| `fakeBar.width` | Empty bar | `\"0\"` |
| `realBar.width` | Empty bar | `\"0\"` |
| `confidenceValue.textContent` | Zero display | `\"0\"` |
| `fakePercent.textContent` | Zero percent | `\"0%\"` |
| `realPercent.textContent` | Zero percent | `\"0%\"` |
| `iconFake` / `iconReal` | Hide both | `.classList.add(\"hidden\")` |
| `inputSection` | Show input | `.classList.remove(\"hidden\")` |
| `resultsSection` | Hide results | `.classList.add(\"hidden\")` |
| `errorSection` | Hide errors | `.classList.add(\"hidden\")` |
| `contentInput.value` | Clear text | `\"\"` |
| Focus | Return to input | `contentInput.focus()` |

This function is called from both the reset button and the error dismiss button, providing consistent cleanup behavior.

**Sources**: [frontend/app.js:177-194]()

---

## Loading State Management

The `setLoading(isLoading)` function [frontend/app.js:206-209]() manages the button's loading state during API requests.

```mermaid
stateDiagram-v2
    [*] --  Idle: setLoading(false)
    Idle --  Loading: setLoading(true)
    Loading --  Idle: setLoading(false)
    
    state Idle {
        [*] --  EnableBtn: analyzeBtn.disabled = false
        EnableBtn --  RemoveClass: classList.remove(\"loading\")
        RemoveClass --  [*]
    }
    
    state Loading {
        [*] --  DisableBtn: analyzeBtn.disabled = true
        DisableBtn --  AddClass: classList.add(\"loading\")
        AddClass --  [*]: Spinner visible via CSS
    }
```

When loading:
- Button is disabled to prevent double-submission
- `.loading` class added, triggering CSS spinner animation
- Button text remains visible alongside spinner

**Sources**: [frontend/app.js:206-209]()

---

## Error Display System

Error handling uses two functions: `showError(message)` [frontend/app.js:196-200]() and `hideError()` [frontend/app.js:202-204]().

### Error Display Flow

```mermaid
graph TB
    ErrorOccurs[\"Error occurs in handleAnalyze()\"]
    
    ErrorOccurs --  CheckType{Error type?}
    
    CheckType -- |\"Network\"| SetNetworkMsg[\"message = 'Unable to connect...'\"]
    CheckType -- |\"Server\"| SetServerMsg[\"message = errorData.detail || status\"]
    CheckType -- |\"Unknown\"| SetGenericMsg[\"message = 'Unexpected error...'\"]
    
    SetNetworkMsg --  CallShowError[\"showError(message)\"]
    SetServerMsg --  CallShowError
    SetGenericMsg --  CallShowError
    
    CallShowError --  SetMessage[\"errorMessage.textContent = message\"]
    SetMessage --  ShowSection[\"errorSection.classList.remove('hidden')\"]
    ShowSection --  HideResults[\"resultsSection.classList.add('hidden')\"]
    
    HideResults --  UserSees[\"User sees error with shake animation\"]
    UserSees --  UserClicks[\"User clicks dismiss button\"]
    UserClicks --  HandleReset[\"handleReset() called\"]
    HandleReset --  HideError[\"errorSection.classList.add('hidden')\"]
```

The error section uses a shake animation (defined in CSS) to draw attention when displayed.

**Sources**: [frontend/app.js:196-204](), [frontend/app.js:100-114]()

---

## Initialization and Health Check

The application initializes on DOM content load [frontend/app.js:224-227]() and performs an API health check.

### Initialization Sequence

```mermaid
sequenceDiagram
    participant Browser
    participant DOMContentLoaded
    participant checkApiHealth
    participant API as \"FastAPI /\"
    participant contentInput
    
    Browser-  DOMContentLoaded: Page loaded
    DOMContentLoaded-  checkApiHealth: checkApiHealth()
    checkApiHealth-  API: GET http://localhost:8000/
    
    alt API available
        API--  checkApiHealth: {status, model_loaded}
        
        alt model_loaded = false
            checkApiHealth-  checkApiHealth: console.warn()
        else model_loaded = true
            checkApiHealth-  checkApiHealth: Silent success
        end
    else API unavailable
        API--  checkApiHealth: Network error
        checkApiHealth-  checkApiHealth: console.warn()
    end
    
    DOMContentLoaded-  contentInput: contentInput.focus()
    DOMContentLoaded--  Browser: Ready for input
```

### Health Check Function

The `checkApiHealth()` function [frontend/app.js:211-222]() performs a non-blocking check:

- Fetches from `GET /` endpoint
- Logs warnings if model not loaded or API unreachable
- Does not block UI initialization
- Provides early detection of backend issues

This allows the application to load even if the API is temporarily unavailable, with user feedback occurring when they attempt analysis.

**Sources**: [frontend/app.js:211-227]()

---

## Component Interaction Summary

The following diagram illustrates how all major functions interact to implement the complete user workflow:

```mermaid
graph TB
    subgraph \"Initialization\"
        DOMLoad[\"DOMContentLoaded event\"]
        DOMLoad --  Health[\"checkApiHealth()\"]
        DOMLoad --  Focus[\"contentInput.focus()\"]
    end
    
    subgraph \"Mode Selection\"
        ModeBtn[\"Mode button click\"]
        ModeBtn --  Switch[\"switchMode(mode)\"]
        Switch --  UpdateUI[\"Update labels, placeholders, rows\"]
    end
    
    subgraph \"Analysis Flow\"
        UserInput[\"User enters text\"]
        UserInput --  Trigger[\"Click Analyze or Enter key\"]
        Trigger --  Analyze[\"handleAnalyze()\"]
        
        Analyze --  Validate[\"Validate input\"]
        Validate -- |Empty| ShowErr[\"showError()\"]
        Validate -- |Valid| Loading[\"setLoading(true)\"]
        
        Loading --  APICall[\"fetch(API_URL + endpoint)\"]
        APICall -- |Success| Show[\"showResults(data)\"]
        APICall -- |Error| ShowErr
        
        Show --  Animate[\"animateValue() × 3\"]
        Animate --  Display[\"User sees results\"]
    end
    
    subgraph \"Reset Flow\"
        ResetClick[\"Reset/Dismiss button\"]
        ResetClick --  Reset[\"handleReset()\"]
        Reset --  Clear[\"Clear all values\"]
        Reset --  Hide[\"Hide results/errors\"]
        Reset --  ShowInput[\"Show input section\"]
        Reset --  FocusInput[\"Focus input\"]
    end
    
    Health -.- |Background| APICall
    Display --  ResetClick
    ShowErr --  ResetClick
```

**Sources**: [frontend/app.js:1-228]()"])</script><script>self.__next_f.push([1,"22:T4fda,"])</script><script>self.__next_f.push([1,"# Styling and Design System

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [frontend/index.html](frontend/index.html)
- [frontend/styles.css](frontend/styles.css)

 /details 



This document describes the CSS architecture, design system, and visual styling implementation for the UnFake frontend application. It covers the theming system built with CSS variables, component-specific styling, animation framework, and responsive design strategy. For information about the HTML structure and component organization, see [User Interface Structure](#4.1). For details on how JavaScript coordinates animations and state changes, see [Application Logic and Interactivity](#4.2).

## Overview

The styling system is implemented entirely in [frontend/styles.css]() and provides a comprehensive design language for the single-page application. The architecture uses CSS custom properties (variables) for theming, keyframe animations for visual feedback, and media queries for responsive behavior. The design system establishes consistent spacing, typography, colors, and transitions across all UI components.

**Sources:** [frontend/styles.css:1-686]()

## CSS Architecture

The stylesheet is organized into distinct sections that follow the component hierarchy of the HTML structure:

```mermaid
graph TD
    Variables[\"CSS Variables br/ :root br/ (lines 1-19)\"]
    Reset[\"Reset   Base Styles br/ (lines 21-40)\"]
    Layout[\"Layout Components\"]
    Interactive[\"Interactive Components\"]
    Utilities[\"Utility Classes   Animations\"]
    
    Variables --  Reset
    Reset --  Layout
    Reset --  Interactive
    
    Layout --  BgGrad[\"background-gradient br/ (lines 42-55)\"]
    Layout --  Container[\"container br/ (lines 57-66)\"]
    Layout --  Header[\"header, logo, tagline br/ (lines 68-117)\"]
    Layout --  Footer[\"footer br/ (lines 571-577)\"]
    
    Interactive --  InputSec[\"input-section br/ input-card br/ (lines 119-214)\"]
    Interactive --  ModeToggle[\"mode-toggle br/ mode-btn br/ (lines 146-190)\"]
    Interactive --  AnalyzeBtn[\"analyze-btn br/ (lines 216-283)\"]
    Interactive --  Results[\"results-section br/ results-card br/ (lines 285-523)\"]
    Interactive --  Error[\"error-section br/ (lines 525-569)\"]
    
    Results --  Verdict[\"verdict-container br/ (lines 298-337)\"]
    Results --  ProbRing[\"probability-ring br/ (lines 339-409)\"]
    Results --  ProbBars[\"probability-bars br/ (lines 411-480)\"]
    
    Utilities --  Hidden[\"hidden br/ (line 580-582)\"]
    Utilities --  Animations[\"Keyframes br/ (lines 584-651)\"]
    Utilities --  Responsive[\"Media Queries br/ (lines 653-685)\"]
```

**Sources:** [frontend/styles.css:1-686]()

## Design Tokens (CSS Variables)

The design system uses CSS custom properties defined in the `:root` selector to establish a consistent theming foundation. These variables are referenced throughout the stylesheet, enabling centralized color and transition management.

### Color System

| Variable | Value | Purpose |
|----------|-------|---------|
| `--primary` | `#6366f1` | Primary brand color (indigo) |
| `--primary-dark` | `#4f46e5` | Darker shade for hover states |
| `--primary-light` | `#818cf8` | Lighter shade for accents |
| `--fake-color` | `#ef4444` | Red color for fake news indicators |
| `--fake-light` | `#fca5a5` | Light red for subtle fake indicators |
| `--real-color` | `#22c55e` | Green color for real news indicators |
| `--real-light` | `#86efac` | Light green for subtle real indicators |
| `--bg-dark` | `#0f0f1a` | Main background color (dark navy) |
| `--bg-card` | `#1a1a2e` | Card background (lighter navy) |
| `--bg-card-hover` | `#252542` | Card hover state |
| `--text-primary` | `#ffffff` | Primary text color (white) |
| `--text-secondary` | `#a1a1aa` | Secondary text (gray) |
| `--text-muted` | `#71717a` | Muted text for labels |
| `--border-color` | `#2d2d44` | Border color for cards and inputs |

**Sources:** [frontend/styles.css:1-19]()

### Transition and Shadow System

```css
--shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
--transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
```

The `--transition` variable uses a custom cubic-bezier easing function (`cubic-bezier(0.4, 0, 0.2, 1)`) that creates a smooth, natural motion effect. This easing is applied consistently to hover states, focus states, and interactive elements throughout the application.

**Sources:** [frontend/styles.css:17-18]()

## Typography System

The application uses the **Inter** font family loaded from Google Fonts with multiple weights (300, 400, 500, 600, 700, 800):

```mermaid
graph LR
    GoogleFonts[\"Google Fonts API\"]
    InterFont[\"Inter Font Family br/ weights: 300-800\"]
    Body[\"body br/ font-family: Inter\"]
    
    Headers[\"Headers br/ .logo-text: 800 br/ .verdict-text: 700\"]
    Labels[\"Labels br/ .tagline: 400 br/ .input-label: 500\"]
    Buttons[\"Buttons br/ .analyze-btn: 600 br/ .mode-btn: 500\"]
    
    GoogleFonts --  InterFont
    InterFont --  Body
    Body --  Headers
    Body --  Labels
    Body --  Buttons
```

Font sizes follow a consistent scale:
- **Large display**: 3rem (`.logo-text`)
- **Headers**: 2rem (`.verdict-text`), 1.5rem (mobile)
- **Body**: 1rem (default)
- **Small text**: 0.95rem (`.preview-text`), 0.875rem (labels)
- **Micro text**: 0.75rem (`.preview-label`)

**Sources:** [frontend/index.html:7-9](), [frontend/styles.css:28-40, 97-117, 326-337]()

## Component Styling Architecture

The following diagram maps CSS class names to their corresponding HTML elements, showing how styles are applied to the component hierarchy:

```mermaid
graph TB
    subgraph \"HTML Structure (index.html)\"
        ContainerDiv[\"div.container\"]
        
        HeaderEl[\"header.header\"]
        LogoDiv[\"div.logo\"]
        LogoIcon[\"span.logo-icon\"]
        LogoText[\"h1.logo-text\"]
        Tagline[\"p.tagline\"]
        
        InputSec[\"section.input-section\"]
        InputCard[\"div.input-card\"]
        ModeToggle[\"div.mode-toggle\"]
        ModeBtns[\"button.mode-btn\"]
        InputLabel[\"label.input-label\"]
        TextArea[\"textarea.statement-input\"]
        AnalyzeBtn[\"button.analyze-btn\"]
        
        ResultsSec[\"section.results-section\"]
        ResultsCard[\"div.results-card\"]
        VerdictCont[\"div.verdict-container\"]
        VerdictIcon[\"div.verdict-icon\"]
        ProbContainer[\"div.probability-container\"]
        ProgressRing[\"svg.progress-ring\"]
        ProgressCircle[\"circle.progress-ring-circle\"]
        ProbBars[\"div.probability-bars\"]
        FakeBar[\"div.fake-bar\"]
        RealBar[\"div.real-bar\"]
        
        ErrorSec[\"section.error-section\"]
        ErrorCard[\"div.error-card\"]
    end
    
    subgraph \"CSS Styling (styles.css)\"
        ContainerStyle[\"container br/ max-width: 700px br/ padding: 2rem\"]
        
        HeaderStyle[\"header br/ text-align: center br/ animation: fadeInDown\"]
        LogoStyle[\"logo br/ display: flex br/ gap: 0.75rem\"]
        LogoIconStyle[\"logo-icon br/ animation: pulse\"]
        LogoTextStyle[\"logo-text br/ font-size: 3rem br/ gradient text\"]
        
        InputSecStyle[\"input-section br/ animation: fadeInUp\"]
        InputCardStyle[\"input-card br/ bg: var(--bg-card) br/ border-radius: 1.5rem\"]
        ModeToggleStyle[\"mode-toggle br/ display: flex br/ gap: 0.5rem\"]
        ModeBtnStyle[\"mode-btn br/ transition: var(--transition) br/ active: bg primary\"]
        TextAreaStyle[\"statement-input br/ resize: vertical br/ min-height: 120px\"]
        AnalyzeBtnStyle[\"analyze-btn br/ gradient background br/ loading states\"]
        
        ResultsSecStyle[\"results-section br/ animation: fadeInUp\"]
        ResultsCardStyle[\"results-card br/ padding: 2.5rem\"]
        VerdictStyle[\"verdict-container br/ animation: bounceIn\"]
        ProbRingStyle[\"probability-ring br/ width: 140px br/ position: relative\"]
        ProgressCircleStyle[\"progress-ring-circle br/ stroke-dasharray: 326.73 br/ transition: 1.5s\"]
        ProbBarsStyle[\"probability-bars br/ gap: 1.25rem br/ transition: width 1.2s\"]
        
        ErrorSecStyle[\"error-section br/ animation: shake\"]
        ErrorCardStyle[\"error-card br/ border: 1px solid fake-color\"]
    end
    
    ContainerDiv --  ContainerStyle
    
    HeaderEl --  HeaderStyle
    LogoDiv --  LogoStyle
    LogoIcon --  LogoIconStyle
    LogoText --  LogoTextStyle
    Tagline --  HeaderStyle
    
    InputSec --  InputSecStyle
    InputCard --  InputCardStyle
    ModeToggle --  ModeToggleStyle
    ModeBtns --  ModeBtnStyle
    TextArea --  TextAreaStyle
    AnalyzeBtn --  AnalyzeBtnStyle
    
    ResultsSec --  ResultsSecStyle
    ResultsCard --  ResultsCardStyle
    VerdictCont --  VerdictStyle
    ProgressRing --  ProbRingStyle
    ProgressCircle --  ProgressCircleStyle
    ProbBars --  ProbBarsStyle
    
    ErrorSec --  ErrorSecStyle
    ErrorCard --  ErrorCardStyle
```

**Sources:** [frontend/index.html:15-164](), [frontend/styles.css:57-577]()

### Input Components

The input section implements a card-based design with rounded corners and focus states:

**Card Container**: [frontend/styles.css:124-136]()
- Background: `var(--bg-card)` with 1px border in `var(--border-color)`
- Border radius: `1.5rem` for modern, rounded appearance
- Focus-within effect: border color changes to `var(--primary)` with a subtle glow shadow

**Mode Toggle**: [frontend/styles.css:147-190]()
- Implements a segmented control pattern with two buttons
- Active state: `background: var(--primary)` with white text
- Inactive state: transparent background with `var(--text-muted)` color
- Hover effect adds a subtle primary-colored background

**Textarea Input**: [frontend/styles.css:192-214]()
- Dark background (`var(--bg-dark)`) for contrast against card
- Minimum height of 120px with vertical resize capability
- Focus state: border changes to primary color with 4px shadow ring

### Button Styling

The `.analyze-btn` class implements a gradient button with sophisticated interaction states:

```mermaid
graph LR
    DefaultState[\"Default State br/ gradient background br/ primary → primary-dark\"]
    HoverState[\"Hover State br/ translateY(-2px) br/ enhanced shadow\"]
    ActiveState[\"Active State br/ translateY(0) br/ pressed effect\"]
    LoadingState[\"Loading State br/ .btn-text: opacity 0 br/ .btn-loader: opacity 1\"]
    DisabledState[\"Disabled State br/ opacity: 0.7 br/ cursor: not-allowed\"]
    
    DefaultState -- |\"mouse enter\"| HoverState
    HoverState -- |\"mouse leave\"| DefaultState
    HoverState -- |\"click\"| ActiveState
    ActiveState -- |\"release\"| HoverState
    DefaultState -- |\"click   analyzing\"| LoadingState
    LoadingState -- |\"analysis complete\"| DefaultState
    DefaultState -- |\":disabled attribute\"| DisabledState
```

The button also implements a shine effect using a pseudo-element that slides across on hover: [frontend/styles.css:233-246]()

**Sources:** [frontend/styles.css:217-283]()

### Results Display Components

**Verdict Container**: [frontend/styles.css:298-337]()
- Centers the verdict icon and text
- Icon animates with `bounceIn` keyframe (scale effect)
- Text color changes dynamically: `.fake` class applies `var(--fake-color)`, `.real` class applies `var(--real-color)`

**Circular Progress Ring**: [frontend/styles.css:339-409]()

The circular progress indicator is implemented using SVG stroke techniques:

| Property | Value | Purpose |
|----------|-------|---------|
| `stroke-dasharray` | `326.73` | Total circumference (2πr where r=52) |
| `stroke-dashoffset` | `326.73` → calculated | Animated from full to percentage |
| `transform` | `rotate(-90deg)` | Start from top (12 o'clock) |
| `transition` | `1.5s cubic-bezier(0.4, 0, 0.2, 1)` | Smooth animation easing |

The JavaScript in [frontend/app.js]() calculates the stroke-dashoffset based on confidence percentage: `offset = circumference - (confidence / 100) * circumference`.

**Probability Bars**: [frontend/styles.css:411-480]()
- Two horizontal bars with gradient fills
- Fake bar: gradient from `var(--fake-color)` to `var(--fake-light)`
- Real bar: gradient from `var(--real-color)` to `var(--real-light)`
- Width animates from 0 to percentage over 1.2 seconds

**Sources:** [frontend/styles.css:285-523]()

## Animation Framework

The application implements seven keyframe animations that provide visual feedback for different interactions:

```mermaid
graph TB
    subgraph \"Page Load Animations\"
        FadeInDown[\"@keyframes fadeInDown br/ header entrance br/ translateY(-30px) → 0\"]
        FadeInUp[\"@keyframes fadeInUp br/ section entrance br/ translateY(30px) → 0\"]
    end
    
    subgraph \"Results Animations\"
        FadeIn[\"@keyframes fadeIn br/ simple opacity br/ 0 → 1\"]
        BounceIn[\"@keyframes bounceIn br/ verdict icon br/ scale(0.3) → 1.1 → 0.9 → 1\"]
    end
    
    subgraph \"Loading Animations\"
        Spin[\"@keyframes spin br/ button loader br/ rotate 360deg\"]
        Pulse[\"@keyframes pulse br/ logo icon br/ scale(1) → 1.1 → 1\"]
    end
    
    subgraph \"Error Animations\"
        Shake[\"@keyframes shake br/ error section br/ translateX(-5px ↔ 5px)\"]
    end
    
    LoadEvent[\"Page Load Event\"] --  FadeInDown
    LoadEvent --  FadeInUp
    
    ShowResults[\"showResults() br/ function call\"] --  FadeIn
    ShowResults --  BounceIn
    
    AnalyzeClick[\"Analyze Button Click\"] --  Spin
    
    LogoRender[\"Logo Render\"] --  Pulse
    
    ErrorEvent[\"showError() br/ function call\"] --  Shake
```

### Animation Specifications

| Animation | Duration | Easing | Applied To | Trigger |
|-----------|----------|--------|------------|---------|
| `fadeInDown` | 0.8s | ease-out | `.header` | Page load |
| `fadeInUp` | 0.8s | ease-out (0.2s delay) | `.input-section` | Page load |
| `fadeInUp` | 0.6s | ease-out | `.results-section` | Results shown |
| `bounceIn` | 0.6s | ease-out | `.verdict-icon` | Results shown |
| `fadeIn` | 0.6s | ease-out (0.2s delay) | `.verdict-text` | Results shown |
| `pulse` | 2s | infinite | `.logo-icon` | Continuous |
| `spin` | 0.8s | linear infinite | `.btn-loader` | Analysis in progress |
| `shake` | 0.5s | ease-out | `.error-section` | Error displayed |

**Sources:** [frontend/styles.css:584-651]()

### Staggered Animation Timing

The results display uses staggered animation delays to create a polished reveal sequence:

1. **Results section fades in** (0s)
2. **Verdict icon bounces in** (0s)
3. **Verdict text fades in** (0.2s delay)
4. **Confidence ring animates** (coordinated via JavaScript)
5. **Probability bars expand** (coordinated via JavaScript)

This is implemented through a combination of CSS animation delays and JavaScript-triggered transitions. See [Application Logic and Interactivity](#4.2) for the JavaScript coordination.

**Sources:** [frontend/styles.css:287-288, 306-309, 328-329]()

## Visual Feedback States

The design system implements comprehensive state management through CSS classes and pseudo-classes:

### Interactive Element States

```mermaid
graph TB
    subgraph \"Button States (.analyze-btn, .mode-btn, .reset-btn)\"
        BtnDefault[\"Default br/ base styles br/ cursor: pointer\"]
        BtnHover[\"Hover br/ transform/background change br/ enhanced shadow\"]
        BtnActive[\"Active/Pressed br/ transform: translateY(0) br/ reduced elevation\"]
        BtnDisabled[\"Disabled br/ opacity: 0.7 br/ cursor: not-allowed\"]
        BtnLoading[\"Loading br/ .loading class br/ spinner visible\"]
        
        BtnDefault --  BtnHover
        BtnHover --  BtnActive
        BtnDefault --  BtnDisabled
        BtnDefault --  BtnLoading
    end
    
    subgraph \"Input States (.statement-input)\"
        InputDefault[\"Default br/ border: var(--border-color)\"]
        InputFocus[\"Focus br/ border: var(--primary) br/ 4px shadow ring\"]
        
        InputDefault --  InputFocus
    end
    
    subgraph \"Card States (.input-card)\"
        CardDefault[\"Default br/ border: var(--border-color)\"]
        CardFocusWithin[\"Focus-within br/ border: var(--primary) br/ shadow ring\"]
        
        CardDefault --  CardFocusWithin
    end
    
    subgraph \"Mode Toggle States (.mode-btn)\"
        ModeInactive[\"Inactive br/ transparent bg br/ muted text\"]
        ModeActive[\"Active (.active class) br/ bg: var(--primary) br/ white text\"]
        
        ModeInactive --  ModeActive
    end
```

**Sources:** [frontend/styles.css:133-136, 173-190, 210-214, 248-261, 519-523]()

### Dynamic Class Application

The JavaScript application adds and removes CSS classes to trigger state changes:

| Class | Applied To | Purpose | Added By |
|-------|------------|---------|----------|
| `.hidden` | Multiple sections | Toggle visibility | `showResults()`, `showError()`, `handleReset()` |
| `.loading` | `.analyze-btn` | Show spinner, hide text | `handleAnalyze()` |
| `.active` | `.mode-btn` | Highlight selected mode | `switchMode()` |
| `.fake` | `.verdict-svg`, `.verdict-text`, `.progress-ring-circle` | Apply red styling | `showResults()` |
| `.real` | `.verdict-svg`, `.verdict-text`, `.progress-ring-circle` | Apply green styling | `showResults()` |

The `.hidden` utility class uses `display: none !important` to ensure visibility toggling takes precedence over other display properties.

**Sources:** [frontend/styles.css:580-582](), [frontend/app.js]() (referenced, see page 4.2)

## Responsive Design Strategy

The application implements a mobile-first responsive design with a single breakpoint at 640px:

```css
@media (max-width: 640px) {
  /* Mobile adaptations */
}
```

### Responsive Adjustments

| Component | Desktop | Mobile (≤640px) |
|-----------|---------|-----------------|
| Container padding | `2rem 1.5rem` | `1.5rem 1rem` |
| Logo text size | `3rem` | `2.25rem` |
| Logo icon size | `2.5rem` | `2rem` |
| Card padding | `2rem` / `2.5rem` | `1.5rem` |
| Border radius | `1.5rem` | `1rem` |
| Verdict text size | `2rem` | `1.5rem` |
| Probability ring size | `140px` × `140px` | `120px` × `120px` |
| Confidence number size | `2.5rem` | `2rem` |

The responsive strategy focuses on:
1. **Reduced whitespace**: Smaller padding values on mobile
2. **Scaled typography**: Proportionally smaller font sizes
3. **Compact components**: Smaller circular progress rings
4. **Tighter border radii**: Less pronounced rounded corners

The layout remains single-column across all breakpoints due to the narrow max-width (700px) of the container.

**Sources:** [frontend/styles.css:653-685]()

## Background and Atmospheric Effects

The application implements a fixed gradient background that creates depth without interfering with content:

**Background Gradient Layer**: [frontend/styles.css:42-55]()
- Fixed position covering the entire viewport
- Three overlapping radial gradients:
  - Top-left (20%, 20%): Primary color (indigo) at 15% opacity
  - Bottom-right (80%, 80%): Purple shade at 10% opacity
  - Center (50%, 50%): Primary color at 5% opacity
- `pointer-events: none` ensures it doesn't interfere with interactions
- `z-index: -1` places it behind all content

This creates a subtle, atmospheric glow effect that reinforces the brand color scheme without overwhelming the interface.

**Sources:** [frontend/styles.css:42-55](), [frontend/index.html:13]()

## Color Application Patterns

The color system follows semantic patterns that convey meaning:

### Status Colors

```mermaid
graph LR
    FakeDetected[\"Fake News Detected\"] --  FakeColor[\"--fake-color: #ef4444 br/ --fake-light: #fca5a5\"]
    RealDetected[\"Real News Detected\"] --  RealColor[\"--real-color: #22c55e br/ --real-light: #86efac\"]
    
    FakeColor --  FakeIcon[\".verdict-svg.fake\"]
    FakeColor --  FakeText[\".verdict-text.fake\"]
    FakeColor --  FakeRing[\".progress-ring-circle.fake\"]
    FakeColor --  FakeBar[\".fake-bar gradient\"]
    FakeColor --  ErrorBorder[\".error-card border\"]
    
    RealColor --  RealIcon[\".verdict-svg.real\"]
    RealColor --  RealText[\".verdict-text.real\"]
    RealColor --  RealRing[\".progress-ring-circle.real\"]
    RealColor --  RealBar[\".real-bar gradient\"]
```

### Interactive Element Colors

- **Primary actions**: Use `var(--primary)` gradient (analyze button, mode toggle active state)
- **Neutral actions**: Use border-only styling with hover effects (reset button)
- **Error actions**: Use `var(--fake-color)` for error dismiss button

**Sources:** [frontend/styles.css:1-19, 217-283, 298-480, 525-569]()"])</script><script>self.__next_f.push([1,"23:T33b4,"])</script><script>self.__next_f.push([1,"# Machine Learning Models

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [data/gradient_boosting_classifier.pkl](data/gradient_boosting_classifier.pkl)
- [data/tfidf_vectorizer.pkl](data/tfidf_vectorizer.pkl)
- [notebooks/train_article.ipynb](notebooks/train_article.ipynb)
- [notebooks/train_headline.ipynb](notebooks/train_headline.ipynb)

 /details 



## Purpose and Scope

This document provides an overview of the dual-model machine learning architecture used in the UnFake fake news detection system. It explains the rationale for using two distinct models, their high-level architectures, and how they integrate with the backend API. For detailed technical information about individual models, see [Headline Model (RoBERTa)](#5.1) and [Article Model (Gradient Boosting)](#5.2). For information about training these models, see [Training Pipeline](#6).

**Sources**: [api/main.py:1-290]()

---

## Dual-Model Architecture Overview

The UnFake system employs two specialized machine learning models optimized for different text input types:

| Model | Text Type | Technology | Max Length | Inference Location |
|-------|-----------|------------|------------|-------------------|
| **Headline Model** | Short statements, headlines | Fine-tuned RoBERTa transformer | 256 tokens | GPU (CUDA) or CPU fallback |
| **Article Model** | Long-form articles | Gradient Boosting + TF-IDF | Unlimited | CPU |

```mermaid
graph TB
    UserInput[User Input]
    
    subgraph \"Model Selection Logic\"
        InputType{Text Type?}
    end
    
    subgraph \"Headline Path - api/main.py:112-151\"
        CleanText[\"clean_text() br/ api/text_processing.py\"]
        Tokenizer[\"AutoTokenizer br/ tokenizer\"]
        RoBERTaModel[\"AutoModelForSequenceClassification br/ headline_model br/ DEVICE: cuda/cpu\"]
        SoftmaxHead[\"torch.softmax() br/ outputs.logits\"]
    end
    
    subgraph \"Article Path - api/main.py:208-239\"
        CleanArticle[\"clean_article() br/ api/text_processing.py\"]
        TfidfVect[\"TfidfVectorizer br/ vectorizer\"]
        GBClassifier[\"GradientBoostingClassifier br/ article_model\"]
        PredictProba[\"predict_proba() br/ predict()\"]
    end
    
    UserInput --  InputType
    InputType -- |Statement| CleanText
    InputType -- |Article| CleanArticle
    
    CleanText --  Tokenizer
    Tokenizer --  RoBERTaModel
    RoBERTaModel --  SoftmaxHead
    SoftmaxHead --  PredResponse[\"PredictionResponse\"]
    
    CleanArticle --  TfidfVect
    TfidfVect --  GBClassifier
    GBClassifier --  PredictProba
    PredictProba --  ArtPredResponse[\"ArticlePredictionResponse\"]
    
    style RoBERTaModel fill:#fff
    style GBClassifier fill:#fff
    style InputType fill:#fff
```

**Sources**: [api/main.py:19-38](), [api/main.py:112-151](), [api/main.py:208-239]()

---

## Model Selection Strategy

The system uses different models for different text types based on the following considerations:

### Text Length Optimization

**Short Statements (Headlines)**:
- Typical length: 50-200 characters
- Require contextual understanding and semantic analysis
- Benefit from transformer attention mechanisms
- RoBERTa's pre-trained language understanding captures nuance

**Long Articles**:
- Typical length: 500-5000+ characters
- Statistical features (word frequency, n-grams) are highly discriminative
- TF-IDF captures document-level patterns effectively
- Gradient Boosting handles high-dimensional sparse features efficiently

### Computational Trade-offs

| Aspect | Headline Model | Article Model |
|--------|---------------|---------------|
| Inference Speed | Slower (~100-300ms) | Faster (~10-50ms) |
| Model Size | ~500MB | ~5MB |
| Memory Usage | High (GPU preferred) | Low (CPU sufficient) |
| Accuracy on Short Text | High | Lower |
| Accuracy on Long Text | Lower (truncation) | High |

**Sources**: [api/main.py:19-24](), [api/main.py:30-38](), [notebooks/train_headline.ipynb:870-878](), [notebooks/train_article.ipynb:88-93]()

---

## Headline Model Overview

The headline model is a **fine-tuned RoBERTa transformer** optimized for binary classification of short statements.

### Key Components

```mermaid
graph LR
    Input[\"Text Input\"]
    Tokenizer[\"AutoTokenizer br/ from HEADLINE_MODEL_DIR\"]
    Model[\"AutoModelForSequenceClassification br/ RoBERTa-base fine-tuned br/ 2 labels: Fake=0, Real=1\"]
    Device[\"DEVICE br/ torch.device('cuda' or 'cpu')\"]
    Output[\"Logits → Softmax br/ Classification\"]
    
    Input --  Tokenizer
    Tokenizer --  Model
    Model --  Device
    Model --  Output
    
    style Model fill:#fff
```

### Model Specifications

- **Base Architecture**: `roberta-base` (12 layers, 768 hidden size)
- **Max Sequence Length**: 256 tokens (`MAX_LENGTH`)
- **Training Data**: PolitiFact statements (~26k samples)
- **Output**: Binary classification (0=Fake, 1=Real)
- **Storage Location**: `data/RoBERTa_Classifier/`

For detailed information about architecture, training process, and inference mechanics, see [Headline Model (RoBERTa)](#5.1).

**Sources**: [api/main.py:19-24](), [api/main.py:60-77](), [notebooks/train_headline.ipynb:869-878]()

---

## Article Model Overview

The article model uses **Gradient Boosting with TF-IDF vectorization** for long-form text classification.

### Key Components

```mermaid
graph LR
    Input[\"Text Input\"]
    Vectorizer[\"TfidfVectorizer br/ from tfidf_vectorizer.pkl\"]
    Features[\"Sparse TF-IDF Matrix\"]
    Model[\"GradientBoostingClassifier br/ from gradient_boosting_classifier.pkl\"]
    Output[\"predict_proba() br/ predict()\"]
    
    Input --  Vectorizer
    Vectorizer --  Features
    Features --  Model
    Model --  Output
    
    style Model fill:#fff
```

### Model Specifications

- **Algorithm**: Gradient Boosting (scikit-learn)
- **Vectorization**: TF-IDF (Term Frequency-Inverse Document Frequency)
- **Training Data**: True.csv + Fake.csv (~45k articles)
- **Output**: Binary classification (0=Fake, 1=Real)
- **Storage**: `data/gradient_boosting_classifier.pkl`, `data/tfidf_vectorizer.pkl`

For detailed information about feature engineering, training process, and model comparison, see [Article Model (Gradient Boosting)](#5.2).

**Sources**: [api/main.py:30-38](), [api/main.py:41-57](), [notebooks/train_article.ipynb:1-275]()

---

## Model Loading and Initialization

Both models are loaded during application startup via the `lifespan` context manager.

### Loading Functions

```mermaid
graph TB
    Lifespan[\"@asynccontextmanager br/ lifespan(app: FastAPI) br/ api/main.py:80-84\"]
    
    subgraph \"Headline Model Loading\"
        LoadHead[\"load_headline_model() br/ api/main.py:60-77\"]
        CheckDir1[\"Check HEADLINE_MODEL_DIR exists\"]
        LoadToken[\"AutoTokenizer.from_pretrained()\"]
        LoadModel1[\"AutoModelForSequenceClassification br/ .from_pretrained()\"]
        MoveDevice[\"model.to(DEVICE) br/ model.eval()\"]
    end
    
    subgraph \"Article Model Loading\"
        LoadArt[\"load_article_model() br/ api/main.py:41-57\"]
        CheckDir2[\"Check pkl files exist\"]
        LoadGBC[\"pickle.load() br/ gradient_boosting_classifier.pkl\"]
        LoadVect[\"pickle.load() br/ tfidf_vectorizer.pkl\"]
    end
    
    Lifespan --  LoadHead
    Lifespan --  LoadArt
    
    LoadHead --  CheckDir1
    CheckDir1 --  LoadToken
    CheckDir1 --  LoadModel1
    LoadModel1 --  MoveDevice
    
    LoadArt --  CheckDir2
    CheckDir2 --  LoadGBC
    CheckDir2 --  LoadVect
    
    style Lifespan fill:#fff
```

### Global Model Variables

The loaded models are stored in module-level global variables:

- `headline_model`: Instance of `AutoModelForSequenceClassification`
- `tokenizer`: Instance of `AutoTokenizer`
- `article_model`: Instance of `GradientBoostingClassifier`
- `vectorizer`: Instance of `TfidfVectorizer`

These variables are initialized as `None` and populated during startup [api/main.py:26-38](). If loading fails, a `RuntimeError` is raised with instructions to train the model first.

**Sources**: [api/main.py:26-38](), [api/main.py:41-57](), [api/main.py:60-77](), [api/main.py:80-84]()

---

## Inference Process Comparison

The two models have distinctly different inference pipelines:

### Headline Model Inference Flow

```mermaid
sequenceDiagram
    participant EP as /predict endpoint
    participant CT as clean_text()
    participant TK as tokenizer
    participant Model as headline_model
    participant Device as DEVICE (GPU/CPU)
    
    EP-  CT: statement text
    CT--  EP: cleaned_text
    EP-  TK: tokenize(max_length=256, padding=\"max_length\")
    TK--  EP: input_ids, attention_mask
    EP-  Device: .to(DEVICE)
    EP-  Model: forward(input_ids, attention_mask)
    Note over Model: with torch.no_grad()
    Model--  EP: outputs.logits
    EP-  EP: torch.softmax(logits)
    EP-  EP: torch.argmax(probs)
    EP--  EP: PredictionResponse
```

**Key Operations**:
- Text cleaning: Basic preprocessing, preserves punctuation [api/text_processing.py]()
- Tokenization: RoBERTa tokenizer, max 256 tokens, padding to fixed length
- Device transfer: Tensors moved to GPU if available
- Inference: `torch.no_grad()` context for efficiency
- Post-processing: Softmax for probabilities, argmax for class

**Sources**: [api/main.py:112-151]()

---

### Article Model Inference Flow

```mermaid
sequenceDiagram
    participant EP as /predict/article endpoint
    participant CA as clean_article()
    participant VEC as vectorizer
    participant Model as article_model
    
    EP-  CA: article text
    CA--  EP: cleaned_text (aggressive cleaning)
    EP-  VEC: transform([cleaned_text])
    VEC--  EP: tfidf_features (sparse matrix)
    EP-  Model: predict_proba(tfidf_features)
    Model--  EP: probabilities [fake, real]
    EP-  Model: predict(tfidf_features)
    Model--  EP: class (0 or 1)
    EP--  EP: ArticlePredictionResponse
```

**Key Operations**:
- Text cleaning: Aggressive preprocessing, removes URLs, HTML, special chars [api/text_processing.py]()
- Vectorization: TF-IDF transformation to sparse feature matrix
- Inference: Standard scikit-learn predict methods
- Post-processing: Extract probabilities from array

**Sources**: [api/main.py:208-239]()

---

## Performance Characteristics

### Model Accuracy

Based on training results:

| Model | Accuracy | F1 Score | Precision | Recall |
|-------|----------|----------|-----------|--------|
| **Headline (RoBERTa)** | ~72% | ~72% | ~72% | ~72% |
| **Article (Gradient Boosting)** | ~99.6% | ~99.6% | ~100% | ~99.6% |

The article model achieves higher accuracy due to:
- Larger training dataset (45k vs 26k samples)
- More discriminative features in long-form text
- Better class balance through preprocessing

**Note**: The headline model's validation performance [notebooks/train_headline.ipynb:1262-1437]() shows room for improvement through hyperparameter tuning or additional training data.

### Resource Requirements

**Headline Model**:
- Model file size: ~500MB
- Memory usage: ~2GB GPU / ~4GB CPU
- Inference time: ~100-300ms per statement (GPU), ~500-1000ms (CPU)
- Optimal batch size: 16-32

**Article Model**:
- Model file size: ~5MB (combined)
- Memory usage: ~100MB
- Inference time: ~10-50ms per article
- Supports large documents without truncation

### Device Selection Logic

The headline model uses automatic device selection [api/main.py:24]():

```python
DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")
```

During loading, the model is moved to the selected device [api/main.py:73]():

```python
headline_model.to(DEVICE)
```

This ensures optimal performance on GPU-enabled systems while maintaining CPU compatibility.

**Sources**: [api/main.py:24](), [api/main.py:73](), [notebooks/train_headline.ipynb:1262-1502](), [notebooks/train_article.ipynb:186-196]()

---

## Model File Locations

The models are loaded from the following paths (relative to project root):

```mermaid
graph TB
    Root[\"Project Root\"]
    
    subgraph \"Headline Model Files\"
        HeadDir[\"data/RoBERTa_Classifier/ br/ HEADLINE_MODEL_DIR\"]
        Config[\"config.json\"]
        Weights[\"pytorch_model.bin / model.safetensors\"]
        TokenConfig[\"tokenizer_config.json\"]
        Vocab[\"vocab.json, merges.txt\"]
    end
    
    subgraph \"Article Model Files\"
        ArtDir[\"data/ br/ ARTICLE_MODEL_DIR, VECTORIZER_DIR\"]
        GBFile[\"gradient_boosting_classifier.pkl\"]
        VecFile[\"tfidf_vectorizer.pkl\"]
    end
    
    Root --  HeadDir
    Root --  ArtDir
    
    HeadDir --  Config
    HeadDir --  Weights
    HeadDir --  TokenConfig
    HeadDir --  Vocab
    
    ArtDir --  GBFile
    ArtDir --  VecFile
    
    style HeadDir fill:#fff
    style ArtDir fill:#fff
```

These paths are defined as constants in [api/main.py:20-34]():

- `HEADLINE_MODEL_DIR`: Path to RoBERTa model directory
- `ARTICLE_MODEL_DIR`: Path to `.pkl` file for Gradient Boosting model
- `VECTORIZER_DIR`: Path to `.pkl` file for TF-IDF vectorizer

If these files are not found, the application raises a `RuntimeError` with instructions to run the training notebooks.

**Sources**: [api/main.py:20-34](), [api/main.py:44-47](), [api/main.py:63-66]()"])</script><script>self.__next_f.push([1,"24:T4718,"])</script><script>self.__next_f.push([1,"# Headline Model (RoBERTa)

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)
- [notebooks/train_headline.ipynb](notebooks/train_headline.ipynb)

 /details 



## Purpose and Scope

This page documents the RoBERTa-based transformer model used for classifying short statements and headlines as \"Real\" or \"Fake\" news. It covers the model architecture, loading process, tokenization pipeline, inference workflow, and device selection logic. This model is specifically optimized for text sequences up to 256 tokens in length.

For information about the alternative model used for longer articles, see [Article Model (Gradient Boosting)](#5.2). For an overview of why the system uses different models for different text types, see [Machine Learning Models](#5). For training details, see [Training the Headline Model](#6.1).

---

## Model Architecture

The headline classifier uses a fine-tuned **RoBERTa** (Robustly Optimized BERT Pretraining Approach) model from HuggingFace Transformers. RoBERTa is a transformer-based language model that improves upon BERT through better training methodology and hyperparameter tuning.

### Model Characteristics

| Property | Value |
|----------|-------|
| **Base Architecture** | `AutoModelForSequenceClassification` |
| **Model Type** | RoBERTa (transformer encoder) |
| **Task Type** | Binary sequence classification |
| **Number of Classes** | 2 (Fake: 0, Real: 1) |
| **Maximum Sequence Length** | 256 tokens |
| **Tokenizer** | `AutoTokenizer` (RoBERTa tokenizer) |
| **Model Storage** | `data/RoBERTa_Classifier/` directory |
| **Inference Mode** | Evaluation mode (no dropout, batch norm in inference) |

The model performs binary classification by processing tokenized text through multiple transformer encoder layers, producing logits for each class which are then converted to probabilities via softmax.

**Sources:** [api/main.py:10-10](), [api/main.py:20-24](), [api/main.py:60-77]()

---

## Model Loading and Initialization

### Loading Process

```mermaid
graph TB
    Startup[\"Application Startup br/ lifespan context manager\"]
    LoadFunc[\"load_headline_model()\"]
    CheckDir{\"Directory Exists? br/ HEADLINE_MODEL_DIR\"}
    LoadTokenizer[\"AutoTokenizer.from_pretrained()\"]
    LoadModel[\"AutoModelForSequenceClassification br/ .from_pretrained()\"]
    MoveDevice[\"model.to(DEVICE)\"]
    SetEval[\"model.eval()\"]
    DeviceCheck{\"torch.cuda.is_available()\"}
    
    Startup --  LoadFunc
    LoadFunc --  CheckDir
    CheckDir -- |No| RaiseError[\"RuntimeError: br/ Model directory not found\"]
    CheckDir -- |Yes| LoadTokenizer
    LoadTokenizer --  LoadModel
    LoadModel --  MoveDevice
    
    DeviceCheck -- |True| GPU[\"DEVICE = cuda\"]
    DeviceCheck -- |False| CPU[\"DEVICE = cpu\"]
    GPU --  MoveDevice
    CPU --  MoveDevice
    
    MoveDevice --  SetEval
    SetEval --  Ready[\"Model Ready br/ headline_model global variable\"]
    
    style Startup fill:#f9f9f9
    style Ready fill:#f9f9f9
    style LoadFunc fill:#e8e8e8
    style RaiseError fill:#e8e8e8
```

The `load_headline_model()` function [api/main.py:60-77]() executes during application startup within the `lifespan` context manager [api/main.py:80-84](). The process:

1. **Directory Validation**: Checks if `HEADLINE_MODEL_DIR` exists [api/main.py:63-67]()
2. **Tokenizer Loading**: Loads the `AutoTokenizer` from the model directory [api/main.py:69]()
3. **Model Loading**: Loads `AutoModelForSequenceClassification` from the model directory [api/main.py:70-72]()
4. **Device Selection**: Determines device based on CUDA availability [api/main.py:24]()
5. **Device Transfer**: Moves model to selected device (GPU or CPU) [api/main.py:73]()
6. **Evaluation Mode**: Sets model to evaluation mode (disables dropout, etc.) [api/main.py:74]()

### Global Variables

| Variable | Type | Purpose |
|----------|------|---------|
| `headline_model` | `AutoModelForSequenceClassification \\| None` | Holds loaded RoBERTa model instance |
| `tokenizer` | `AutoTokenizer \\| None` | Holds RoBERTa tokenizer instance |
| `HEADLINE_MODEL_DIR` | `str` | Path to model directory (`data/RoBERTa_Classifier/`) |
| `MAX_LENGTH` | `int` | Maximum token sequence length (256) |
| `DEVICE` | `torch.device` | Inference device (cuda or cpu) |

**Sources:** [api/main.py:19-27](), [api/main.py:60-77](), [api/main.py:80-84]()

---

## Tokenization Pipeline

### Tokenization Configuration

```mermaid
graph LR
    Input[\"Input Text br/ (cleaned)\"]
    Tokenizer[\"tokenizer()\"]
    
    subgraph \"Tokenization Parameters\"
        AddSpecial[\"add_special_tokens=True br/ [CLS] ... [SEP]\"]
        MaxLen[\"max_length=256\"]
        Padding[\"padding='max_length'\"]
        Truncation[\"truncation=True\"]
        AttMask[\"return_attention_mask=True\"]
        Tensors[\"return_tensors='pt'\"]
    end
    
    Output[\"Encoding Dictionary\"]
    InputIds[\"input_ids br/ torch.Tensor\"]
    AttMaskOut[\"attention_mask br/ torch.Tensor\"]
    
    Input --  Tokenizer
    AddSpecial -.-  Tokenizer
    MaxLen -.-  Tokenizer
    Padding -.-  Tokenizer
    Truncation -.-  Tokenizer
    AttMask -.-  Tokenizer
    Tensors -.-  Tokenizer
    
    Tokenizer --  Output
    Output --  InputIds
    Output --  AttMaskOut
    
    style Input fill:#f9f9f9
    style Output fill:#f9f9f9
```

The tokenizer [api/main.py:122-130]() converts preprocessed text into model-ready tensors with the following configuration:

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `add_special_tokens` | `True` | Adds `[CLS]` token at start, `[SEP]` at end |
| `max_length` | `256` | Maximum sequence length (defined in `MAX_LENGTH`) |
| `padding` | `\"max_length\"` | Pads sequences to exactly 256 tokens |
| `truncation` | `True` | Truncates sequences longer than 256 tokens |
| `return_attention_mask` | `True` | Returns mask indicating real vs. padded tokens |
| `return_tensors` | `\"pt\"` | Returns PyTorch tensors |

### Tokenization Output

The tokenizer returns an encoding dictionary containing:
- **`input_ids`**: Tensor of token IDs (shape: `[1, 256]`)
- **`attention_mask`**: Binary mask where `1` = real token, `0` = padding (shape: `[1, 256]`)

Both tensors are moved to the inference device (GPU or CPU) before model inference [api/main.py:132-133]().

**Sources:** [api/main.py:122-130](), [api/main.py:132-133]()

---

## Text Preprocessing

Before tokenization, input text undergoes preprocessing via `clean_text()` [api/text_processing.py:4-9]():

### Preprocessing Steps

```python
def clean_text(text: str) -  str:
    text = text.lower()                           # Convert to lowercase
    text = re.sub(r\"[^a-z0-9\\s,.!?]\", \" \", text)  # Keep only alphanumeric + basic punctuation
    text = re.sub(r\"\\s+\", \" \", text)              # Normalize whitespace
    text = text.strip()                           # Remove leading/trailing whitespace
    return text
```

This preprocessing:
- **Normalizes case**: Converts all characters to lowercase
- **Preserves punctuation**: Retains periods, commas, exclamation marks, and question marks (important for statement context)
- **Removes special characters**: Eliminates emojis, symbols, unusual characters
- **Normalizes whitespace**: Collapses multiple spaces into single spaces

The preprocessing is intentionally lightweight to preserve punctuation cues that may be semantically meaningful for fake news detection in statements.

**Sources:** [api/text_processing.py:4-9](), [api/main.py:120]()

---

## Inference Pipeline

### Complete Inference Workflow

```mermaid
graph TB
    Request[\"POST /predict br/ StatementRequest\"]
    Validate[\"Validate Request br/ StatementRequest schema\"]
    CheckModel{\"Model Loaded? br/ headline_model is not None\"}
    CheckEmpty{\"Text Empty? br/ statement.strip()\"}
    
    CleanText[\"clean_text(statement)\"]
    Tokenize[\"tokenizer(cleaned_text) br/ max_length=256\"]
    MoveDevice[\"Move tensors to DEVICE br/ input_ids, attention_mask\"]
    
    NoGrad[\"torch.no_grad() context\"]
    Forward[\"headline_model(input_ids, attention_mask)\"]
    Softmax[\"torch.softmax(logits, dim=1)\"]
    Argmax[\"torch.argmax(probs, dim=1)\"]
    
    ExtractProbs[\"Extract probabilities: br/ prob_fake = probs[0][0] br/ prob_real = probs[0][1] br/ confidence = probs[0][pred_class]\"]
    
    MapClass[\"Map class to label: br/ 1 -  'Real' br/ 0 -  'Fake'\"]
    Response[\"PredictionResponse br/ prediction, confidence, probabilities\"]
    
    Request --  Validate
    Validate --  CheckModel
    CheckModel -- |No| Error503[\"HTTPException 503 br/ Model not loaded\"]
    CheckModel -- |Yes| CheckEmpty
    CheckEmpty -- |Yes| Error400[\"HTTPException 400 br/ Statement cannot be empty\"]
    CheckEmpty -- |No| CleanText
    
    CleanText --  Tokenize
    Tokenize --  MoveDevice
    MoveDevice --  NoGrad
    
    NoGrad --  Forward
    Forward --  Softmax
    Softmax --  Argmax
    Argmax --  ExtractProbs
    ExtractProbs --  MapClass
    MapClass --  Response
    
    style Request fill:#f9f9f9
    style Response fill:#f9f9f9
    style NoGrad fill:#e8e8e8
```

### Inference Steps (Single Statement)

The `/predict` endpoint [api/main.py:112-151]() implements the following workflow:

1. **Request Validation**: Validates incoming JSON against `StatementRequest` schema [api/types.py:4-12]()
2. **Model Availability Check**: Ensures `headline_model` and `tokenizer` are loaded [api/main.py:114-115]()
3. **Empty Check**: Validates statement is not empty [api/main.py:117-118]()
4. **Text Cleaning**: Applies `clean_text()` preprocessing [api/main.py:120]()
5. **Tokenization**: Converts text to model inputs [api/main.py:122-130]()
6. **Device Transfer**: Moves tensors to GPU/CPU [api/main.py:132-133]()
7. **Inference**: Forward pass in `torch.no_grad()` context [api/main.py:135-136]()
8. **Probability Calculation**: Applies softmax to logits [api/main.py:137]()
9. **Class Prediction**: Uses argmax to determine class [api/main.py:138]()
10. **Probability Extraction**: Extracts individual class probabilities [api/main.py:140-142]()
11. **Label Mapping**: Maps class index to human-readable label [api/main.py:144]()
12. **Response Construction**: Builds `PredictionResponse` [api/main.py:146-151]()

### Probability Calculation Details

```python
# Logits from model output
outputs = headline_model(input_ids=input_ids, attention_mask=attention_mask)

# Convert logits to probabilities using softmax
probs = torch.softmax(outputs.logits, dim=1)  # Shape: [1, 2]

# Get predicted class (0=Fake, 1=Real)
pred_class = torch.argmax(probs, dim=1).item()

# Extract individual probabilities
prob_fake = probs[0][0].item()  # Probability of class 0
prob_real = probs[0][1].item()  # Probability of class 1
confidence = probs[0][pred_class].item()  # Confidence = prob of predicted class
```

**Sources:** [api/main.py:112-151](), [api/main.py:135-142]()

---

## Batch Prediction

### Batch Processing Architecture

The `/predict/batch` endpoint [api/main.py:154-205]() processes multiple statements in a single request:

| Configuration | Value | Purpose |
|---------------|-------|---------|
| **Maximum Batch Size** | 100 statements | Rate limiting |
| **Processing Strategy** | Sequential iteration | One statement at a time |
| **Error Handling** | Individual failures | Empty statements logged but don't fail entire batch |
| **Return Format** | List of results | Each with prediction, confidence, probabilities |

### Batch Processing Flow

```mermaid
graph TB
    BatchReq[\"POST /predict/batch br/ list[str]\"]
    ValidateList{\"Validate: br/ - Not empty br/ - Max 100 statements\"}
    InitResults[\"results = []\"]
    
    LoopStart[\"For each statement\"]
    CheckEmpty{\"statement.strip()?\"}
    
    SingleInference[\"Execute Single Inference: br/ 1. clean_text() br/ 2. tokenize() br/ 3. to(DEVICE) br/ 4. forward() br/ 5. softmax() br/ 6. extract probs\"]
    
    AppendError[\"Append error entry: br/ {statement, error}\"]
    AppendResult[\"Append result entry: br/ {statement, prediction, br/ confidence, probabilities}\"]
    
    LoopEnd{\"More statements?\"}
    Response[\"Return: br/ {results: [...]} \"]
    
    BatchReq --  ValidateList
    ValidateList -- |Invalid| Error400[\"HTTPException 400\"]
    ValidateList -- |Valid| InitResults
    InitResults --  LoopStart
    
    LoopStart --  CheckEmpty
    CheckEmpty -- |Empty| AppendError
    CheckEmpty -- |Not Empty| SingleInference
    SingleInference --  AppendResult
    
    AppendError --  LoopEnd
    AppendResult --  LoopEnd
    LoopEnd -- |Yes| LoopStart
    LoopEnd -- |No| Response
    
    style BatchReq fill:#f9f9f9
    style Response fill:#f9f9f9
```

Each statement in the batch undergoes the identical inference pipeline as single predictions, with results accumulated in a list. Empty statements are handled gracefully with error entries rather than failing the entire batch.

**Sources:** [api/main.py:154-205]()

---

## Device Selection and GPU Acceleration

### Device Selection Logic

The model automatically selects the optimal compute device at startup [api/main.py:24]():

```python
DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")
```

| Condition | Device | Performance Impact |
|-----------|--------|-------------------|
| CUDA available | GPU (`cuda`) | ~10-100x faster inference |
| CUDA not available | CPU (`cpu`) | Standard CPU inference |

### GPU Optimization

When GPU is available:
- Model is moved to GPU memory via `headline_model.to(DEVICE)` [api/main.py:73]()
- All input tensors are moved to GPU before inference [api/main.py:132-133]()
- Forward pass executes on GPU with CUDA acceleration
- Results are moved back to CPU memory for JSON serialization

### Evaluation Mode

The model is set to evaluation mode [api/main.py:74]() which:
- Disables dropout layers (all neurons active during inference)
- Sets batch normalization layers to use running statistics
- Prevents gradient computation (automatically enforced by `torch.no_grad()`)

**Sources:** [api/main.py:24](), [api/main.py:73-74](), [api/main.py:132-133](), [api/main.py:135]()

---

## Response Format

### PredictionResponse Structure

```python
class PredictionResponse(BaseModel):
    statement: str        # Original input statement
    prediction: str       # \"Real\" or \"Fake\"
    confidence: float     # Probability of predicted class (0-1, rounded to 2 decimals)
    probabilities: dict   # {\"fake\": float, \"real\": float}
```

### Example Response

```json
{
  \"statement\": \"Scientists confirm that regular exercise improves cardiovascular health.\",
  \"prediction\": \"Real\",
  \"confidence\": 0.92,
  \"probabilities\": {
    \"fake\": 0.08,
    \"real\": 0.92
  }
}
```

The `confidence` value equals the probability of the predicted class. For example, if `prediction = \"Real\"`, then `confidence = probabilities[\"real\"]`.

**Sources:** [api/types.py:26-40](), [api/main.py:146-151]()

---

## Performance Considerations

### Inference Latency

| Factor | Impact on Latency |
|--------|------------------|
| **Device Type** | GPU: ~10-50ms, CPU: ~100-500ms per statement |
| **Sequence Length** | Longer sequences require more computation (max 256 tokens) |
| **Batch Size** | Batch endpoint processes sequentially (no parallel speedup) |
| **Model Loading** | One-time startup cost (~1-5 seconds) |

### Memory Requirements

| Component | Approximate Size |
|-----------|-----------------|
| **Model Parameters** | ~125 million parameters (~500MB) |
| **Model Memory** | ~1-2GB GPU memory for inference |
| **Tokenizer** | Negligible (~few MB) |
| **Per-Request Overhead** | ~1-2MB for tensors (256 tokens) |

### Optimization Strategies

1. **GPU Acceleration**: Use CUDA-capable GPU for ~10-100x speedup
2. **Evaluation Mode**: Ensures dropout/batch norm optimizations [api/main.py:74]()
3. **No Gradient Context**: `torch.no_grad()` prevents gradient computation [api/main.py:135]()
4. **Model Caching**: Loaded once at startup, reused for all requests
5. **Truncation**: Sequences limited to 256 tokens for consistent performance

**Sources:** [api/main.py:73-74](), [api/main.py:135]()

---

## Error Handling

### Model Loading Errors

```python
if not os.path.exists(HEADLINE_MODEL_DIR):
    raise RuntimeError(
        f\"Model directory not found: {HEADLINE_MODEL_DIR}. \"
        \"Please train the model first using the train.ipynb notebook.\"
    )
```

Raised during startup if model directory doesn't exist [api/main.py:63-67]().

### Inference Errors

| Error Code | Condition | Message |
|------------|-----------|---------|
| **503** | Model not loaded | \"Model not loaded\" [api/main.py:114-115]() |
| **400** | Empty statement | \"Statement cannot be empty\" [api/main.py:117-118]() |
| **400** | Empty batch | \"Statements list cannot be empty\" [api/main.py:159-160]() |
| **400** | Batch too large | \"Maximum 100 statements per batch\" [api/main.py:162-163]() |

All errors are raised as `HTTPException` with appropriate status codes and descriptive messages.

**Sources:** [api/main.py:63-67](), [api/main.py:114-118](), [api/main.py:159-163]()

---

## Integration Points

### Upstream Integration

- **Frontend**: The frontend [app.js] calls `/predict` with user input statements
- **API Router**: Endpoint defined in FastAPI application [api/main.py:112-151]()
- **Request Validation**: Uses `StatementRequest` from [api/types.py:4-12]()

### Downstream Integration

- **Text Processing**: Calls `clean_text()` from [api/text_processing.py:4-9]()
- **Response Format**: Returns `PredictionResponse` from [api/types.py:26-40]()

### Relationship to Article Model

The headline model is one of two classification models in the system:
- **Headline Model (this page)**: RoBERTa transformer for short statements (≤256 tokens)
- **Article Model (see [5.2](#5.2))**: Gradient Boosting + TF-IDF for longer articles

Both models are loaded during the same startup sequence but serve different endpoints and use different preprocessing strategies.

**Sources:** [api/main.py:80-84](), [api/main.py:112-151](), [api/text_processing.py:4-9]()"])</script><script>self.__next_f.push([1,"25:T4356,"])</script><script>self.__next_f.push([1,"# Article Model (Gradient Boosting)

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [api/main.py](api/main.py)
- [api/text_processing.py](api/text_processing.py)
- [api/types.py](api/types.py)
- [data/gradient_boosting_classifier.pkl](data/gradient_boosting_classifier.pkl)
- [data/tfidf_vectorizer.pkl](data/tfidf_vectorizer.pkl)
- [notebooks/train_article.ipynb](notebooks/train_article.ipynb)

 /details 



## Purpose and Scope

This page documents the Gradient Boosting Classifier used for classifying longer article texts as real or fake news. This model handles full-length news articles (as opposed to short headlines/statements). It uses traditional machine learning techniques: TF-IDF vectorization for feature extraction and a scikit-learn Gradient Boosting Classifier for classification.

For information about the headline/statement classification model that uses deep learning, see [Headline Model (RoBERTa)](#5.1). For the overall ML architecture and model selection rationale, see [Machine Learning Models](#5).

**Sources:** [api/main.py:29-58](), [notebooks/train_article.ipynb:1-274]()

---

## Model Architecture Overview

The article model implements a traditional machine learning pipeline consisting of two serialized components that work in sequence:

```mermaid
graph LR
    Input[\"Raw Article Text\"]
    Clean[\"clean_article()\"]
    Vectorizer[\"TfidfVectorizer\"]
    Features[\"Sparse TF-IDF Matrix\"]
    Classifier[\"GradientBoostingClassifier\"]
    Output[\"Prediction + Probabilities\"]
    
    Input --  Clean
    Clean --  Vectorizer
    Vectorizer --  Features
    Features --  Classifier
    Classifier --  Output
```

**Architecture: Article Classification Pipeline**

The pipeline consists of:

| Component | Type | Purpose | Serialized As |
|-----------|------|---------|---------------|
| Text Preprocessor | `clean_article()` function | Remove noise, normalize text | N/A (code function) |
| Feature Extractor | `TfidfVectorizer` | Convert text to numerical features | `tfidf_vectorizer.pkl` |
| Classifier | `GradientBoostingClassifier` | Binary classification (fake/real) | `gradient_boosting_classifier.pkl` |

**Sources:** [api/main.py:30-38](), [notebooks/train_article.ipynb:88-93]()

---

## Model Artifacts and Storage

The trained model is persisted as two separate pickle files that must be loaded together:

```python
# File paths defined in api/main.py
ARTICLE_MODEL_DIR = os.path.join(
    os.path.dirname(__file__), \"..\", \"data\", \"gradient_boosting_classifier.pkl\"
)
VECTORIZER_DIR = os.path.join(
    os.path.dirname(__file__), \"..\", \"data\", \"tfidf_vectorizer.pkl\"
)
```

### Pickle File Contents

| File | Contains | Size Considerations |
|------|----------|---------------------|
| `gradient_boosting_classifier.pkl` | Trained `GradientBoostingClassifier` with all decision trees | Moderate (~MB range) |
| `tfidf_vectorizer.pkl` | Fitted `TfidfVectorizer` with vocabulary and IDF weights | Large (~MB range, depends on vocabulary size) |

The model is loaded at application startup in the `load_article_model()` function [api/main.py:41-57]():

```python
def load_article_model():
    global article_model, vectorizer
    
    if not os.path.exists(ARTICLE_MODEL_DIR) or not os.path.exists(VECTORIZER_DIR):
        raise RuntimeError(...)
    
    with open(ARTICLE_MODEL_DIR, \"rb\") as f:
        article_model = pickle.load(f)
    
    with open(VECTORIZER_DIR, \"rb\") as f:
        vectorizer = pickle.load(f)
```

**Sources:** [api/main.py:30-38](), [api/main.py:41-57](), [notebooks/train_article.ipynb:205-211]()

---

## Training Pipeline

The model is trained using the Jupyter notebook [notebooks/train_article.ipynb](). The training process follows this sequence:

```mermaid
graph TB
    TrueCSV[\"True.csv br/ (Real news articles)\"]
    FakeCSV[\"Fake.csv br/ (Fake news articles)\"]
    
    Load[\"Load   Label Data br/ true['label'] = 1 br/ fake['label'] = 0\"]
    Concat[\"pd.concat([true, fake])\"]
    Shuffle[\"df.sample(frac=1)\"]
    
    Clean[\"clean_text() br/ Remove URLs, HTML, numbers, etc.\"]
    
    Split[\"train_test_split br/ (test_size=0.25)\"]
    
    FitVect[\"vect.fit_transform(X_train)\"]
    TransVect[\"vect.transform(X_test)\"]
    
    CompareModels[\"Train   Compare: br/ - DecisionTreeClassifier br/ - RandomForestClassifier br/ - GradientBoostingClassifier\"]
    
    BestModel[\"Select Best Model br/ (GradientBoostingClassifier)\"]
    
    Save[\"Pickle Serialization br/ gradient_boosting_classifier.pkl br/ tfidf_vectorizer.pkl\"]
    
    TrueCSV --  Load
    FakeCSV --  Load
    Load --  Concat
    Concat --  Shuffle
    Shuffle --  Clean
    Clean --  Split
    Split --  FitVect
    Split --  TransVect
    FitVect --  CompareModels
    TransVect --  CompareModels
    CompareModels --  BestModel
    BestModel --  Save
```

**Training Pipeline: From Raw Data to Serialized Model**

### Data Preparation

The training data consists of 44,898 articles from two CSV files [notebooks/train_article.ipynb:31-39]():

- **True.csv**: Real news articles (labeled as 1)
- **Fake.csv**: Fake news articles (labeled as 0)

The notebook concatenates both datasets, shuffles the data, and resets the index to ensure random distribution.

### Text Preprocessing

The `clean_text()` function [notebooks/train_article.ipynb:52-61]() performs aggressive text normalization:

1. Convert to lowercase and strip whitespace
2. Remove URLs (`https?://\\S+|www\\.\\S+`)
3. Remove bracketed content (`\\[.*?\\]`)
4. Remove words containing digits (`\\w*\\d\\w*`)
5. Remove HTML tags (` .*? +`)
6. Replace newlines with spaces
7. Collapse multiple spaces
8. Remove all non-word characters (`\\W`)

This is identical to the `clean_article()` function used at inference time [api/text_processing.py:12-21]().

### Feature Extraction

The `TfidfVectorizer` [notebooks/train_article.ipynb:88-92]() is initialized with default parameters:

- No max_features limit (uses full vocabulary)
- Default n-gram range (1, 1) - unigrams only
- Default min_df and max_df thresholds

The vectorizer is fitted on the training set and transforms both training and test sets into sparse TF-IDF matrices.

### Model Comparison

The notebook trains three classifier types [notebooks/train_article.ipynb:119-195]():

| Model | Test Accuracy | Notes |
|-------|---------------|-------|
| `DecisionTreeClassifier` | 99.62% | High variance, prone to overfitting |
| `RandomForestClassifier` | 98.90% | Good generalization |
| `GradientBoostingClassifier` | **99.64%** | **Best performance, selected** |

The Gradient Boosting Classifier achieves the highest test accuracy with perfect precision and recall scores across both classes.

### Model Serialization

The best-performing model and its vectorizer are saved using pickle [notebooks/train_article.ipynb:205-211]():

```python
with open(\"gradient_boosting_classifier.pkl\", \"wb\") as f:
    pickle.dump(model, f)
with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:
    pickle.dump(vect, f)
```

**Sources:** [notebooks/train_article.ipynb:31-39](), [notebooks/train_article.ipynb:52-64](), [notebooks/train_article.ipynb:74-78](), [notebooks/train_article.ipynb:88-92](), [notebooks/train_article.ipynb:119-195](), [notebooks/train_article.ipynb:205-211]()

---

## Text Preprocessing for Articles

The `clean_article()` function [api/text_processing.py:12-21]() applies the same aggressive preprocessing used during training:

```python
def clean_article(text: str) -  str:
    text = text.lower().strip()
    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)      # Remove URLs
    text = re.sub(r\"\\[.*?\\]\", \"\", text)                     # Remove brackets
    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)                    # Remove words with digits
    text = re.sub(r\" .*? +\", \"\", text)                      # Remove HTML tags
    text = re.sub(r\"\
\", \" \", text)                         # Replace newlines
    text = re.sub(r\"\\s+\", \" \", text)                        # Collapse whitespace
    text = re.sub(r\"\\W\", \" \", text)                         # Remove non-word chars
    return text
```

This preprocessing is more aggressive than the `clean_text()` function used for headlines because articles often contain formatting artifacts, URLs, and other noise that must be removed for optimal classification.

**Sources:** [api/text_processing.py:12-21]()

---

## Runtime Inference Pipeline

When the API receives an article classification request, the following sequence occurs:

```mermaid
sequenceDiagram
    participant Client
    participant Endpoint as \"/predict/article\"
    participant Validator as \"ArticleRequest\"
    participant Cleaner as \"clean_article()\"
    participant Vectorizer as \"TfidfVectorizer\"
    participant Model as \"GradientBoostingClassifier\"
    participant Response as \"ArticlePredictionResponse\"
    
    Client-  Endpoint: POST {\"article\": \"...\"}
    Endpoint-  Validator: Validate input
    Validator--  Endpoint: ArticleRequest object
    
    Endpoint-  Endpoint: Check if article_model is None
    Endpoint-  Endpoint: Check if article.strip() is empty
    
    Endpoint-  Cleaner: clean_article(request.article)
    Cleaner--  Endpoint: cleaned_text
    
    Endpoint-  Vectorizer: vectorizer.transform([cleaned_text])
    Vectorizer--  Endpoint: text_vectorized (sparse matrix)
    
    Endpoint-  Model: article_model.predict_proba(text_vectorized)
    Model--  Endpoint: probabilities [prob_fake, prob_real]
    
    Endpoint-  Model: article_model.predict(text_vectorized)
    Model--  Endpoint: pred_class (0 or 1)
    
    Endpoint-  Endpoint: Map pred_class to \"Fake\"/\"Real\"
    Endpoint-  Response: Build ArticlePredictionResponse
    Response--  Client: JSON response
```

**Inference Sequence: Article Classification Request Flow**

### Endpoint Implementation

The `/predict/article` endpoint [api/main.py:208-239]() handles single article predictions:

```python
@app.post(\"/predict/article\", response_model=ArticlePredictionResponse)
async def predict_article(request: ArticleRequest):
    if article_model is None or vectorizer is None:
        raise HTTPException(status_code=503, detail=\"Article model not loaded\")
    
    if not request.article.strip():
        raise HTTPException(status_code=400, detail=\"Article cannot be empty\")
    
    cleaned_text = clean_article(request.article)
    text_vectorized = vectorizer.transform([cleaned_text])
    
    probabilities = article_model.predict_proba(text_vectorized)[0]
    pred_class = article_model.predict(text_vectorized)[0]
    
    prob_fake = probabilities[0]
    prob_real = probabilities[1]
    confidence = probabilities[pred_class]
    
    prediction = \"Real\" if pred_class == 1 else \"Fake\"
    
    return ArticlePredictionResponse(...)
```

### Batch Prediction Support

The `/predict/article/batch` endpoint [api/main.py:242-283]() supports batch processing of up to 50 articles. It iterates over each article, applying the same prediction logic with graceful error handling for empty articles.

### Request and Response Models

**ArticleRequest** [api/types.py:15-23]():
```python
class ArticleRequest(BaseModel):
    article: str
```

**ArticlePredictionResponse** [api/types.py:43-57]():
```python
class ArticlePredictionResponse(BaseModel):
    article: str           # Truncated to 500 chars in response
    prediction: str        # \"Real\" or \"Fake\"
    confidence: float      # 0.0 to 1.0
    probabilities: dict    # {\"fake\": float, \"real\": float}
```

**Sources:** [api/main.py:208-239](), [api/main.py:242-283](), [api/types.py:15-23](), [api/types.py:43-57]()

---

## Model Performance Characteristics

### Accuracy Metrics

Based on the training notebook evaluation [notebooks/train_article.ipynb:169-195](), the Gradient Boosting Classifier achieves:

| Metric | Fake Class (0) | Real Class (1) | Overall |
|--------|----------------|----------------|---------|
| Precision | 1.00 | 1.00 | 1.00 |
| Recall | 1.00 | 1.00 | 1.00 |
| F1-Score | 1.00 | 1.00 | 1.00 |
| **Test Accuracy** | - | - | **99.64%** |

Test set size: 11,225 articles (25% of 44,898 total)

### Inference Speed

The article model runs entirely on CPU [api/main.py:37-38]() and benefits from:

- **Fast vectorization**: TF-IDF transform is a sparse matrix multiplication
- **Efficient prediction**: Gradient Boosting uses sequential tree evaluation
- **No GPU required**: sklearn models are CPU-optimized

Typical inference time per article: ~10-50ms depending on article length and CPU performance.

### Memory Footprint

The model has a moderate memory footprint:

- `TfidfVectorizer` vocabulary size depends on training data (typically 50k-200k terms)
- `GradientBoostingClassifier` stores decision trees (default: 100 estimators)
- Total memory usage: ~50-200 MB when loaded

**Sources:** [notebooks/train_article.ipynb:169-195](), [api/main.py:37-38]()

---

## Model Lifecycle and State Management

```mermaid
graph TB
    Startup[\"Application Startup\"]
    Lifespan[\"lifespan Context Manager\"]
    LoadFunc[\"load_article_model()\"]
    CheckFiles[\"Check File Existence\"]
    LoadPickle[\"Load Pickle Files\"]
    GlobalVars[\"Set Global Variables: br/ article_model br/ vectorizer\"]
    Ready[\"Model Ready for Inference\"]
    
    Request[\"Inference Request\"]
    CheckLoaded[\"Check if article_model is None\"]
    Preprocess[\"Text Preprocessing\"]
    Vectorize[\"TF-IDF Vectorization\"]
    Predict[\"Model Prediction\"]
    Response[\"Return Response\"]
    
    Startup --  Lifespan
    Lifespan --  LoadFunc
    LoadFunc --  CheckFiles
    CheckFiles -- |Files Missing| RuntimeError
    CheckFiles -- |Files Found| LoadPickle
    LoadPickle --  GlobalVars
    GlobalVars --  Ready
    
    Ready --  Request
    Request --  CheckLoaded
    CheckLoaded -- |None| HTTPException503
    CheckLoaded -- |Loaded| Preprocess
    Preprocess --  Vectorize
    Vectorize --  Predict
    Predict --  Response
```

**Model Lifecycle: From Application Startup to Inference**

### Startup Sequence

The article model is loaded during application startup through the `lifespan` context manager [api/main.py:80-84]():

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    load_headline_model()
    load_article_model()
    yield
```

### Global State

The model and vectorizer are stored as global variables [api/main.py:37-38]():

```python
article_model: GradientBoostingClassifier | None = None
vectorizer: TfidfVectorizer | None = None
```

This approach:
- Loads models once at startup (not per-request)
- Avoids repeated deserialization overhead
- Enables immediate inference availability

### Error Handling

The system implements multiple layers of error checking:

1. **Startup validation**: Raises `RuntimeError` if pickle files are missing [api/main.py:44-48]()
2. **Runtime validation**: Raises `HTTPException(503)` if model is not loaded [api/main.py:210-211]()
3. **Input validation**: Raises `HTTPException(400)` for empty articles [api/main.py:213-214]()

**Sources:** [api/main.py:37-38](), [api/main.py:41-57](), [api/main.py:80-84](), [api/main.py:208-214]()

---

## Integration with API Endpoints

The article model integrates with the FastAPI application through two primary endpoints:

| Endpoint | Method | Request Type | Response Type | Max Batch Size |
|----------|--------|--------------|---------------|----------------|
| `/predict/article` | POST | `ArticleRequest` | `ArticlePredictionResponse` | 1 |
| `/predict/article/batch` | POST | `list[str]` | `{\"results\": [...]}` | 50 |

### Single Article Prediction

The `/predict/article` endpoint [api/main.py:208-239]() is documented in detail in [Article Prediction Endpoints](#3.1.2).

### Batch Article Prediction

The `/predict/article/batch` endpoint [api/main.py:242-283]() processes multiple articles in a single request:

- Accepts a list of article strings (max 50)
- Returns a list of prediction results
- Handles individual article errors gracefully (continues processing remaining articles)
- Truncates article text in responses to first 500 characters

**Sources:** [api/main.py:208-239](), [api/main.py:242-283]()

---

## Comparison with Headline Model

The article model differs from the headline model (see [Headline Model (RoBERTa)](#5.1)) in several key aspects:

| Aspect | Article Model | Headline Model |
|--------|---------------|----------------|
| **Algorithm** | Gradient Boosting (traditional ML) | RoBERTa (transformer) |
| **Feature Extraction** | TF-IDF vectorization | Learned embeddings |
| **Training Data** | True.csv + Fake.csv (44,898 articles) | PolitiFact + HuggingFace datasets |
| **Preprocessing** | Aggressive cleaning (remove URLs, HTML, etc.) | Light cleaning (preserve punctuation) |
| **Model Size** | ~50-200 MB | ~500+ MB |
| **Hardware** | CPU only | GPU-accelerated (CUDA optional) |
| **Inference Speed** | 10-50ms | 50-200ms |
| **Text Length** | Optimized for long articles | Optimized for short statements |

The dual-model architecture allows the system to optimize for different text characteristics: traditional ML for feature-rich articles and deep learning for nuanced short-form text.

**Sources:** [api/main.py:19-78](), [api/main.py:208-239]()"])</script><script>self.__next_f.push([1,"26:T6045,"])</script><script>self.__next_f.push([1,"# Training Pipeline

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [data/gradient_boosting_classifier.pkl](data/gradient_boosting_classifier.pkl)
- [data/tfidf_vectorizer.pkl](data/tfidf_vectorizer.pkl)
- [notebooks/train_article.ipynb](notebooks/train_article.ipynb)
- [notebooks/train_headline.ipynb](notebooks/train_headline.ipynb)

 /details 



This document describes the offline training process for UnFake's dual-model architecture. The training pipeline produces two distinct ML models optimized for different text types: a fine-tuned RoBERTa transformer for short statements/headlines and a Gradient Boosting Classifier with TF-IDF vectorization for longer articles.

For information about the trained models themselves and their architectures, see [Machine Learning Models](#5). For details about how these models are loaded and used at runtime, see [Model Loading and Lifecycle](#3.3).

---

## Overview

The UnFake training pipeline implements two independent training workflows, each executed in a Jupyter notebook:

| Notebook | Model Type | Input Data | Output Artifacts | Target Use Case |
|----------|-----------|-----------|------------------|----------------|
| `train_headline.ipynb` | Fine-tuned RoBERTa | PolitiFact statements | RoBERTa model + AutoTokenizer | Short statements (headlines) |
| `train_article.ipynb` | Gradient Boosting Classifier | True.csv + Fake.csv | GradientBoostingClassifier + TfidfVectorizer | Full articles |

Both pipelines follow similar high-level stages:
1. **Data Loading**: Import datasets from CSV files or external sources
2. **Preprocessing**: Clean and transform text data
3. **Feature Engineering**: Tokenization (RoBERTa) or TF-IDF vectorization (articles)
4. **Model Training**: Fine-tuning (RoBERTa) or traditional ML training (Gradient Boosting)
5. **Evaluation**: Validation and test set metrics
6. **Serialization**: Save model artifacts for deployment

**Sources:** [notebooks/train_article.ipynb](), [notebooks/train_headline.ipynb]()

---

## Training Architecture Overview

```mermaid
graph TB
    subgraph \"Data Sources\"
        TrueCSV[\"True.csv br/ Real News Articles\"]
        FakeCSV[\"Fake.csv br/ Fake News Articles\"]
        PolitiFact[\"politifact_statements.csv br/ Fact-Checked Statements\"]
    end
    
    subgraph \"Training Notebooks\"
        TrainArticle[\"train_article.ipynb\"]
        TrainHeadline[\"train_headline.ipynb\"]
    end
    
    subgraph \"Processing Steps\"
        ArticleClean[\"clean_text() br/ Aggressive cleaning\"]
        HeadlineClean[\"clean_text() br/ Basic cleaning\"]
        TfidfVec[\"TfidfVectorizer br/ fit_transform()\"]
        Tokenizer[\"AutoTokenizer br/ RoBERTa-base\"]
    end
    
    subgraph \"Model Training\"
        CompareModels[\"Compare Models: br/ DecisionTree br/ RandomForest br/ GradientBoosting\"]
        FineTune[\"Fine-tune RoBERTa br/ AdamW + Warmup br/ 4 epochs\"]
    end
    
    subgraph \"Output Artifacts\"
        GBCFile[\"gradient_boosting_classifier.pkl\"]
        VectFile[\"tfidf_vectorizer.pkl\"]
        RoBERTaDir[\"HEADLINE_MODEL_DIR/ br/ model + tokenizer\"]
    end
    
    TrueCSV --  TrainArticle
    FakeCSV --  TrainArticle
    PolitiFact --  TrainHeadline
    
    TrainArticle --  ArticleClean
    TrainHeadline --  HeadlineClean
    
    ArticleClean --  TfidfVec
    HeadlineClean --  Tokenizer
    
    TfidfVec --  CompareModels
    Tokenizer --  FineTune
    
    CompareModels --  GBCFile
    CompareModels --  VectFile
    FineTune --  RoBERTaDir
```

**Sources:** [notebooks/train_article.ipynb](), [notebooks/train_headline.ipynb]()

---

## Data Sources and Preparation

### Article Training Data

The article model is trained on two CSV files located in the `data/` directory:

- **True.csv**: Contains genuine news articles
- **Fake.csv**: Contains fake news articles

These datasets are loaded and combined in [notebooks/train_article.ipynb:31-39]():

```python
true = pd.read_csv(\"../data/True.csv\")
fake = pd.read_csv(\"../data/Fake.csv\")
true[\"label\"] = 1
fake[\"label\"] = 0
df = pd.concat([true, fake], ignore_index=True)
df = df.sample(frac=1)  # Shuffle
```

The combined dataset contains **44,898 articles** with binary labels (1=real, 0=fake).

### Headline Training Data

The headline model uses `politifact_statements.csv`, which contains fact-checked political statements from PolitiFact. The dataset includes columns: `verdict`, `statement`, `statement_date`, `statement_source`, `factcheck_date`, and `url`.

Verdicts are mapped to binary labels in [notebooks/train_headline.ipynb:310-326]():

```python
LABEL_MAP = {
    \"True\": 1,
    \"Mostly True\": 1,
    \"Half True\": 1,
    \"Mostly False\": 0,
    \"False\": 0,
    \"Pants on Fire\": 0,
}
df[\"label\"] = df[\"verdict\"].map(LABEL_MAP)
```

Additional preprocessing includes:
- **Spanish text removal**: Filters out statements containing Spanish characters [notebooks/train_headline.ipynb:339-345]()
- **Class imbalance handling**: Computes class weights using `sklearn.utils.class_weight.compute_class_weight` [notebooks/train_headline.ipynb:374-377]()

**Sources:** [notebooks/train_article.ipynb:31-39](), [notebooks/train_headline.ipynb:283-326](), [notebooks/train_headline.ipynb:339-377]()

---

## Text Preprocessing

### Article Text Cleaning

The article pipeline applies aggressive text cleaning in [notebooks/train_article.ipynb:52-64]():

```python
def clean_text(text):
    text = text.lower().strip()
    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)  # Remove URLs
    text = re.sub(r\"\\[.*?\\]\", \"\", text)  # Remove brackets
    text = re.sub(r\"\\w*\\d\\w*\", \"\", text)  # Remove words with digits
    text = re.sub(r\" .*? +\", \"\", text)  # Remove HTML tags
    text = re.sub(r\"\
\", \" \", text)  # Replace newlines
    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace
    text = re.sub(r\"\\W\", \" \", text)  # Remove non-alphanumeric
    return text
```

This cleaning strategy removes structural elements (URLs, HTML, brackets) and normalizes text to lowercase alphanumeric tokens.

### Headline Text Cleaning

The headline pipeline uses a lighter cleaning approach in [notebooks/train_headline.ipynb:611-625]():

```python
def clean_text(text: str, remove_stopwords=False, apply_stemming=False):
    text = text.lower()
    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)
    text = re.sub(r\"\\s+\", \" \", text)
    text = text.strip()
    
    tokens = text.split()
    if remove_stopwords:
        tokens = [word for word in tokens if word not in stop_words]
    if apply_stemming:
        tokens = [stemmer.stem(word) for word in tokens]
    
    return \" \".join(tokens)
```

Key differences from article cleaning:
- Preserves punctuation context (important for statement analysis)
- Optional stopword removal and stemming (not used by default)
- Simpler regex patterns focused on normalization rather than aggressive filtering

**Sources:** [notebooks/train_article.ipynb:52-64](), [notebooks/train_headline.ipynb:611-634]()

---

## Article Model Training

### Feature Extraction

The article model uses TF-IDF (Term Frequency-Inverse Document Frequency) vectorization implemented in [notebooks/train_article.ipynb:88-92]():

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vect = TfidfVectorizer()
X_train = vect.fit_transform(X_train)
X_test = vect.transform(X_test)
```

The vectorizer learns vocabulary and IDF weights from the training set, then transforms both train and test sets into sparse feature matrices.

### Data Split

A standard 75/25 train-test split is performed in [notebooks/train_article.ipynb:76-78]():

```python
X_train, X_test, y_train, y_test = train_test_split(
    df[\"text\"], df[\"label\"], test_size=0.25
)
```

### Model Comparison

Three traditional ML classifiers are trained and evaluated:

| Model | Implementation | Accuracy | F1-Score |
|-------|---------------|----------|----------|
| Decision Tree | `DecisionTreeClassifier()` | 99.6% | 1.00 |
| Random Forest | `RandomForestClassifier()` | 98.9% | 0.99 |
| **Gradient Boosting** | `GradientBoostingClassifier()` | **99.6%** | **1.00** |

The Gradient Boosting Classifier is selected as the final model based on its superior performance. Training code is in [notebooks/train_article.ipynb:188-195]():

```python
from sklearn.ensemble import GradientBoostingClassifier

model = GradientBoostingClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### Model Serialization

The trained model and vectorizer are serialized using Python's `pickle` module in [notebooks/train_article.ipynb:206-211]():

```python
import pickle

with open(\"gradient_boosting_classifier.pkl\", \"wb\") as f:
    pickle.dump(model, f)
with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:
    pickle.dump(vect, f)
```

These files are saved in the notebook directory and should be moved to `data/` for deployment.

**Sources:** [notebooks/train_article.ipynb:76-92](), [notebooks/train_article.ipynb:119-195](), [notebooks/train_article.ipynb:206-211]()

---

## Headline Model Training

### Training Configuration

The headline model uses the following hyperparameters defined in [notebooks/train_headline.ipynb:869-875]():

```python
MODEL_NAME = \"RoBERTa-base\"
MAX_LENGTH = 256
BATCH_SIZE = 16
LEARNING_RATE = 2e-5
NUM_EPOCHS = 4
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01
```

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `MODEL_NAME` | `\"RoBERTa-base\"` | Pre-trained model from HuggingFace |
| `MAX_LENGTH` | 256 | Maximum token sequence length |
| `BATCH_SIZE` | 16 | Training batch size |
| `LEARNING_RATE` | 2e-5 | AdamW learning rate |
| `NUM_EPOCHS` | 4 | Maximum training epochs |
| `WARMUP_RATIO` | 0.1 | Linear warmup proportion |
| `WEIGHT_DECAY` | 0.01 | L2 regularization |

### Data Split

The dataset is split into train (80%), validation (10%), and test (10%) sets with stratification in [notebooks/train_headline.ipynb:685-694]():

```python
train_df, temp_df = train_test_split(
    df, test_size=0.2, random_state=SEED, stratify=df[\"label\"]
)
val_df, test_df = train_test_split(
    temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"label\"]
)
```

This produces approximately:
- **20,799** training examples
- **2,600** validation examples
- **2,600** test examples

### Dataset and DataLoader

A custom PyTorch `Dataset` class handles tokenization in [notebooks/train_headline.ipynb:914-940]():

```python
class FakeNewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding=\"max_length\",
            truncation=True,
            return_tensors=\"pt\",
        )
        
        return {
            \"input_ids\": encoding[\"input_ids\"].flatten(),
            \"attention_mask\": encoding[\"attention_mask\"].flatten(),
            \"label\": torch.tensor(label, dtype=torch.long),
        }
```

PyTorch `DataLoader` instances are created in [notebooks/train_headline.ipynb:954-956]():

```python
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE * 2, shuffle=False)
```

### Model and Optimizer Setup

The model is initialized from HuggingFace's pre-trained RoBERTa in [notebooks/train_headline.ipynb:1032-1042]():

```python
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)
model.to(DEVICE)

optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

total_steps = len(train_loader) * NUM_EPOCHS
warmup_steps = int(total_steps * WARMUP_RATIO)

scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps
)
```

Key components:
- **AutoModelForSequenceClassification**: Loads RoBERTa with a classification head
- **AdamW optimizer**: Weight decay variant of Adam
- **Linear warmup scheduler**: Gradually increases learning rate for the first 10% of steps

### Training Loop

The training loop implements standard fine-tuning with gradient clipping in [notebooks/train_headline.ipynb:1069-1104]():

```python
def train_epoch(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    for batch in dataloader:
        input_ids = batch[\"input_ids\"].to(device)
        attention_mask = batch[\"attention_mask\"].to(device)
        labels = batch[\"label\"].to(device)
        
        optimizer.zero_grad()
        
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        preds = torch.argmax(outputs.logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    return avg_loss, accuracy, f1
```

Key features:
- **Gradient clipping**: Prevents exploding gradients (max norm = 1.0)
- **Scheduler update**: Called after each optimizer step
- **Metric tracking**: Computes loss, accuracy, and F1 score

### Evaluation

The evaluation function runs inference in `torch.no_grad()` context [notebooks/train_headline.ipynb:1107-1147]():

```python
def evaluate(model, dataloader, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch[\"input_ids\"].to(device)
            attention_mask = batch[\"attention_mask\"].to(device)
            labels = batch[\"label\"].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits
            
            preds = torch.argmax(logits, dim=1)
            probs = torch.softmax(logits, dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())
    
    return {
        \"loss\": avg_loss,
        \"accuracy\": accuracy,
        \"f1\": f1,
        \"precision\": precision,
        \"recall\": recall,
        \"predictions\": all_preds,
        \"labels\": all_labels,
        \"probabilities\": all_probs,
    }
```

### Early Stopping

The main training loop implements early stopping with patience=2 in [notebooks/train_headline.ipynb:1440-1501]():

```python
best_val_f1 = 0
patience = 2
patience_counter = 0

for epoch in range(NUM_EPOCHS):
    train_loss, train_acc, train_f1 = train_epoch(
        model, train_loader, optimizer, scheduler, DEVICE
    )
    
    val_results = evaluate(model, val_loader, DEVICE)
    
    if val_results[\"f1\"]   best_val_f1:
        best_val_f1 = val_results[\"f1\"]
        patience_counter = 0
        torch.save(
            {
                \"epoch\": epoch,
                \"model_state_dict\": model.state_dict(),
                \"optimizer_state_dict\": optimizer.state_dict(),
                \"val_f1\": best_val_f1,
            },
            os.path.join(MODEL_DIR, \"best_model.pt\"),
        )
    else:
        patience_counter += 1
        if patience_counter  = patience:
            print(f\"\
Early stopping triggered at epoch {epoch + 1}\")
            break
```

The model with the highest validation F1 score is saved to `MODEL_DIR/best_model.pt`.

**Sources:** [notebooks/train_headline.ipynb:869-875](), [notebooks/train_headline.ipynb:685-694](), [notebooks/train_headline.ipynb:914-960](), [notebooks/train_headline.ipynb:1032-1042](), [notebooks/train_headline.ipynb:1069-1147](), [notebooks/train_headline.ipynb:1440-1501]()

---

## Training Pipeline Flow Diagram

```mermaid
flowchart TD
    Start[\"Start Training\"]
    
    subgraph Article[\"Article Model Pipeline\"]
        LoadArticleData[\"Load True.csv + Fake.csv br/ Concatenate and shuffle\"]
        LabelArticle[\"Assign labels: br/ True=1, Fake=0\"]
        CleanArticle[\"clean_text(): br/ Remove URLs, HTML, brackets br/ Lowercase, normalize\"]
        SplitArticle[\"train_test_split br/ 75% train / 25% test\"]
        TfIdfFit[\"TfidfVectorizer br/ fit_transform(X_train)\"]
        TrainModels[\"Train 3 classifiers: br/ DecisionTree br/ RandomForest br/ GradientBoosting\"]
        CompareArticle[\"Compare performance br/ Select best model\"]
        SaveArticle[\"pickle.dump(): br/ gradient_boosting_classifier.pkl br/ tfidf_vectorizer.pkl\"]
    end
    
    subgraph Headline[\"Headline Model Pipeline\"]
        LoadHeadlineData[\"Load politifact_statements.csv\"]
        MapLabels[\"Map verdicts to binary: br/ LABEL_MAP\"]
        FilterSpanish[\"Remove Spanish text\"]
        ComputeWeights[\"Compute class weights br/ Handle imbalance\"]
        CleanHeadline[\"clean_text(): br/ Lowercase, basic cleaning\"]
        SplitHeadline[\"train_test_split: br/ 80% train / 10% val / 10% test\"]
        CreateDataset[\"FakeNewsDataset br/ AutoTokenizer tokenization br/ max_length=256\"]
        LoadModel[\"AutoModelForSequenceClassification br/ from_pretrained('RoBERTa-base')\"]
        SetupOptim[\"AdamW optimizer br/ Linear warmup scheduler\"]
        TrainLoop[\"Fine-tuning loop: br/ 4 epochs max br/ gradient clipping\"]
        EarlyStop[\"Early stopping br/ patience=2 br/ Based on val F1\"]
        SaveHeadline[\"torch.save(): br/ best_model.pt br/ to MODEL_DIR\"]
    end
    
    Start --  LoadArticleData
    Start --  LoadHeadlineData
    
    LoadArticleData --  LabelArticle
    LabelArticle --  CleanArticle
    CleanArticle --  SplitArticle
    SplitArticle --  TfIdfFit
    TfIdfFit --  TrainModels
    TrainModels --  CompareArticle
    CompareArticle --  SaveArticle
    
    LoadHeadlineData --  MapLabels
    MapLabels --  FilterSpanish
    FilterSpanish --  ComputeWeights
    ComputeWeights --  CleanHeadline
    CleanHeadline --  SplitHeadline
    SplitHeadline --  CreateDataset
    CreateDataset --  LoadModel
    LoadModel --  SetupOptim
    SetupOptim --  TrainLoop
    TrainLoop --  EarlyStop
    EarlyStop --  SaveHeadline
    
    SaveArticle --  End[\"Deployment\"]
    SaveHeadline --  End
```

**Sources:** [notebooks/train_article.ipynb](), [notebooks/train_headline.ipynb]()

---

## Model Artifacts

### Article Model Artifacts

The article training pipeline produces two pickle files:

1. **gradient_boosting_classifier.pkl**: Serialized `GradientBoostingClassifier` instance
   - Location: Should be placed in `data/gradient_boosting_classifier.pkl`
   - Size: Contains trained model with all learned parameters
   - Usage: Loaded via `pickle.load()` at application startup

2. **tfidf_vectorizer.pkl**: Fitted `TfidfVectorizer` instance
   - Location: Should be placed in `data/tfidf_vectorizer.pkl`
   - Size: Contains vocabulary and IDF weights learned from training data
   - Usage: Must be used to transform new text before prediction

### Headline Model Artifacts

The headline training pipeline produces a PyTorch checkpoint:

1. **best_model.pt**: Complete model state dictionary
   - Location: `MODEL_DIR/best_model.pt` (default: `./models/best_model.pt`)
   - Contents:
     - `epoch`: Training epoch when best F1 was achieved
     - `model_state_dict`: RoBERTa model weights
     - `optimizer_state_dict`: Optimizer state (for resuming training)
     - `val_f1`: Best validation F1 score
   - Usage: Loaded into `AutoModelForSequenceClassification` instance

Additionally, the pre-trained `AutoTokenizer` must be saved separately (typically done by calling `tokenizer.save_pretrained()` or by reloading from HuggingFace at deployment).

### Deployment Configuration

At deployment, the FastAPI application expects:

| Artifact | Environment Variable | Default Path |
|----------|---------------------|--------------|
| RoBERTa model + tokenizer | `HEADLINE_MODEL_DIR` | `~/model/` |
| Gradient Boosting model | `ARTICLE_MODEL_DIR` | `data/gradient_boosting_classifier.pkl` |
| TF-IDF vectorizer | `VECTORIZER_DIR` | `data/tfidf_vectorizer.pkl` |

See [Model Loading and Lifecycle](#3.3) for details on how these artifacts are loaded at application startup.

**Sources:** [notebooks/train_article.ipynb:206-211](), [notebooks/train_headline.ipynb:1482-1490](), [notebooks/train_headline.ipynb:146-147]()

---

## Training Hyperparameters Summary

### Article Model

| Parameter | Value | Notes |
|-----------|-------|-------|
| Vectorization | TF-IDF | Default sklearn settings |
| Model | Gradient Boosting | Default sklearn hyperparameters |
| Train/Test Split | 75% / 25% | Random split |
| Evaluation Metric | Accuracy, F1 | Classification report |

### Headline Model

| Parameter | Value | Notes |
|-----------|-------|-------|
| Base Model | RoBERTa-base | From HuggingFace |
| Max Sequence Length | 256 tokens | Truncation applied |
| Batch Size | 16 (train), 32 (val/test) | Larger batch for evaluation |
| Learning Rate | 2e-5 | Standard for fine-tuning |
| Weight Decay | 0.01 | L2 regularization |
| Warmup Ratio | 0.1 | 10% of total steps |
| Max Epochs | 4 | With early stopping |
| Early Stopping Patience | 2 epochs | Based on validation F1 |
| Gradient Clipping | Max norm = 1.0 | Prevents instability |
| Optimizer | AdamW | Weight decay variant |
| Train/Val/Test Split | 80% / 10% / 10% | Stratified split |

**Sources:** [notebooks/train_article.ipynb:76-92](), [notebooks/train_headline.ipynb:869-875](), [notebooks/train_headline.ipynb:1036-1042]()

---

## Running the Training Pipeline

### Prerequisites

Both notebooks require Python 3.12+ with the following dependencies:
- **Article model**: `pandas`, `numpy`, `scikit-learn`, `pickle`
- **Headline model**: `pandas`, `numpy`, `scikit-learn`, `torch`, `transformers`, `nltk`, `tqdm`

### Execution Steps

**Article Model:**
1. Ensure `data/True.csv` and `data/Fake.csv` exist
2. Open `notebooks/train_article.ipynb` in Jupyter
3. Execute all cells sequentially
4. Move generated `.pkl` files to `data/` directory

**Headline Model:**
1. Ensure `data/politifact_statements.csv` exists
2. Open `notebooks/train_headline.ipynb` in Jupyter
3. Execute all cells sequentially
4. Note the `MODEL_DIR` location (default: `./models`)
5. For deployment, copy model files to `HEADLINE_MODEL_DIR` (default: `~/model/`)

### GPU Acceleration

The headline training notebook automatically detects and uses CUDA if available [notebooks/train_headline.ipynb:136-142]():

```python
DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")
print(f\"Device: {DEVICE}\")
```

Training on GPU significantly reduces training time (from hours to minutes).

**Sources:** [notebooks/train_article.ipynb](), [notebooks/train_headline.ipynb](), [notebooks/train_headline.ipynb:136-142]()

---

## Performance Metrics

### Article Model Results

The Gradient Boosting Classifier achieved strong performance on the test set:

| Metric | Fake (0) | Real (1) | Overall |
|--------|----------|----------|---------|
| Precision | 1.00 | 1.00 | 1.00 |
| Recall | 1.00 | 1.00 | 1.00 |
| F1-Score | 1.00 | 1.00 | 1.00 |
| **Accuracy** | - | - | **99.6%** |

Test set size: 11,225 articles (5,788 fake, 5,437 real)

### Headline Model Results

Training history from the notebook shows progressive improvement:

| Epoch | Train Loss | Train Acc | Train F1 | Val Loss | Val Acc | Val F1 |
|-------|-----------|-----------|----------|----------|---------|--------|
| 1 | 0.5898 | 0.6770 | 0.6810 | 0.5534 | 0.7004 | **0.7044** |
| 2 | 0.5086 | 0.7376 | 0.7410 | 0.5388 | 0.6935 | 0.6961 |
| 3 | 0.4285 | 0.7947 | 0.7974 | 0.5794 | 0.7162 | **0.7197** |
| 4 | 0.3483 | 0.8407 | 0.8426 | 0.6679 | 0.7181 | **0.7218** |

Best validation F1: **0.7218** (achieved at epoch 4)

**Sources:** [notebooks/train_article.ipynb:169-183](), [notebooks/train_headline.ipynb:1264-1436]()

---

## Next Steps

For information about:
- **Model architecture details**: See [Machine Learning Models](#5)
- **Model loading at runtime**: See [Model Loading and Lifecycle](#3.3)
- **Inference implementation**: See [Backend API](#3)
- **Individual training processes**: See [Training the Headline Model](#6.1) and [Training the Article Model](#6.2)"])</script><script>self.__next_f.push([1,"27:T4078,"])</script><script>self.__next_f.push([1,"# Training the Headline Model

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [data/politifact_statements.csv](data/politifact_statements.csv)
- [notebooks/train_headline.ipynb](notebooks/train_headline.ipynb)
- [pyproject.toml](pyproject.toml)

 /details 



## Purpose and Scope

This page documents the training pipeline for the headline classification model used by the UnFake system. The headline model is a fine-tuned RoBERTa transformer that classifies short statements and news headlines as fake or real. This covers data preparation, model configuration, training procedures, and model export as implemented in [notebooks/train_headline.ipynb](). For information about how this trained model is deployed and used for inference, see [Model Loading and Lifecycle](#3.3). For details about the article classification model, see [Training the Article Model](#6.2).

## Training Pipeline Overview

```mermaid
flowchart TB
    subgraph \"Data Preparation\"
        CSV[\"politifact_statements.csv\"]
        LoadData[\"Load Dataset br/ (pd.read_csv)\"]
        LabelMap[\"Label Mapping br/ (LABEL_MAP dict)\"]
        CleanText[\"Text Cleaning br/ (clean_text function)\"]
        RemoveSpanish[\"Language Filter br/ (remove_spanish)\"]
    end
    
    subgraph \"Dataset Configuration\"
        TrainTest[\"Train/Val/Test Split br/ (train_test_split)\"]
        FakeNewsDataset[\"FakeNewsDataset class br/ (PyTorch Dataset)\"]
        AutoTokenizer[\"AutoTokenizer br/ (roberta-base)\"]
        DataLoader[\"DataLoader br/ (batch_size=16)\"]
    end
    
    subgraph \"Model Setup\"
        PretrainedModel[\"AutoModelForSequenceClassification br/ (roberta-base, num_labels=2)\"]
        ClassWeights[\"Class Weights br/ (compute_class_weight)\"]
        AdamW[\"AdamW Optimizer br/ (lr=2e-5)\"]
        Scheduler[\"Linear Warmup Scheduler br/ (warmup_ratio=0.1)\"]
    end
    
    subgraph \"Training Loop\"
        TrainEpoch[\"train_epoch function\"]
        Evaluate[\"evaluate function\"]
        EarlyStopping[\"Early Stopping br/ (patience=2)\"]
        SaveBest[\"Save Best Model br/ (best_model.pt)\"]
    end
    
    subgraph \"Model Export\"
        ModelDir[\"./models directory\"]
        CheckpointFile[\"best_model.pt\"]
    end
    
    CSV --  LoadData
    LoadData --  LabelMap
    LabelMap --  RemoveSpanish
    RemoveSpanish --  CleanText
    CleanText --  TrainTest
    
    TrainTest --  FakeNewsDataset
    AutoTokenizer --  FakeNewsDataset
    FakeNewsDataset --  DataLoader
    
    PretrainedModel --  TrainEpoch
    ClassWeights --  TrainEpoch
    AdamW --  TrainEpoch
    Scheduler --  TrainEpoch
    DataLoader --  TrainEpoch
    
    TrainEpoch --  Evaluate
    Evaluate --  EarlyStopping
    EarlyStopping --  SaveBest
    
    SaveBest --  ModelDir
    ModelDir --  CheckpointFile
```

**Sources:** [notebooks/train_headline.ipynb:1-1502]()

## Data Preparation

### Dataset Loading and Label Mapping

The training process begins by loading the PolitiFact statements dataset from [data/politifact_statements.csv](). This dataset contains fact-checked political statements with multi-class verdicts that are mapped to binary labels.

```mermaid
flowchart LR
    subgraph \"Verdict Categories\"
        True[\"True\"]
        MostlyTrue[\"Mostly True\"]
        HalfTrue[\"Half True\"]
        MostlyFalse[\"Mostly False\"]
        False[\"False\"]
        PantsOnFire[\"Pants on Fire\"]
    end
    
    subgraph \"Binary Mapping (LABEL_MAP)\"
        Label1[\"1 (Real)\"]
        Label0[\"0 (Fake)\"]
    end
    
    True --  Label1
    MostlyTrue --  Label1
    HalfTrue --  Label1
    
    MostlyFalse --  Label0
    False --  Label0
    PantsOnFire --  Label0
```

The label mapping logic at [notebooks/train_headline.ipynb:310-327]() converts the six-class PolitiFact verdicts into a binary classification task. Statements rated as \"True\", \"Mostly True\", or \"Half True\" are labeled as real (1), while \"Mostly False\", \"False\", and \"Pants on Fire\" are labeled as fake (0).

| Verdict Category | Binary Label | Description |
|-----------------|--------------|-------------|
| True | 1 (Real) | Completely accurate statements |
| Mostly True | 1 (Real) | Largely accurate with minor issues |
| Half True | 1 (Real) | Mixed accuracy |
| Mostly False | 0 (Fake) | Largely inaccurate |
| False | 0 (Fake) | Completely inaccurate |
| Pants on Fire | 0 (Fake) | Ridiculously false claims |

**Sources:** [notebooks/train_headline.ipynb:283-327](), [data/politifact_statements.csv:1-150]()

### Text Preprocessing

```mermaid
flowchart TB
    Input[\"Raw Statement Text\"]
    
    subgraph \"Preprocessing Steps\"
        StripQuotes[\"Strip Quotes br/ (str.strip)\"]
        ReplaceAmp[\"Replace   with 'and' br/ (str.replace)\"]
        RemoveUnicode[\"Remove Unicode br/ (regex)\"]
        CleanFunc[\"clean_text function\"]
        
        subgraph \"clean_text Steps\"
            Lowercase[\"Convert to Lowercase\"]
            RemoveNonAlpha[\"Remove Non-Alphanumeric br/ (regex: [^a-z0-9\\\\s])\"]
            CollapseWhitespace[\"Collapse Whitespace br/ (regex: \\\\s+)\"]
            StripSpace[\"Strip Leading/Trailing\"]
        end
    end
    
    Output[\"cleaned_statement column\"]
    
    Input --  StripQuotes
    StripQuotes --  ReplaceAmp
    ReplaceAmp --  RemoveUnicode
    RemoveUnicode --  CleanFunc
    
    CleanFunc --  Lowercase
    Lowercase --  RemoveNonAlpha
    RemoveNonAlpha --  CollapseWhitespace
    CollapseWhitespace --  StripSpace
    
    StripSpace --  Output
```

The preprocessing pipeline is implemented in two stages:

1. **Initial cleaning** [notebooks/train_headline.ipynb:628-633](): Removes quotes, replaces ampersands, and strips Unicode characters
2. **Deep cleaning via `clean_text`** [notebooks/train_headline.ipynb:611-625](): Lowercases text, removes non-alphanumeric characters (except spaces), and normalizes whitespace

Additionally, a language filter at [notebooks/train_headline.ipynb:339-346]() removes Spanish-language statements by detecting Spanish-specific characters using the `remove_spanish` function.

**Sources:** [notebooks/train_headline.ipynb:339-346, 611-643]()

## Model Architecture and Configuration

### Model Selection

The notebook uses `roberta-base` as the foundation model, accessed through HuggingFace's `AutoModelForSequenceClassification` API. The model is configured at [notebooks/train_headline.ipynb:869-878, 1032-1047]().

```mermaid
flowchart TB
    subgraph \"Base Model\"
        RoBERTa[\"roberta-base br/ (HuggingFace)\"]
    end
    
    subgraph \"Model Configuration\"
        ModelName[\"MODEL_NAME = 'RoBERTa-base'\"]
        NumLabels[\"num_labels=2\"]
        MaxLength[\"MAX_LENGTH = 256\"]
    end
    
    subgraph \"Tokenizer Setup\"
        AutoTok[\"AutoTokenizer.from_pretrained\"]
        TokenConfig[\"padding='max_length' br/ truncation=True\"]
    end
    
    subgraph \"Instantiated Model\"
        SeqClass[\"AutoModelForSequenceClassification\"]
        Classifier[\"Classification Head br/ (Dense + Output Layer)\"]
    end
    
    RoBERTa --  ModelName
    ModelName --  AutoTok
    ModelName --  SeqClass
    NumLabels --  SeqClass
    MaxLength --  TokenConfig
    AutoTok --  TokenConfig
    SeqClass --  Classifier
```

| Configuration Parameter | Value | Purpose |
|------------------------|-------|---------|
| `MODEL_NAME` | `\"RoBERTa-base\"` | Pre-trained transformer model |
| `MAX_LENGTH` | 256 | Maximum sequence length in tokens |
| `BATCH_SIZE` | 16 | Training batch size |
| `LEARNING_RATE` | 2e-5 | AdamW optimizer learning rate |
| `NUM_EPOCHS` | 4 | Maximum training epochs |
| `WARMUP_RATIO` | 0.1 | Linear warmup proportion |
| `WEIGHT_DECAY` | 0.01 | L2 regularization strength |

**Sources:** [notebooks/train_headline.ipynb:869-878, 1032-1047]()

### Class Imbalance Handling

The dataset exhibits class imbalance with approximately 61.5% fake statements and 38.5% real statements. The training pipeline addresses this through class weight computation:

```mermaid
flowchart LR
    LabelDist[\"Label Distribution br/ Fake: 15978 br/ Real: 10021\"]
    ComputeWeights[\"compute_class_weight br/ (class_weight='balanced')\"]
    Weights[\"Class Weights br/ Fake: 0.8136 br/ Real: 1.2972\"]
    Device[\"Move to DEVICE br/ (CUDA/CPU)\"]
    
    LabelDist --  ComputeWeights
    ComputeWeights --  Weights
    Weights --  Device
```

The class weights are computed at [notebooks/train_headline.ipynb:374-384]() using scikit-learn's `compute_class_weight` with the `\"balanced\"` strategy, which inversely weights classes proportional to their frequency.

**Sources:** [notebooks/train_headline.ipynb:374-384]()

## Dataset Splits and DataLoaders

### FakeNewsDataset Class

The custom PyTorch `Dataset` implementation is defined at [notebooks/train_headline.ipynb:914-940]():

```mermaid
classDiagram
    class FakeNewsDataset {
        +texts: list
        +labels: list
        +tokenizer: AutoTokenizer
        +max_length: int
        +__init__(texts, labels, tokenizer, max_length)
        +__len__() int
        +__getitem__(idx) dict
    }
    
    class DataItem {
        +input_ids: Tensor
        +attention_mask: Tensor
        +label: Tensor
    }
    
    FakeNewsDataset --  DataItem : returns
```

The `__getitem__` method tokenizes each statement on-the-fly and returns:
- `input_ids`: Tokenized and padded sequence
- `attention_mask`: Mask indicating real vs. padding tokens
- `label`: Binary class label (0 or 1)

### Data Split Configuration

The dataset is split into training (80%), validation (10%), and test (10%) sets using stratified sampling to maintain class distribution:

```mermaid
flowchart TB
    FullDataset[\"Full Dataset br/ (25,999 statements)\"]
    
    subgraph \"First Split (80/20)\"
        TrainSet[\"Training Set br/ (20,799 samples)\"]
        TempSet[\"Temp Set br/ (5,200 samples)\"]
    end
    
    subgraph \"Second Split (50/50 of Temp)\"
        ValSet[\"Validation Set br/ (2,600 samples)\"]
        TestSet[\"Test Set br/ (2,600 samples)\"]
    end
    
    subgraph \"DataLoaders\"
        TrainLoader[\"train_loader br/ (batch_size=16, shuffle=True)\"]
        ValLoader[\"val_loader br/ (batch_size=32, shuffle=False)\"]
        TestLoader[\"test_loader br/ (batch_size=32, shuffle=False)\"]
    end
    
    FullDataset --  TrainSet
    FullDataset --  TempSet
    TempSet --  ValSet
    TempSet --  TestSet
    
    TrainSet --  TrainLoader
    ValSet --  ValLoader
    TestSet --  TestLoader
```

The splits are created at [notebooks/train_headline.ipynb:685-696]() with `stratify` parameters to ensure balanced label distribution across all splits.

**Sources:** [notebooks/train_headline.ipynb:685-696, 914-960]()

## Training Process

### Optimizer and Scheduler Configuration

```mermaid
flowchart TB
    subgraph \"Optimization Setup\"
        Model[\"RoBERTa Model Parameters\"]
        AdamW[\"AdamW Optimizer br/ lr=2e-5 br/ weight_decay=0.01\"]
        
        TotalSteps[\"total_steps = len(train_loader) * NUM_EPOCHS br/ (5,200 steps)\"]
        WarmupSteps[\"warmup_steps = total_steps * 0.1 br/ (520 steps)\"]
        
        Scheduler[\"get_linear_schedule_with_warmup br/ (Linear warmup + decay)\"]
    end
    
    Model --  AdamW
    TotalSteps --  Scheduler
    WarmupSteps --  Scheduler
```

The optimizer configuration at [notebooks/train_headline.ipynb:1035-1047]() uses AdamW with a learning rate of 2e-5 and weight decay of 0.01. The learning rate schedule includes a linear warmup phase for the first 10% of training steps, followed by linear decay.

**Sources:** [notebooks/train_headline.ipynb:1035-1047]()

### Training Loop Implementation

The `train_epoch` function at [notebooks/train_headline.ipynb:1069-1104]() implements the training loop:

```mermaid
flowchart TB
    Start[\"train_epoch called\"]
    
    subgraph \"Batch Processing\"
        GetBatch[\"Get batch from DataLoader\"]
        MoveDevice[\"Move tensors to DEVICE\"]
        ZeroGrad[\"optimizer.zero_grad()\"]
        Forward[\"model forward pass\"]
        CalcLoss[\"Calculate loss\"]
        Backward[\"loss.backward()\"]
        ClipGrad[\"Clip gradients (max_norm=1.0)\"]
        OptStep[\"optimizer.step()\"]
        SchedStep[\"scheduler.step()\"]
        UpdateMetrics[\"Accumulate loss, predictions\"]
    end
    
    CalcMetrics[\"Calculate epoch metrics br/ (avg_loss, accuracy, f1)\"]
    Return[\"Return metrics\"]
    
    Start --  GetBatch
    GetBatch --  MoveDevice
    MoveDevice --  ZeroGrad
    ZeroGrad --  Forward
    Forward --  CalcLoss
    CalcLoss --  Backward
    Backward --  ClipGrad
    ClipGrad --  OptStep
    OptStep --  SchedStep
    SchedStep --  UpdateMetrics
    UpdateMetrics --  GetBatch
    GetBatch --  CalcMetrics
    CalcMetrics --  Return
```

Key features:
- Gradient clipping with `max_norm=1.0` to prevent exploding gradients
- Learning rate scheduling after each batch
- Accumulation of predictions and labels for epoch-level metrics

**Sources:** [notebooks/train_headline.ipynb:1069-1104]()

### Evaluation and Early Stopping

The `evaluate` function at [notebooks/train_headline.ipynb:1107-1147]() computes validation metrics:

```mermaid
flowchart TB
    Start[\"evaluate called\"]
    
    subgraph \"Evaluation Loop\"
        NoGrad[\"torch.no_grad() context\"]
        GetBatch[\"Get batch from DataLoader\"]
        MoveDevice[\"Move tensors to DEVICE\"]
        Forward[\"model forward pass\"]
        CalcLoss[\"Accumulate loss\"]
        Softmax[\"torch.softmax for probabilities\"]
        Argmax[\"torch.argmax for predictions\"]
        Store[\"Store predictions, labels, probs\"]
    end
    
    subgraph \"Metrics Calculation\"
        AvgLoss[\"Average loss\"]
        Accuracy[\"accuracy_score\"]
        F1[\"f1_score (weighted)\"]
        Precision[\"precision_score (weighted)\"]
        Recall[\"recall_score (weighted)\"]
    end
    
    ReturnDict[\"Return metrics dict\"]
    
    Start --  NoGrad
    NoGrad --  GetBatch
    GetBatch --  MoveDevice
    MoveDevice --  Forward
    Forward --  CalcLoss
    Forward --  Softmax
    Softmax --  Argmax
    Argmax --  Store
    Store --  GetBatch
    
    GetBatch --  AvgLoss
    Store --  Accuracy
    Store --  F1
    Store --  Precision
    Store --  Recall
    
    AvgLoss --  ReturnDict
    Accuracy --  ReturnDict
    F1 --  ReturnDict
    Precision --  ReturnDict
    Recall --  ReturnDict
```

The main training loop at [notebooks/train_headline.ipynb:1440-1501]() implements early stopping with patience of 2 epochs:

| Epoch | Train Loss | Train F1 | Val Loss | Val F1 | Action |
|-------|-----------|----------|----------|--------|--------|
| 1 | 0.5898 | 0.6810 | 0.5534 | 0.7044 | ✓ Saved (best F1) |
| 2 | 0.5086 | 0.7410 | 0.5388 | 0.6961 | No improvement (1/2) |
| 3 | 0.4285 | 0.7974 | 0.5794 | 0.7197 | ✓ Saved (best F1) |
| 4 | 0.3483 | 0.8426 | 0.6679 | 0.7218 | ✓ Saved (best F1) |

The model with the highest validation F1 score is saved to `best_model.pt` at [notebooks/train_headline.ipynb:1482-1491]().

**Sources:** [notebooks/train_headline.ipynb:1069-1147, 1440-1501]()

## Model Export

### Checkpoint Structure

The best model is saved as a PyTorch checkpoint containing:

```mermaid
flowchart LR
    subgraph \"Checkpoint Dictionary\"
        Epoch[\"epoch: int\"]
        ModelState[\"model_state_dict: OrderedDict\"]
        OptimizerState[\"optimizer_state_dict: dict\"]
        ValF1[\"val_f1: float\"]
    end
    
    subgraph \"File System\"
        ModelDir[\"./models/ directory\"]
        CheckpointFile[\"best_model.pt\"]
    end
    
    Epoch --  CheckpointFile
    ModelState --  CheckpointFile
    OptimizerState --  CheckpointFile
    ValF1 --  CheckpointFile
    
    CheckpointFile --  ModelDir
```

The checkpoint is saved at [notebooks/train_headline.ipynb:1482-1490]() using `torch.save()` and includes:
- `epoch`: Epoch number when best model was achieved
- `model_state_dict`: Complete model weights and biases
- `optimizer_state_dict`: Optimizer state for potential resume
- `val_f1`: Best validation F1 score achieved

### Deployment Integration

For the model to be used by the FastAPI backend, it must be moved to the `HEADLINE_MODEL_DIR` location (default: `~/model/`). The deployment process involves:

1. Saving model and tokenizer to the target directory
2. The FastAPI `load_headline_model()` function at [api/main.py]() loads the model
3. Device placement (GPU if available, else CPU) is handled at startup
4. Model is set to evaluation mode for inference

**Sources:** [notebooks/train_headline.ipynb:1482-1490](), [api/main.py]()"])</script><script>self.__next_f.push([1,"28:T438c,"])</script><script>self.__next_f.push([1,"# Training the Article Model

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [data/Fake.csv](data/Fake.csv)
- [data/True.csv](data/True.csv)
- [data/gradient_boosting_classifier.pkl](data/gradient_boosting_classifier.pkl)
- [data/tfidf_vectorizer.pkl](data/tfidf_vectorizer.pkl)
- [notebooks/train_article.ipynb](notebooks/train_article.ipynb)

 /details 



## Purpose and Scope

This document describes the training process for the article classification model used in the UnFake system. The article model is a Gradient Boosting Classifier paired with TF-IDF vectorization, designed to classify longer news articles as real or fake. This page covers the complete training pipeline implemented in the `train_article.ipynb` notebook, including data loading, preprocessing, feature extraction, model training, evaluation, and serialization.

For information about the trained model's architecture and inference process, see [Article Model (Gradient Boosting)](#5.2). For details about using this model at runtime, see [Model Loading and Lifecycle](#3.3). For the headline model training process, see [Training the Headline Model](#6.1).

**Sources**: [notebooks/train_article.ipynb:1-274]()

---

## Training Pipeline Overview

The article model training follows a traditional machine learning pipeline optimized for text classification. The process transforms raw news article text into a binary classification model capable of distinguishing real news from fake news.

```mermaid
graph TB
    LoadData[\"Load Training Data br/ True.csv + Fake.csv\"]
    Label[\"Assign Labels br/ true=1, fake=0\"]
    Concat[\"Concatenate   Shuffle br/ df.sample(frac=1)\"]
    Clean[\"Text Cleaning br/ clean_text()\"]
    Split[\"Train/Test Split br/ 75/25\"]
    Vectorize[\"TF-IDF Vectorization br/ TfidfVectorizer\"]
    TrainModels[\"Train Multiple Models br/ DT, RF, GB\"]
    Evaluate[\"Evaluate Performance br/ classification_report\"]
    Select[\"Select Best Model br/ Gradient Boosting\"]
    Serialize[\"Serialize Artifacts br/ pickle.dump()\"]
    
    LoadData --  Label
    Label --  Concat
    Concat --  Clean
    Clean --  Split
    Split --  Vectorize
    Vectorize --  TrainModels
    TrainModels --  Evaluate
    Evaluate --  Select
    Select --  Serialize
```

**Sources**: [notebooks/train_article.ipynb:1-274]()

---

## Data Loading and Preparation

### Dataset Sources

The training process begins by loading two CSV datasets containing news articles with their corresponding text and metadata. These datasets are combined to create a balanced binary classification problem.

| Dataset | Label | Content Type | Purpose |
|---------|-------|--------------|---------|
| `True.csv` | 1 | Verified real news articles | Positive class examples |
| `Fake.csv` | 0 | Known fake news articles | Negative class examples |

**Implementation**: [notebooks/train_article.ipynb:30-39]()

```python
true = pd.read_csv(\"../data/True.csv\")
fake = pd.read_csv(\"../data/Fake.csv\")
true[\"label\"] = 1
fake[\"label\"] = 0
df = pd.concat([true, fake], ignore_index=True)
df = df.sample(frac=1)
df.reset_index(inplace=True)
```

The combined dataset contains 44,898 articles after concatenation. The `sample(frac=1)` operation randomly shuffles all rows to prevent any temporal or categorical ordering bias during training.

**Sources**: [notebooks/train_article.ipynb:30-39](), [data/True.csv:1-3](), [data/Fake.csv:1-3]()

---

## Text Preprocessing

### Cleaning Pipeline

The `clean_text` function applies aggressive text normalization to remove noise and standardize the input text. This preprocessing is crucial for effective TF-IDF vectorization and model training.

```mermaid
graph LR
    Input[\"Raw Article Text\"]
    Lower[\"Convert to Lowercase br/ text.lower()\"]
    URLs[\"Remove URLs br/ https?://S+\"]
    Brackets[\"Remove Brackets br/ [.*?]\"]
    Numbers[\"Remove Numbers br/ wd*\"]
    HTML[\"Remove HTML Tags br/  .*? +\"]
    Newlines[\"Replace Newlines br/ n → space\"]
    Spaces[\"Normalize Spaces br/ s+ → single space\"]
    NonWord[\"Remove Non-Word Chars br/ W → space\"]
    Output[\"Cleaned Text\"]
    
    Input --  Lower --  URLs --  Brackets --  Numbers --  HTML --  Newlines --  Spaces --  NonWord --  Output
```

**Implementation**: [notebooks/train_article.ipynb:52-64]()

### Cleaning Operations

The function performs the following transformations using regular expressions:

| Operation | Pattern | Purpose |
|-----------|---------|---------|
| Lowercase | `text.lower()` | Normalize case |
| URL removal | `r\"https?://\\S+\\|www\\.\\S+\"` | Remove hyperlinks |
| Bracket removal | `r\"\\[.*?\\]\"` | Remove editorial notes |
| Number removal | `r\"\\w*\\d\\w*\"` | Remove alphanumeric tokens |
| HTML removal | `r\" .*? +\"` | Remove markup tags |
| Newline normalization | `r\"\
\"` | Replace with spaces |
| Space normalization | `r\"\\s+\"` | Collapse whitespace |
| Non-word removal | `r\"\\W\"` | Keep only word characters |

The cleaned text is applied to the entire `text` column using pandas' `apply()` method:

```python
df[\"text\"] = df[\"text\"].apply(clean_text)
```

**Sources**: [notebooks/train_article.ipynb:52-64]()

---

## Train/Test Split

The preprocessed dataset is split into training and testing sets using scikit-learn's `train_test_split` function with a 75/25 ratio. This split ensures sufficient data for training while reserving a representative sample for unbiased evaluation.

**Implementation**: [notebooks/train_article.ipynb:76-78]()

```python
X_train, X_test, y_train, y_test = train_test_split(
    df[\"text\"], df[\"label\"], test_size=0.25
)
```

| Split | Size (approx.) | Purpose |
|-------|---------------|---------|
| Training | 33,673 samples | Model fitting |
| Testing | 11,225 samples | Performance evaluation |

**Sources**: [notebooks/train_article.ipynb:76-78]()

---

## Feature Extraction with TF-IDF

### Vectorization Process

The training pipeline uses `TfidfVectorizer` from scikit-learn to convert cleaned text into numerical feature vectors. TF-IDF (Term Frequency-Inverse Document Frequency) captures both term importance within a document and term rarity across the corpus.

```mermaid
graph TB
    TrainText[\"Training Text br/ 33,673 samples\"]
    TestText[\"Test Text br/ 11,225 samples\"]
    Vectorizer[\"TfidfVectorizer br/ fit on training data\"]
    TrainMatrix[\"Training Matrix br/ X_train br/ sparse matrix\"]
    TestMatrix[\"Test Matrix br/ X_test br/ sparse matrix\"]
    
    TrainText --  Vectorizer
    Vectorizer -- |fit_transform| TrainMatrix
    Vectorizer -- |transform| TestMatrix
    TestText --  TestMatrix
```

**Implementation**: [notebooks/train_article.ipynb:90-92]()

```python
vect = TfidfVectorizer()
X_train = vect.fit_transform(X_train)
X_test = vect.transform(X_test)
```

### Vectorizer Configuration

The `TfidfVectorizer` is initialized with default parameters, which include:

- **Tokenization**: Word-level tokens using whitespace and punctuation
- **Vocabulary size**: Unlimited (all tokens in training data)
- **Min/max document frequency**: No constraints
- **N-gram range**: Unigrams only (1, 1)
- **Output format**: Sparse matrix representation

The fitted vectorizer learns a vocabulary of 132,208 features (as shown in [data/tfidf_vectorizer.pkl:1-10]()), creating a high-dimensional but sparse feature space suitable for the Gradient Boosting classifier.

**Sources**: [notebooks/train_article.ipynb:90-92](), [data/tfidf_vectorizer.pkl:1-10]()

---

## Model Training and Comparison

### Multi-Model Evaluation

The training notebook evaluates three tree-based classification algorithms to identify the best performer for article classification. Each model is trained on the same TF-IDF features and evaluated using identical metrics.

```mermaid
graph TB
    Features[\"TF-IDF Feature Matrix br/ X_train, X_test\"]
    
    DT[\"DecisionTreeClassifier br/ model.fit()\"]
    RF[\"RandomForestClassifier br/ model.fit()\"]
    GB[\"GradientBoostingClassifier br/ model.fit()\"]
    
    EvalDT[\"Evaluate DT br/ Accuracy: 0.996\"]
    EvalRF[\"Evaluate RF br/ Accuracy: 0.989\"]
    EvalGB[\"Evaluate GB br/ Accuracy: 0.996\"]
    
    Select[\"Select Best Model br/ Gradient Boosting\"]
    
    Features --  DT
    Features --  RF
    Features --  GB
    
    DT --  EvalDT
    RF --  EvalRF
    GB --  EvalGB
    
    EvalDT --  Select
    EvalRF --  Select
    EvalGB --  Select
```

**Sources**: [notebooks/train_article.ipynb:118-195]()

### Model Performance Comparison

Each model is trained with default hyperparameters and evaluated on the test set using scikit-learn's `classification_report` and `score` methods.

#### Decision Tree Classifier

**Implementation**: [notebooks/train_article.ipynb:123-128]()

```python
model = DecisionTreeClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

**Performance**:
- Accuracy: 0.996
- Precision (class 0): 0.99
- Precision (class 1): 1.00
- Recall (class 0): 1.00
- Recall (class 1): 0.99
- F1-score: 1.00 (weighted average)

#### Random Forest Classifier

**Implementation**: [notebooks/train_article.ipynb:155-160]()

```python
model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

**Performance**:
- Accuracy: 0.989
- Precision (class 0): 0.99
- Precision (class 1): 0.99
- Recall (class 0): 0.99
- Recall (class 1): 0.99
- F1-score: 0.99 (weighted average)

#### Gradient Boosting Classifier (Selected)

**Implementation**: [notebooks/train_article.ipynb:190-195]()

```python
model = GradientBoostingClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

**Performance**:
- Accuracy: 0.996
- Precision (class 0): 1.00
- Precision (class 1): 1.00
- Recall (class 0): 1.00
- Recall (class 1): 1.00
- F1-score: 1.00 (weighted average)

### Model Selection Rationale

The Gradient Boosting Classifier achieves the highest performance with perfect precision and recall on both classes. This model demonstrates superior generalization compared to the single Decision Tree (which may overfit) and slightly better performance than Random Forest. The selected model balances accuracy with the ensemble method's robustness to variance.

**Sources**: [notebooks/train_article.ipynb:118-195]()

---

## Model Serialization

### Artifact Generation

After training and evaluation, the notebook serializes both the trained Gradient Boosting model and the fitted TF-IDF vectorizer using Python's `pickle` module. These artifacts enable deployment without retraining.

```mermaid
graph LR
    Model[\"Trained Model br/ GradientBoostingClassifier\"]
    Vectorizer[\"Fitted Vectorizer br/ TfidfVectorizer\"]
    
    PickleModel[\"pickle.dump() br/ model → file\"]
    PickleVect[\"pickle.dump() br/ vect → file\"]
    
    ModelFile[\"gradient_boosting_classifier.pkl br/ Binary artifact\"]
    VectFile[\"tfidf_vectorizer.pkl br/ Binary artifact\"]
    
    Model --  PickleModel --  ModelFile
    Vectorizer --  PickleVect --  VectFile
```

**Implementation**: [notebooks/train_article.ipynb:208-211]()

```python
with open(\"gradient_boosting_classifier.pkl\", \"wb\") as f:
    pickle.dump(model, f)
with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:
    pickle.dump(vect, f)
```

### Serialized Artifacts

| Artifact | File | Contents | Size |
|----------|------|----------|------|
| Model | `gradient_boosting_classifier.pkl` | Trained GradientBoostingClassifier with 100 estimators, learned decision trees, feature importances | Binary (sklearn object) |
| Vectorizer | `tfidf_vectorizer.pkl` | Fitted TfidfVectorizer with vocabulary (132,208 features), IDF weights, transformation parameters | Binary (sklearn object) |

The serialized files are stored in the `data/` directory and loaded by the FastAPI application at startup (see [Model Loading and Lifecycle](#3.3)).

**Sources**: [notebooks/train_article.ipynb:208-211](), [data/gradient_boosting_classifier.pkl:1-10](), [data/tfidf_vectorizer.pkl:1-10]()

---

## Model Testing

### Sample Inference Validation

The notebook includes a validation step to test the trained model on sample statements before serialization. This ensures the model produces reasonable predictions on unseen text.

**Implementation**: [notebooks/train_article.ipynb:229-241]()

```python
sample_texts = [
    \"Breaking news: Scientists discover a cure for the common cold!\",
    \"You won't believe what this celebrity did last night!\",
    \"Trump won the 2020 election by a landslide!\",
    \"New study shows that eating chocolate improves brain function.\",
    \"Scientists discover new cure for cancer\",
    \"Trump lost the 2020 election due to widespread fraud\",
    \"Joe Biden won the 2020 presidential election\",
]

sample_texts = vect.transform(sample_texts)
predictions = model.predict(sample_texts)
```

The sample predictions demonstrate the model's behavior on sensational headlines and politically charged statements. All samples predicted class 0 (fake), indicating the model's sensitivity to linguistic patterns common in misinformation.

**Sources**: [notebooks/train_article.ipynb:229-241]()

---

## Training Notebook Structure

### Cell-by-Cell Breakdown

The `train_article.ipynb` notebook is organized into logical sections, each implemented as one or more Jupyter cells:

```mermaid
graph TB
    Cell1[\"Cell 1: Import pandas br/ Line 10\"]
    Cell2[\"Cell 2: Load   Prepare Data br/ Lines 31-39\"]
    Cell3[\"Cell 3: Define clean_text br/ Lines 52-64\"]
    Cell4[\"Cell 4: Train/Test Split br/ Lines 76-78\"]
    Cell5[\"Cell 5: TF-IDF Vectorization br/ Lines 90-92\"]
    Cell6[\"Cell 6: Train Decision Tree br/ Lines 123-128\"]
    Cell7[\"Cell 7: Train Random Forest br/ Lines 155-160\"]
    Cell8[\"Cell 8: Train Gradient Boosting br/ Lines 190-195\"]
    Cell9[\"Cell 9: Serialize Artifacts br/ Lines 208-211\"]
    Cell10[\"Cell 10: Test Samples br/ Lines 229-241\"]
    
    Cell1 --  Cell2
    Cell2 --  Cell3
    Cell3 --  Cell4
    Cell4 --  Cell5
    Cell5 --  Cell6
    Cell5 --  Cell7
    Cell5 --  Cell8
    Cell8 --  Cell9
    Cell9 --  Cell10
```

### Execution Order

| Cell | Lines | Purpose | Dependencies |
|------|-------|---------|--------------|
| 1 | 10 | Import pandas | None |
| 2 | 31-39 | Load True.csv, Fake.csv, label, concatenate, shuffle | pandas |
| 3 | 52-64 | Define and apply text cleaning function | re, pandas |
| 4 | 76-78 | Split into train/test sets | sklearn.model_selection |
| 5 | 90-92 | Fit TF-IDF vectorizer and transform splits | sklearn.feature_extraction.text |
| 6 | 123-128 | Train Decision Tree, evaluate | sklearn.tree |
| 7 | 155-160 | Train Random Forest, evaluate | sklearn.ensemble |
| 8 | 190-195 | Train Gradient Boosting, evaluate | sklearn.ensemble |
| 9 | 208-211 | Serialize model and vectorizer | pickle |
| 10 | 229-241 | Validate with sample predictions | Trained model |

**Sources**: [notebooks/train_article.ipynb:1-274]()

---

## Integration with FastAPI Backend

### Runtime Model Loading

The serialized artifacts produced by this training process are loaded by the FastAPI application during startup through the `lifespan` context manager and `load_article_model` function.

```mermaid
graph LR
    TrainOutput[\"Training Output br/ gradient_boosting_classifier.pkl br/ tfidf_vectorizer.pkl\"]
    
    LoadFunc[\"load_article_model() br/ api/main.py\"]
    PickleLoad[\"pickle.load() br/ Read binary files\"]
    
    GlobalState[\"Global State br/ article_model br/ tfidf_vectorizer\"]
    
    Inference[\"Runtime Inference br/ /predict/article endpoint\"]
    
    TrainOutput --  LoadFunc
    LoadFunc --  PickleLoad
    PickleLoad --  GlobalState
    GlobalState --  Inference
```

The loaded model and vectorizer are stored as global variables accessible to all prediction endpoints. For detailed information about the loading mechanism, see [Model Loading and Lifecycle](#3.3). For endpoint implementation, see [Article Prediction Endpoints](#3.1.2).

**Sources**: [notebooks/train_article.ipynb:208-211](), [data/gradient_boosting_classifier.pkl:1-10](), [data/tfidf_vectorizer.pkl:1-10]()

---

## Retraining Considerations

### When to Retrain

The article model should be retrained when:

- **New training data becomes available**: Additional labeled articles improve generalization
- **Model performance degrades**: Metrics drop on production data
- **Text patterns shift**: Misinformation tactics evolve over time
- **Feature engineering changes**: Modifications to `clean_text()` require retraining

### Retraining Process

1. Place new labeled data in `data/True.csv` and `data/Fake.csv`
2. Execute all cells in `notebooks/train_article.ipynb` sequentially
3. Review evaluation metrics to ensure performance meets requirements
4. Verify the new `.pkl` files are generated in `data/`
5. Restart the FastAPI application to load the new model

### Hyperparameter Tuning

The current implementation uses default hyperparameters for all models. For production optimization, consider tuning:

- `GradientBoostingClassifier` parameters: `n_estimators`, `learning_rate`, `max_depth`, `min_samples_split`
- `TfidfVectorizer` parameters: `max_features`, `ngram_range`, `min_df`, `max_df`

Grid search or random search with cross-validation can identify optimal configurations.

**Sources**: [notebooks/train_article.ipynb:1-274]()"])</script><script>self.__next_f.push([1,"29:T43a4,"])</script><script>self.__next_f.push([1,"# Data and Datasets

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [data/Fake.csv](data/Fake.csv)
- [data/README.md](data/README.md)
- [data/True.csv](data/True.csv)
- [data/download.py](data/download.py)
- [data/fake_news_dataset_2.csv](data/fake_news_dataset_2.csv)
- [data/fake_news_test_dataset_2.csv](data/fake_news_test_dataset_2.csv)
- [data/politifact_statements.csv](data/politifact_statements.csv)

 /details 



## Purpose and Scope

This page documents the datasets used in the UnFake system for training and evaluating machine learning models that detect fake news. It covers dataset sources, formats, schemas, and how they are consumed by the training pipeline. For information about how these datasets are used during model training, see [Training Pipeline](#6). For details on the trained models themselves, see [Machine Learning Models](#5).

---

## Dataset Taxonomy

The UnFake system uses three primary categories of datasets: statement-level data for short claims, article-level data for longer news content, and combined datasets that merge both types. Each category serves a distinct purpose in training specialized models.

```mermaid
graph TB
    Root[\"UnFake Datasets\"]
    
    Root --  Statements[\"Statement/Headline Datasets\"]
    Root --  Articles[\"Article Datasets\"]
    Root --  Combined[\"Combined Datasets\"]
    
    Statements --  PolitiFact[\"politifact_statements.csv\"]
    Statements --  HF[\"HuggingFace Datasets\"]
    
    Articles --  TrueCSV[\"True.csv\"]
    Articles --  FakeCSV[\"Fake.csv\"]
    
    Combined --  FakeNews2[\"fake_news_dataset_2.csv\"]
    Combined --  FakeTest2[\"fake_news_test_dataset_2.csv\"]
    
    PolitiFact --  HeadlineModel[\"RoBERTa Headline Model\"]
    HF --  HeadlineModel
    
    TrueCSV --  ArticleModel[\"Gradient Boosting Article Model\"]
    FakeCSV --  ArticleModel
    
    style HeadlineModel fill:#e8e8e8
    style ArticleModel fill:#e8e8e8
```

**Sources:** [data/politifact_statements.csv:1](), [data/True.csv:1](), [data/Fake.csv:1](), [data/fake_news_dataset_2.csv:1](), [data/fake_news_test_dataset_2.csv:1](), [data/README.md:1-5]()

---

## Statement/Headline Datasets

### PolitiFact Statements Dataset

The `politifact_statements.csv` file contains fact-checked political statements from PolitiFact.org, primarily used for training the RoBERTa-based headline classifier. This dataset focuses on short-form claims typical of political discourse.

**Schema:**

| Column | Type | Description |
|--------|------|-------------|
| `verdict` | string | Fact-check rating (e.g., \"False\", \"Half True\", \"Pants on Fire\") |
| `statement` | string | The claim being fact-checked |
| `statement_date` | date | When the statement was made |
| `statement_source` | string | Who made the statement |
| `factcheck_date` | date | When PolitiFact fact-checked it |
| `url` | string | Link to the full fact-check article |

**Example records:**

```csv
verdict,statement,statement_date,statement_source,factcheck_date,url
False,\"\"\"This Thanksgiving, turkey prices are at their lowest point since 2000.\"\"\",2025-11-24,Richard Hudson,2025-11-26,https://www.politifact.com/factchecks/2025/nov/26/...
Half True,\"The Trump administration removed nursing from the list of professional degree programs...\",2025-11-20,Social Media,2025-11-26,https://www.politifact.com/factchecks/2025/nov/26/...
```

**Verdict Distribution:**

The dataset includes multiple verdict levels reflecting PolitiFact's rating system:
- `False` - Statement is not accurate
- `Mostly False` - More inaccurate than accurate
- `Half True` - Partially accurate
- `Mostly True` - More accurate than inaccurate  
- `True` - Accurate statement
- `Pants on Fire` - Ridiculously false
- Various specialized verdicts

**Sources:** [data/politifact_statements.csv:1-150]()

### HuggingFace Datasets

The training pipeline also downloads datasets directly from HuggingFace using the `download.py` script. The primary dataset is `ErfanMoosaviMonazzah/fake-news-detection-dataset-English`.

**Download Script:**

```python
# data/download.py:4-14
dataset = \"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\"
name = dataset.split(\"/\")[-1]

ds = load_dataset(dataset)
os.makedirs(\"data\", exist_ok=True)

if isinstance(ds, dict):
  for split in ds.keys():
    ds[split].to_csv(f\"data/{split}_{name}.csv\", index=False)
else:
  ds.to_csv(f\"data/{name}.csv\", index=False)
```

This script automatically downloads and converts HuggingFace datasets to CSV format, creating separate files for each split (train/test/validation).

**Sources:** [data/download.py:1-15](), [data/README.md:3-4]()

---

## Article Datasets

### True.csv - Real News Articles

The `True.csv` file contains legitimate news articles collected from reputable sources. This dataset is used to train the article classification model to recognize authentic journalism.

**Schema:**

| Column | Type | Description |
|--------|------|-------------|
| `title` | string | Article headline |
| `text` | string | Full article content |
| `subject` | string | News category (e.g., \"politicsNews\", \"worldnews\") |
| `date` | string | Publication date |

**Example record:**

```csv
title,text,subject,date
\"As U.S. budget fight looms, Republicans flip their fiscal script\",\"WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress...\",politicsNews,\"December 31, 2017 \"
```

**Subject Categories:**

The dataset includes various news subjects:
- `politicsNews` - U.S. political coverage
- `worldnews` - International news
- `News` - General news topics

**Dataset Size:** The True.csv file contains authentic news articles spanning multiple years, with comprehensive political and world news coverage.

**Sources:** [data/True.csv:1-150]()

### Fake.csv - Fake News Articles

The `Fake.csv` file mirrors the structure of True.csv but contains articles identified as fake news, misinformation, or misleading content. This provides the negative examples for training the article classifier.

**Schema:**

| Column | Type | Description |
|--------|------|-------------|
| `title` | string | Article headline (often sensationalized) |
| `text` | string | Full article content (contains misinformation) |
| `subject` | string | Claimed news category |
| `date` | string | Claimed publication date |

**Example record:**

```csv
title,text,subject,date
\" Donald Trump Sends Out Embarrassing New Year's Eve Message; This is Disturbing\",\"Donald Trump just couldn t wish all Americans a Happy New Year...\",News,\"December 31, 2017\"
```

**Common Patterns:**

Fake news articles often exhibit:
- Sensationalized or ALL CAPS titles
- Emotional language and charged rhetoric
- Lack of credible sources
- Subject categorization that doesn't match content

**Dataset Size:** The Fake.csv file contains a large corpus of identified fake news spanning various topics and publication dates.

**Sources:** [data/Fake.csv:1-150]()

---

## Combined Datasets

### fake_news_dataset_2.csv and Test Dataset

These files combine both real and fake news with explicit binary labels, providing a unified format for model evaluation and comparison.

**Schema:**

| Column | Type | Description |
|--------|------|-------------|
| `Unnamed: 0` | integer | Row index from original dataset |
| `title` | string | Article headline |
| `text` | string | Full article content |
| `subject` | string | News category |
| `date` | string | Publication date |
| `label` | integer | Binary classification (0=fake, 1=real) |

**Label Encoding:**
- `0` = Fake news
- `1` = Real news

**Example records:**

```csv
Unnamed: 0,title,text,subject,date,label
2619,Ex-CIA head says Trump remarks on Russia interference 'disgraceful',\"Former CIA director John Brennan...\",politicsNews,\"July 22, 2017 \",1
3115,Democrats link U.S. debt limit vote to Republican tax cut moves,\"Democrats in the U.S. Congress warned...\",politicsNews,\"June 20, 2017 \",1
```

**Dataset Split:**

- `fake_news_dataset_2.csv` - Training/validation data
- `fake_news_test_dataset_2.csv` - Held-out test set for final evaluation

**Sources:** [data/fake_news_dataset_2.csv:1-150](), [data/fake_news_test_dataset_2.csv:1-150]()

---

## Dataset Sources and Provenance

```mermaid
graph LR
    Kaggle[\"Kaggle br/ emineyetm/fake-news-detection-datasets\"]
    HF1[\"HuggingFace br/ ErfanMoosaviMonazzah/fake-news-detection-dataset-English\"]
    HF2[\"HuggingFace br/ mohammadjavadpirhadi/fake-news-detection-dataset-english\"]
    PolitiFact[\"PolitiFact.org br/ Fact-Checking Database\"]
    
    Kaggle --  LocalData[\"Local data/ Directory\"]
    HF1 --  LocalData
    HF2 --  LocalData
    PolitiFact --  LocalData
    
    LocalData --  TrueCSV[\"True.csv\"]
    LocalData --  FakeCSV[\"Fake.csv\"]
    LocalData --  PolitiCSV[\"politifact_statements.csv\"]
    LocalData --  Combined[\"Combined Datasets\"]
    
    DownloadScript[\"download.py\"] --  HF1
    DownloadScript --  LocalData
```

**Primary Sources:**

1. **Kaggle Repository:**
   - URL: `https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets`
   - Contains True.csv and Fake.csv article collections

2. **HuggingFace Datasets:**
   - URL: `https://huggingface.co/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English`
   - Equivalent to: `https://huggingface.co/datasets/mohammadjavadpirhadi/fake-news-detection-dataset-english`
   - Provides programmatic access via HuggingFace datasets library

3. **PolitiFact.org:**
   - Curated database of fact-checked political statements
   - Includes detailed metadata and source attribution

**Sources:** [data/README.md:1-5]()

---

## Data Format Specifications

### Text Preprocessing Requirements

Based on the text processing pipeline, datasets undergo different cleaning procedures depending on their target model:

**Statement Data (for RoBERTa):**

```python
# Minimal cleaning preserving punctuation
# See: api/text_processing.py for clean_text()
```

Statements retain most original formatting since the transformer model handles tokenization.

**Article Data (for Gradient Boosting):**

```python
# Aggressive cleaning removing URLs, HTML, brackets
# See: api/text_processing.py for clean_article()
```

Articles undergo extensive preprocessing to remove noise before TF-IDF vectorization.

**Sources:** [api/text_processing.py:1-50]()

### Character Encoding

All CSV files use UTF-8 encoding to support international characters and special symbols commonly found in news content and political statements.

### CSV Parsing Considerations

**Quote Escaping:** Statement and article text often contains embedded quotation marks, requiring proper CSV quote escaping (doubled quotes within quoted fields).

**Multiline Fields:** The `text` column in article datasets contains multiline content, requiring parsers to handle quoted multiline fields correctly.

**Delimiter:** All datasets use comma (`,`) as the field delimiter.

---

## Dataset Usage in Training Pipeline

```mermaid
graph TB
    subgraph \"Statement Training\"
        PolitiFact[\"politifact_statements.csv\"]
        HFData[\"HuggingFace Datasets\"]
        HeadlineNB[\"notebooks/train_headline.ipynb\"]
        
        PolitiFact --  HeadlineNB
        HFData --  HeadlineNB
        HeadlineNB --  RoBERTa[\"HEADLINE_MODEL_DIR/ br/ RoBERTa + Tokenizer\"]
    end
    
    subgraph \"Article Training\"
        TrueData[\"True.csv\"]
        FakeData[\"Fake.csv\"]
        ArticleNB[\"notebooks/train_article.ipynb\"]
        
        TrueData --  ArticleNB
        FakeData --  ArticleNB
        ArticleNB --  GBC[\"gradient_boosting_classifier.pkl\"]
        ArticleNB --  TfIdf[\"tfidf_vectorizer.pkl\"]
    end
    
    subgraph \"Deployment\"
        FastAPI[\"api/main.py br/ lifespan()\"]
        RoBERTa --  FastAPI
        GBC --  FastAPI
        TfIdf --  FastAPI
    end
    
    style RoBERTa fill:#e8e8e8
    style GBC fill:#e8e8e8
    style TfIdf fill:#e8e8e8
```

### Headline Model Training

The headline/statement model training process (`train_headline.ipynb`) consumes:

1. **PolitiFact statements** - Loaded directly from CSV
2. **HuggingFace datasets** - Downloaded via `datasets` library
3. **Combined data** - Merged and preprocessed for transformer fine-tuning

**Training Steps:**
1. Load and merge statement datasets
2. Tokenize with RoBERTa tokenizer (max_length=256)
3. Create PyTorch DataLoaders
4. Fine-tune pre-trained RoBERTa model
5. Export model and tokenizer to `HEADLINE_MODEL_DIR`

**Sources:** [notebooks/train_headline.ipynb:1-100]()

### Article Model Training

The article model training process (`train_article.ipynb`) consumes:

1. **True.csv** - Labeled as class 1 (real)
2. **Fake.csv** - Labeled as class 0 (fake)

**Training Steps:**
1. Load True.csv and Fake.csv
2. Assign binary labels (0=fake, 1=real)
3. Clean text with aggressive preprocessing
4. Fit TF-IDF vectorizer on corpus
5. Train multiple classifiers (Decision Tree, Random Forest, Gradient Boosting)
6. Select best performer (Gradient Boosting)
7. Serialize model as `gradient_boosting_classifier.pkl`
8. Serialize vectorizer as `tfidf_vectorizer.pkl`

**Sources:** [notebooks/train_article.ipynb:1-100]()

---

## Dataset Statistics

### Statement Dataset Characteristics

**PolitiFact Statements:**
- **Records:** 150+ fact-checked claims shown in sample
- **Average Length:** Short form (typically 1-3 sentences)
- **Verdict Categories:** 6+ distinct rating levels
- **Temporal Coverage:** Recent political statements (2015-2025)
- **Source Diversity:** Politicians, social media, public figures

### Article Dataset Characteristics

**True.csv:**
- **Records:** Thousands of legitimate news articles
- **Average Length:** Long form (500-2000+ words typical)
- **Subject Distribution:** Heavy focus on politics and world news
- **Sources:** Reuters, other reputable news organizations
- **Date Range:** Multi-year coverage

**Fake.csv:**
- **Records:** Comparable size to True.csv
- **Average Length:** Variable, often shorter than real articles
- **Subject Distribution:** Claims diverse subjects but often political
- **Characteristics:** Sensational headlines, emotional language
- **Date Range:** Mirrors real news timeline

**Sources:** [data/politifact_statements.csv:1-150](), [data/True.csv:1-150](), [data/Fake.csv:1-150]()

---

## Data Quality Considerations

### Label Accuracy

**Statement Data:**
- Labels derived from professional fact-checkers at PolitiFact
- Multi-class verdict system provides nuanced ratings
- Source attribution included for verification

**Article Data:**
- Binary classification (real vs. fake)
- Real articles sourced from known reputable outlets
- Fake articles identified through fact-checking processes
- Some subjective judgment in borderline cases

### Data Balance

The training pipeline must handle potential class imbalance between real and fake examples. The notebooks implement techniques to ensure balanced training.

### Temporal Bias

Datasets reflect news and political discourse from specific time periods. Models trained on this data may perform differently on very recent or very old content not represented in training data.

**Sources:** [data/True.csv:1-10](), [data/Fake.csv:1-10](), [data/politifact_statements.csv:1-10]()

---

## Working with the Datasets

### Loading Datasets in Python

**Loading Statement Data:**

```python
import pandas as pd

# Load PolitiFact statements
statements = pd.read_csv('data/politifact_statements.csv')

# Access columns
verdicts = statements['verdict']
texts = statements['statement']
```

**Loading Article Data:**

```python
# Load real and fake articles
true_articles = pd.read_csv('data/True.csv')
fake_articles = pd.read_csv('data/Fake.csv')

# Add labels
true_articles['label'] = 1
fake_articles['label'] = 0

# Combine
all_articles = pd.concat([true_articles, fake_articles], ignore_index=True)
```

**Downloading from HuggingFace:**

```python
from datasets import load_dataset

# Download dataset
ds = load_dataset(\"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\")

# Convert to pandas
train_df = ds['train'].to_pandas()
```

**Sources:** [data/download.py:4-14]()

### Data Splitting

The training notebooks implement train/validation/test splits to ensure proper model evaluation and prevent overfitting. The specific split ratios and stratification strategies are defined within the training code.

### Augmentation

No explicit data augmentation is mentioned in the provided code, though text preprocessing serves as a form of normalization. The models rely on the substantial size of the datasets rather than synthetic augmentation.

---

## Future Dataset Considerations

### Expansion Opportunities

1. **Multilingual Support:** Current datasets focus on English-language content
2. **Source Diversity:** Additional fact-checking sources beyond PolitiFact
3. **Temporal Updates:** Regular refreshes to include recent claims and articles
4. **Domain Expansion:** Coverage of non-political fake news (health, science, etc.)

### Maintenance

Datasets require periodic updates to:
- Include new fake news patterns and tactics
- Reflect evolving language and discourse
- Address model drift over time
- Incorporate new verification data from fact-checkers

**Sources:** [data/README.md:1-5]()"])</script><script>self.__next_f.push([1,"2a:T451c,"])</script><script>self.__next_f.push([1,"# Deployment and Configuration

 details 
 summary Relevant source files /summary 

The following files were used as context for generating this wiki page:

- [.env.example](.env.example)
- [.gitignore](.gitignore)
- [.python-version](.python-version)
- [api/main.py](api/main.py)
- [pyproject.toml](pyproject.toml)
- [uv.lock](uv.lock)

 /details 



This page documents the deployment process, configuration options, and runtime environment setup for the UnFake application. It covers dependency installation, environment variables, model artifact locations, server configuration, and production deployment considerations.

For information about training models to generate the required artifacts, see [Training Pipeline](#6). For details on the API endpoints that the deployment serves, see [API Endpoints](#3.1).

---

## Prerequisites

### Python Version

The UnFake application requires Python 3.12 or higher, as specified in [pyproject.toml:6]() and [.python-version:1]().

### Hardware Requirements

The application supports both CPU and GPU execution:
- **CPU**: Minimum requirement for running both models
- **GPU**: Optional but recommended for the RoBERTa headline model
  - CUDA-compatible GPU required
  - PyTorch will automatically detect and utilize available GPU

---

## Dependency Management

### Core Dependencies

All project dependencies are defined in [pyproject.toml:7-26](). The application uses the following key packages:

| Package | Version | Purpose |
|---------|---------|---------|
| `fastapi` |  =0.127.0 | Web framework and API routing |
| `uvicorn` |  =0.34.0 | ASGI server |
| `torch` |  =2.9.1 | Deep learning framework for RoBERTa model |
| `transformers` |  =4.57.3 | HuggingFace library for RoBERTa |
| `scikit-learn` |  =1.8.0 | Machine learning library for Gradient Boosting |
| `nltk` |  =3.9.2 | Natural language processing utilities |
| `beautifulsoup4` |  =4.14.2 | HTML parsing for text cleaning |
| `pandas` |  =2.3.3 | Data manipulation |
| `numpy` |  =2.3.5 | Numerical computing |

### Installation

Using `uv` (recommended):
```bash
uv sync
```

Using `pip`:
```bash
pip install -e .
```

**Sources**: [pyproject.toml:1-33](), [uv.lock:1-10]()

---

## Configuration Constants

### Model Directory Paths

The application defines several critical path constants in [api/main.py:19-38]():

```mermaid
graph LR
    subgraph \"Configuration Constants\"
        HEADLINE_MODEL_DIR[\"HEADLINE_MODEL_DIR\"]
        ARTICLE_MODEL_DIR[\"ARTICLE_MODEL_DIR\"]
        VECTORIZER_DIR[\"VECTORIZER_DIR\"]
        MAX_LENGTH[\"MAX_LENGTH\"]
        DEVICE[\"DEVICE\"]
    end
    
    subgraph \"Filesystem Locations\"
        HeadlineDir[\"data/RoBERTa_Classifier/\"]
        ArticleFile[\"data/gradient_boosting_classifier.pkl\"]
        VectorizerFile[\"data/tfidf_vectorizer.pkl\"]
    end
    
    subgraph \"Runtime Values\"
        MaxLen[\"256 tokens\"]
        DeviceVal[\"cuda / cpu\"]
    end
    
    HEADLINE_MODEL_DIR --  HeadlineDir
    ARTICLE_MODEL_DIR --  ArticleFile
    VECTORIZER_DIR --  VectorizerFile
    MAX_LENGTH --  MaxLen
    DEVICE --  DeviceVal
```

#### Path Configuration Details

| Constant | Default Value | Line Reference | Description |
|----------|---------------|----------------|-------------|
| `HEADLINE_MODEL_DIR` | `data/RoBERTa_Classifier` | [api/main.py:20-22]() | Directory containing RoBERTa model and tokenizer |
| `ARTICLE_MODEL_DIR` | `data/gradient_boosting_classifier.pkl` | [api/main.py:30-32]() | Pickled Gradient Boosting model file |
| `VECTORIZER_DIR` | `data/tfidf_vectorizer.pkl` | [api/main.py:33-35]() | Pickled TF-IDF vectorizer file |
| `MAX_LENGTH` | `256` | [api/main.py:23]() | Maximum token length for headline tokenization |
| `DEVICE` | `torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` | [api/main.py:24]() | Computation device for RoBERTa inference |

**Sources**: [api/main.py:19-38]()

---

## Environment Variables

### API Keys Configuration

The application supports environment variables for external API integrations. An example environment file is provided in [.env.example:1]():

```bash
GEMINI_API_KEY=
```

**Note**: The current implementation does not actively use `GEMINI_API_KEY` in the core prediction API, but it may be used for future fact-checking integrations.

### Loading Environment Variables

If using environment variables, create a `.env` file in the project root:

```bash
cp .env.example .env
# Edit .env with your actual values
```

**Sources**: [.env.example:1]()

---

## Model Artifacts Setup

Before starting the application, ensure all required model artifacts are present. These files are generated by the training pipeline (see [Training Pipeline](#6)).

### Required Files Structure

```mermaid
graph TB
    subgraph \"Project Root\"
        DataDir[\"data/\"]
    end
    
    subgraph \"Model Artifacts\"
        RoBERTaDir[\"data/RoBERTa_Classifier/\"]
        GBCFile[\"data/gradient_boosting_classifier.pkl\"]
        TFIDFFile[\"data/tfidf_vectorizer.pkl\"]
    end
    
    subgraph \"RoBERTa Model Contents\"
        ConfigJSON[\"config.json\"]
        ModelBin[\"pytorch_model.bin\"]
        TokenizerConfig[\"tokenizer_config.json\"]
        Vocab[\"vocab.json\"]
        Merges[\"merges.txt\"]
    end
    
    DataDir --  RoBERTaDir
    DataDir --  GBCFile
    DataDir --  TFIDFFile
    
    RoBERTaDir --  ConfigJSON
    RoBERTaDir --  ModelBin
    RoBERTaDir --  TokenizerConfig
    RoBERTaDir --  Vocab
    RoBERTaDir --  Merges
```

### Verification

The application validates model presence during startup via [api/main.py:60-77]():

- **Headline Model**: Checks if `HEADLINE_MODEL_DIR` exists, raises `RuntimeError` if missing
- **Article Model**: Checks if both `ARTICLE_MODEL_DIR` and `VECTORIZER_DIR` exist, raises `RuntimeError` if either is missing

**Sources**: [api/main.py:41-77]()

---

## Application Startup and Lifespan

### Lifespan Manager

The FastAPI application uses an asynchronous context manager for startup and shutdown logic defined in [api/main.py:80-84]():

```mermaid
sequenceDiagram
    participant App as \"FastAPI Application\"
    participant Lifespan as \"lifespan() context\"
    participant LoadHead as \"load_headline_model()\"
    participant LoadArt as \"load_article_model()\"
    participant Global as \"Global Variables\"
    
    App-  Lifespan: Start application
    Lifespan-  LoadHead: Call
    LoadHead-  LoadHead: Check HEADLINE_MODEL_DIR exists
    LoadHead-  LoadHead: Load AutoTokenizer
    LoadHead-  LoadHead: Load AutoModelForSequenceClassification
    LoadHead-  LoadHead: Move to DEVICE (GPU/CPU)
    LoadHead-  LoadHead: Set to eval mode
    LoadHead-  Global: Set headline_model, tokenizer
    
    Lifespan-  LoadArt: Call
    LoadArt-  LoadArt: Check ARTICLE_MODEL_DIR exists
    LoadArt-  LoadArt: Check VECTORIZER_DIR exists
    LoadArt-  LoadArt: pickle.load() model
    LoadArt-  LoadArt: pickle.load() vectorizer
    LoadArt-  Global: Set article_model, vectorizer
    
    Lifespan-  App: Yield (app ready)
    Note over App: Application serves requests
    Lifespan-  App: Shutdown (cleanup)
```

### Model Loading Functions

#### Headline Model Loading

The `load_headline_model()` function in [api/main.py:60-77]():

1. Validates `HEADLINE_MODEL_DIR` exists
2. Loads `AutoTokenizer` from pretrained checkpoint
3. Loads `AutoModelForSequenceClassification` from pretrained checkpoint
4. Moves model to `DEVICE` (GPU if available, else CPU)
5. Sets model to evaluation mode with `.eval()`
6. Assigns to global variables `headline_model` and `tokenizer`

#### Article Model Loading

The `load_article_model()` function in [api/main.py:41-57]():

1. Validates both `ARTICLE_MODEL_DIR` and `VECTORIZER_DIR` exist
2. Deserializes `GradientBoostingClassifier` from pickle file
3. Deserializes `TfidfVectorizer` from pickle file
4. Assigns to global variables `article_model` and `vectorizer`

**Sources**: [api/main.py:41-84]()

---

## Server Configuration

### FastAPI Application Setup

The FastAPI application is configured in [api/main.py:87-100]():

```mermaid
graph TB
    subgraph \"FastAPI Configuration\"
        App[\"FastAPI app\"]
        Title[\"title='Fake News Classifier API'\"]
        Desc[\"description='API for classifying...'\"]
        Version[\"version='1.0.0'\"]
        LifespanParam[\"lifespan=lifespan\"]
    end
    
    subgraph \"Middleware\"
        CORS[\"CORSMiddleware\"]
        Origins[\"allow_origins=['*']\"]
        Creds[\"allow_credentials=True\"]
        Methods[\"allow_methods=['*']\"]
        Headers[\"allow_headers=['*']\"]
    end
    
    App --  Title
    App --  Desc
    App --  Version
    App --  LifespanParam
    
    App --  CORS
    CORS --  Origins
    CORS --  Creds
    CORS --  Methods
    CORS --  Headers
```

### CORS Configuration

CORS (Cross-Origin Resource Sharing) is configured permissively in [api/main.py:94-100]() to allow frontend access from any origin:

| Setting | Value | Purpose |
|---------|-------|---------|
| `allow_origins` | `[\"*\"]` | Accept requests from any domain |
| `allow_credentials` | `True` | Allow cookies and authentication |
| `allow_methods` | `[\"*\"]` | Allow all HTTP methods |
| `allow_headers` | `[\"*\"]` | Allow all request headers |

**Production Note**: For production deployments, restrict `allow_origins` to specific frontend domains instead of using `[\"*\"]`.

**Sources**: [api/main.py:87-100]()

---

## Device Selection and GPU Configuration

### Automatic Device Detection

The application automatically detects GPU availability at startup using [api/main.py:24]():

```python
DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")
```

### Device Usage by Model

```mermaid
graph LR
    subgraph \"Models and Device Assignment\"
        HeadlineModel[\"headline_model br/ (RoBERTa)\"]
        ArticleModel[\"article_model br/ (GradientBoosting)\"]
    end
    
    subgraph \"Device Options\"
        GPU[\"GPU (CUDA)\"]
        CPU[\"CPU\"]
    end
    
    HeadlineModel -- |\".to(DEVICE)\"| GPU
    HeadlineModel -- |\"fallback\"| CPU
    ArticleModel -- |\"always\"| CPU
    
    style GPU fill:#fff
    style CPU fill:#fff
```

| Model | Device Assignment | Code Reference |
|-------|------------------|----------------|
| RoBERTa (Headline) | GPU if available, else CPU | [api/main.py:73]() |
| Gradient Boosting (Article) | Always CPU | N/A (scikit-learn) |
| TF-IDF Vectorizer | Always CPU | N/A (scikit-learn) |

### GPU Requirements

- **CUDA Toolkit**: PyTorch with CUDA support required
- **GPU Memory**: Minimum 2GB VRAM recommended for RoBERTa model
- **Driver**: NVIDIA drivers with CUDA compatibility

**Sources**: [api/main.py:24](), [api/main.py:73]()

---

## Starting the Application

### Development Mode

#### Using uvicorn directly

From the project root:

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

#### Using the main module

The application can also be started directly via [api/main.py:286-289]():

```bash
python -m api.main
```

This runs Uvicorn with the default configuration:
- **Host**: `0.0.0.0` (all interfaces)
- **Port**: `8000`
- **Reload**: Disabled (not suitable for development)

### Configuration Options

```mermaid
graph TB
    subgraph \"Uvicorn Server Configuration\"
        Host[\"--host 0.0.0.0\"]
        Port[\"--port 8000\"]
        Reload[\"--reload\"]
        Workers[\"--workers N\"]
        LogLevel[\"--log-level info\"]
    end
    
    subgraph \"Application Access\"
        LocalAccess[\"http://localhost:8000\"]
        NetworkAccess[\"http:// ip :8000\"]
        HealthCheck[\"GET /\"]
    end
    
    Host --  NetworkAccess
    Port --  LocalAccess
    Port --  NetworkAccess
    
    LocalAccess --  HealthCheck
    NetworkAccess --  HealthCheck
```

| Option | Development | Production | Description |
|--------|-------------|------------|-------------|
| `--host` | `127.0.0.1` | `0.0.0.0` | Listen address |
| `--port` | `8000` | `8000` | Listen port |
| `--reload` | `--reload` | (omit) | Auto-reload on code changes |
| `--workers` | `1` | `2-4` | Number of worker processes |
| `--log-level` | `debug` | `info` | Logging verbosity |

**Sources**: [api/main.py:286-289]()

---

## Health Check and Readiness

The root endpoint provides a health check at [api/main.py:103-109]():

### Endpoint: `GET /`

**Response**:
```json
{
  \"status\": \"healthy\",
  \"message\": \"Fake News Classifier API is running\",
  \"model_loaded\": true
}
```

### Usage

Verify the application is running and models are loaded:

```bash
curl http://localhost:8000/
```

If `model_loaded` is `false`, the models failed to load during startup. Check logs for `RuntimeError` messages from [api/main.py:45-47]() or [api/main.py:63-67]().

**Sources**: [api/main.py:103-109]()

---

## Production Deployment Considerations

### Process Management

For production, use a process manager instead of running Uvicorn directly:

#### Option 1: Gunicorn with Uvicorn Workers

```bash
gunicorn api.main:app \\
  --workers 4 \\
  --worker-class uvicorn.workers.UvicornWorker \\
  --bind 0.0.0.0:8000 \\
  --timeout 120
```

#### Option 2: Systemd Service

Create `/etc/systemd/system/unfake.service`:

```ini
[Unit]
Description=UnFake API Service
After=network.target

[Service]
Type=simple
User=unfake
WorkingDirectory=/opt/unfake
Environment=\"PATH=/opt/unfake/.venv/bin\"
ExecStart=/opt/unfake/.venv/bin/uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 4
Restart=always

[Install]
WantedBy=multi-user.target
```

### Reverse Proxy Configuration

#### Nginx Configuration

```nginx
server {
    listen 80;
    server_name api.unfake.example.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Increase timeout for inference
        proxy_read_timeout 120s;
    }
}
```

### Security Hardening

1. **CORS**: Update [api/main.py:96]() to specify allowed origins:
   ```python
   allow_origins=[\"https://unfake.example.com\"]
   ```

2. **API Keys**: Implement authentication middleware if required

3. **Rate Limiting**: Add rate limiting to prevent abuse

4. **HTTPS**: Always use HTTPS in production with proper SSL certificates

### Resource Monitoring

```mermaid
graph TB
    subgraph \"Resource Requirements\"
        CPU[\"CPU: 2-4 cores\"]
        RAM[\"RAM: 4-8 GB\"]
        GPU[\"GPU: Optional br/ 2+ GB VRAM\"]
        Disk[\"Disk: 5 GB\"]
    end
    
    subgraph \"Monitoring Metrics\"
        ReqLatency[\"Request Latency\"]
        ModelLoad[\"Model Load Time\"]
        MemUsage[\"Memory Usage\"]
        GPUUtil[\"GPU Utilization\"]
    end
    
    CPU --  ReqLatency
    RAM --  MemUsage
    GPU --  GPUUtil
    Disk --  ModelLoad
```

### Performance Optimization

| Component | Optimization | Expected Impact |
|-----------|-------------|-----------------|
| RoBERTa Inference | Use GPU | 5-10x faster |
| Worker Processes | Multiple workers | Handle concurrent requests |
| Model Loading | Preload at startup | Eliminate cold start |
| Batch Endpoints | Use for bulk operations | Reduce overhead |

**Sources**: [api/main.py:1-289]()

---

## Frontend Deployment

The frontend static files are located in the `frontend/` directory and can be served separately:

### Static File Serving

#### Option 1: Nginx

```nginx
server {
    listen 80;
    server_name unfake.example.com;
    root /opt/unfake/frontend;
    index index.html;

    location / {
        try_files $uri $uri/ /index.html;
    }
}
```

#### Option 2: Python HTTP Server (Development Only)

```bash
cd frontend
python -m http.server 8080
```

### Frontend Configuration

Update the API URL in `frontend/app.js` if deploying the backend on a different domain or port. The frontend currently connects to `http://localhost:8000` by default.

**Sources**: Project structure from diagrams

---

## Troubleshooting

### Common Issues

#### Model Files Not Found

**Error**: `RuntimeError: Model directory not found`

**Solution**: 
1. Verify model files exist at the configured paths
2. Train models using the notebooks (see [Training Pipeline](#6))
3. Check file permissions

#### CUDA Out of Memory

**Error**: `RuntimeError: CUDA out of memory`

**Solution**:
1. Reduce batch size for batch endpoints
2. Use CPU-only deployment: `CUDA_VISIBLE_DEVICES=\"\" uvicorn api.main:app`
3. Upgrade GPU memory

#### Port Already in Use

**Error**: `OSError: [Errno 98] Address already in use`

**Solution**:
1. Change port: `uvicorn api.main:app --port 8001`
2. Kill existing process: `lsof -ti:8000 | xargs kill -9`

#### NLTK Data Missing

**Error**: `LookupError: Resource punkt not found`

**Solution**:
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

**Sources**: [api/main.py:41-77]()

---

## Environment Variables Reference

| Variable | Default | Required | Description |
|----------|---------|----------|-------------|
| `GEMINI_API_KEY` | None | No | Google Gemini API key for future integrations |
| `HEADLINE_MODEL_DIR` | `data/RoBERTa_Classifier` | Yes | Path to RoBERTa model directory |
| `ARTICLE_MODEL_DIR` | `data/gradient_boosting_classifier.pkl` | Yes | Path to article model file |
| `VECTORIZER_DIR` | `data/tfidf_vectorizer.pkl` | Yes | Path to TF-IDF vectorizer file |
| `CUDA_VISIBLE_DEVICES` | All | No | Restrict GPU visibility (set to `\"\"` for CPU-only) |

**Note**: Path constants are defined in code ([api/main.py:20-35]()) rather than environment variables. To customize paths, modify the source code or pass them as environment variables by updating the application initialization.
